{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch + Transformers\n",
    "\n",
    "```bash\n",
    "conda activate argmining19-ssc\n",
    "pip install transformers\n",
    "pip install future  # for torch.utils.tensorboard\n",
    "pip install tensorboardX\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:03.907498Z",
     "start_time": "2019-12-02T09:12:02.413498Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 10:12:03.844632 139647375513408 file_utils.py:39] PyTorch version 1.1.0 available.\n",
      "I1202 10:12:03.896306 139647375513408 modeling_xlnet.py:194] Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "#from mxnet.gluon.data import Dataset, SimpleDataset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from tensorboardX import SummaryWriter\n",
    "from transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:03.925840Z",
     "start_time": "2019-12-02T09:12:03.922770Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:04.326701Z",
     "start_time": "2019-12-02T09:12:04.321484Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "logger = logging.getLogger(\"NB: pytorch-BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:04.911840Z",
     "start_time": "2019-12-02T09:12:04.905201Z"
    }
   },
   "outputs": [],
   "source": [
    "# set repeatable random state\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "# https://pytorch.org/docs/stable/notes/randomness.html\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:05.101585Z",
     "start_time": "2019-12-02T09:12:05.097516Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply progress bars for pandas .apply() -> .progress_apply()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:05.409291Z",
     "start_time": "2019-12-02T09:12:05.271577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66f7bbfdf7a410dabda75126970e7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make tqdm jupyter friendly\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# for .progress_apply() we have to hack it like this?\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:05.455343Z",
     "start_time": "2019-12-02T09:12:05.447629Z"
    },
    "code_folding": [
     0,
     4
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        time_end = time.time()\n",
    "        time_delta = datetime.timedelta(seconds=(time_end - self.time_start))\n",
    "        if self.name:\n",
    "            print((\"Time for [{}]: {}\".format(self.name, time_delta)))\n",
    "        else:\n",
    "            print((\"Time: {}\".format(time_delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Same Side Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:05.774009Z",
     "start_time": "2019-12-02T09:12:05.770189Z"
    }
   },
   "outputs": [],
   "source": [
    "load_new = False\n",
    "# store tagged data in pickle object\n",
    "\n",
    "more_tests = False\n",
    "# whether to compute various things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:05.947405Z",
     "start_time": "2019-12-02T09:12:05.943666Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cross_path = 'data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = 'data/same-side-classification/within-topic/{}.csv'\n",
    "new_within_test = 'data/same-side-classification/within-topic/within_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load within-topics and cross-topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:06.309147Z",
     "start_time": "2019-12-02T09:12:06.298105Z"
    },
    "code_folding": [
     0,
     1,
     11
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "if load_new:\n",
    "    with Timer(\"read cross\"):\n",
    "        cross_traindev_df = pd.read_csv(data_cross_path.format('training'),\n",
    "                                        quotechar='\"',\n",
    "                                        quoting=csv.QUOTE_ALL,\n",
    "                                        encoding='utf-8',\n",
    "                                        escapechar='\\\\',\n",
    "                                        doublequote=False,\n",
    "                                        index_col='id')\n",
    "        cross_test_df = pd.read_csv(data_cross_path.format('test'), index_col='id')\n",
    "\n",
    "    with Timer(\"read within\"):\n",
    "        within_traindev_df = pd.read_csv(data_within_path.format('training'),\n",
    "                                         quotechar='\"',\n",
    "                                         quoting=csv.QUOTE_ALL,\n",
    "                                         encoding='utf-8',\n",
    "                                         escapechar='\\\\',\n",
    "                                         doublequote=False,\n",
    "                                         index_col='id')\n",
    "        # within_test_df = pd.read_csv(data_within_path.format('test'),\n",
    "        #                              quotechar='\"',\n",
    "        #                              quoting=csv.QUOTE_ALL,\n",
    "        #                              encoding='utf-8',\n",
    "        #                              escapechar='\\\\',\n",
    "        #                              doublequote=True,  # <-- change, \"\" as quote escape in text?\n",
    "        #                              index_col='id')\n",
    "        within_test_df = pd.read_csv(data_within_path.format('test'), index_col='id')\n",
    "\n",
    "    with Timer(\"read new within\"):\n",
    "        new_within_test_df = pd.read_csv(new_within_test, index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:06.488499Z",
     "start_time": "2019-12-02T09:12:06.485428Z"
    }
   },
   "outputs": [],
   "source": [
    "#! head -n 5 data/same-side-classification/within-topic/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:06.653550Z",
     "start_time": "2019-12-02T09:12:06.650257Z"
    }
   },
   "outputs": [],
   "source": [
    "#! head -n 5 data/same-side-classification/within-topic/within_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:06.836053Z",
     "start_time": "2019-12-02T09:12:06.825007Z"
    },
    "code_folding": [
     0,
     2
    ]
   },
   "outputs": [],
   "source": [
    "if load_new:\n",
    "    # Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "    def add_tag(row):\n",
    "        title = row['topic'].lower().strip()\n",
    "        if \"abortion\" in title:\n",
    "            row['tag'] = 'abortion'\n",
    "        elif \"gay marriage\"  in title:\n",
    "            row['tag'] = 'gay marriage'\n",
    "        else:\n",
    "            row['tag'] = 'NA'\n",
    "        return row\n",
    "\n",
    "\n",
    "    with Timer(\"tag cross traindev\"):\n",
    "        cross_traindev_df = cross_traindev_df.progress_apply(add_tag, axis=1)\n",
    "    with Timer(\"tag cross test\"):\n",
    "        cross_test_df = cross_test_df.progress_apply(add_tag, axis=1)\n",
    "\n",
    "    with Timer(\"tag within traindev\"):\n",
    "        within_traindev_df = within_traindev_df.progress_apply(add_tag, axis=1)\n",
    "    with Timer(\"tag within test\"):\n",
    "        within_test_df = within_test_df.progress_apply(add_tag, axis=1)\n",
    "    with Timer(\"tag new within test\"):\n",
    "        new_within_test_df = new_within_test_df.progress_apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cache data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:07.178900Z",
     "start_time": "2019-12-02T09:12:07.174963Z"
    }
   },
   "outputs": [],
   "source": [
    "FN_TAGGED = \"data/same-side-classification/tagged_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:07.347519Z",
     "start_time": "2019-12-02T09:12:07.341555Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if load_new:\n",
    "    with open(FN_TAGGED, \"wb\") as fp:\n",
    "        pickle.dump(cross_traindev_df, fp)\n",
    "        pickle.dump(cross_test_df, fp)\n",
    "        pickle.dump(within_traindev_df, fp)\n",
    "        pickle.dump(within_test_df, fp)\n",
    "        pickle.dump(new_within_test_df, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:07.707335Z",
     "start_time": "2019-12-02T09:12:07.511024Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "with open(FN_TAGGED, \"rb\") as fp:\n",
    "    cross_traindev_df = pickle.load(fp)\n",
    "    cross_test_df = pickle.load(fp)\n",
    "    within_traindev_df = pickle.load(fp)\n",
    "    within_test_df = pickle.load(fp)\n",
    "    new_within_test_df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview about each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:07.924360Z",
     "start_time": "2019-12-02T09:12:07.904373Z"
    },
    "code_folding": [
     4
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# requires nltk  wordtokenize\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# model uses BERT Tokenizer ...\n",
    "\n",
    "def get_overview(df, task='same-side', class_name='is_same_side'):\n",
    "    # Total instance numbers\n",
    "    total = len(df)\n",
    "    print(\"Task: \", task)\n",
    "    print('=' * 40, '\\n')\n",
    "\n",
    "    print('Total instances: ', total)\n",
    "    print('\\n')\n",
    "\n",
    "    print('For each topic:')\n",
    "    for tag, tag_df in df.groupby(['tag']):\n",
    "        print(tag, ': ', len(tag_df), ' instances')\n",
    "        if class_name in df.columns:\n",
    "            for is_same_side, side_df in tag_df.groupby([class_name]):\n",
    "                print('\\t\\t', is_same_side, ': ', len(side_df), ' instances')\n",
    "    print('\\n')\n",
    "\n",
    "    if class_name in df.columns:\n",
    "        print('For each class value:')\n",
    "        for class_value, class_df in df.groupby([class_name]):\n",
    "            print(class_value, ': ', len(class_df), ' instances')\n",
    "        print('\\n')\n",
    "\n",
    "    print('Unique argument1:', len(df['argument1'].unique()))\n",
    "    print('Unique argument2:', len(df['argument2'].unique()))\n",
    "    arguments = df['argument1'].values\n",
    "    arguments = np.concatenate([arguments, df['argument2'].values])\n",
    "\n",
    "    print('Unique total arguments:', len(set(list(arguments))), '\\n')\n",
    "    \n",
    "    return\n",
    "\n",
    "    print('-' * 40, '\\n')\n",
    "\n",
    "    arguments_length_lst = [\n",
    "        len(word_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_length_lst.extend(\n",
    "        [len(word_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Words:')\n",
    "    print('\\tshortest argument:', min(arguments_length_lst), ' words')\n",
    "    print('\\tlongest argument:', max(arguments_length_lst), ' words')\n",
    "    print('\\targument average length:', np.mean(arguments_length_lst),\n",
    "          ' words')\n",
    "\n",
    "    arguments_sent_length_lst = [\n",
    "        len(sent_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_sent_length_lst.extend(\n",
    "        [len(sent_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Sentences:')\n",
    "    print('\\tshortest argument:', min(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\tlongest argument:', max(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\targument average length:', np.mean(arguments_sent_length_lst),\n",
    "          ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:08.334603Z",
     "start_time": "2019-12-02T09:12:08.149980Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:  same-side\n",
      "======================================== \n",
      "\n",
      "Total instances:  61048\n",
      "\n",
      "\n",
      "For each topic:\n",
      "abortion :  61048  instances\n",
      "\t\t False :  29853  instances\n",
      "\t\t True :  31195  instances\n",
      "\n",
      "\n",
      "For each class value:\n",
      "False :  29853  instances\n",
      "True :  31195  instances\n",
      "\n",
      "\n",
      "Unique argument1: 7828\n",
      "Unique argument2: 7806\n",
      "Unique total arguments: 9361 \n",
      "\n",
      "Time for [overview cross]: 0:00:00.182008\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"overview cross\"):\n",
    "    get_overview(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:08.684303Z",
     "start_time": "2019-12-02T09:12:08.460452Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:  same-side\n",
      "======================================== \n",
      "\n",
      "Total instances:  63903\n",
      "\n",
      "\n",
      "For each topic:\n",
      "abortion :  40840  instances\n",
      "\t\t False :  20006  instances\n",
      "\t\t True :  20834  instances\n",
      "gay marriage :  23063  instances\n",
      "\t\t False :  9786  instances\n",
      "\t\t True :  13277  instances\n",
      "\n",
      "\n",
      "For each class value:\n",
      "False :  29792  instances\n",
      "True :  34111  instances\n",
      "\n",
      "\n",
      "Unique argument1: 10508\n",
      "Unique argument2: 10453\n",
      "Unique total arguments: 13574 \n",
      "\n",
      "Time for [overview within]: 0:00:00.221472\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"overview within\"):\n",
    "    get_overview(within_traindev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count raw length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:08.807934Z",
     "start_time": "2019-12-02T09:12:08.804589Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "if more_tests:\n",
    "    def compute_arg_len(row):\n",
    "        row['argument1_len'] = len(row['argument1'])\n",
    "        row['argument2_len'] = len(row['argument2'])\n",
    "        row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "        row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff'])\n",
    "        return row\n",
    "\n",
    "\n",
    "    cross_traindev_df = cross_traindev_df.progress_apply(compute_arg_len, axis=1)\n",
    "    within_traindev_df = within_traindev_df.progress_apply(compute_arg_len, axis=1)\n",
    "    cross_test_df = cross_test_df.progress_apply(compute_arg_len, axis=1)\n",
    "    within_test_df = within_test_df.progress_apply(compute_arg_len, axis=1)\n",
    "\n",
    "    cross_traindev_df.describe()\n",
    "    within_traindev_df.describe()\n",
    "    within_test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize and count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:09.032382Z",
     "start_time": "2019-12-02T09:12:09.018577Z"
    },
    "code_folding": [
     4,
     13
    ]
   },
   "outputs": [],
   "source": [
    "# BERT Tokenizer\n",
    "\n",
    "# config_class, model_class, tokenizer_class = BertConfig, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "if False:\n",
    "    ctx = mx.cpu()\n",
    "    _, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                        dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                        pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                        use_decoder=False, use_classifier=False)\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    tokenizer = bert_tokenizer\n",
    "\n",
    "if False:\n",
    "    from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "    # nltk.download('punct')\n",
    "\n",
    "\n",
    "    # tokenizer from BERT\n",
    "    def tokenize_arguments(row):\n",
    "        # tokenize\n",
    "        row['argument1_tokens'] = tokenizer(row['argument1'])\n",
    "        row['argument2_tokens'] = tokenizer(row['argument2'])\n",
    "\n",
    "        # count tokens\n",
    "        row['argument1_len'] = len(row['argument1_tokens'])\n",
    "        row['argument2_len'] = len(row['argument2_tokens'])\n",
    "        # token number diff\n",
    "        row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "        row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff'])\n",
    "        return row\n",
    "\n",
    "\n",
    "    cross_traindev_df = cross_traindev_df.progress_apply(tokenize_arguments, axis=1)\n",
    "    within_traindev_df = within_traindev_df.progress_apply(tokenize_arguments, axis=1)\n",
    "    cross_test_df = cross_test_df.progress_apply(tokenize_arguments, axis=1)\n",
    "    within_test_df = within_test_df.progress_apply(tokenize_arguments, axis=1)\n",
    "\n",
    "    cross_traindev_df.describe()\n",
    "    within_traindev_df.describe()\n",
    "    within_test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:09.218443Z",
     "start_time": "2019-12-02T09:12:09.213133Z"
    },
    "code_folding": [
     0,
     1
    ]
   },
   "outputs": [],
   "source": [
    "if more_tests:\n",
    "    def plot_lengths(df, slicen=None, abs_diff=True, title=None):\n",
    "        if df is None:\n",
    "            print(\"no lengths to plot\")\n",
    "            return\n",
    "\n",
    "        arg1_lens = df['argument1_len']\n",
    "        arg2_lens = df['argument2_len']\n",
    "        arg_diff_len = df['argument12_len_diff']\n",
    "\n",
    "        if abs_diff:\n",
    "            arg_diff_len = np.abs(arg_diff_len)\n",
    "\n",
    "        if slicen is not None:\n",
    "            arg1_lens = arg1_lens[slicen]\n",
    "            arg2_lens = arg2_lens[slicen]\n",
    "            arg_diff_len = arg_diff_len[slicen]\n",
    "\n",
    "        x = np.arange(len(arg1_lens))  # arange/linspace\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(x, arg1_lens, label='argument1')  # Linie: '-', 'o-', '.-'\n",
    "        plt.plot(x, arg2_lens, label='argument2')  # Linie: '-', 'o-', '.-'\n",
    "        plt.legend()\n",
    "        plt.title('Lengths of arguments' if not title else title)\n",
    "        plt.ylabel('Lengths of arguments 1 and 2')\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(x, arg_diff_len)\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Differences')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    plot_lengths(within_traindev_df, slice(None, None, 500), title='Length of arguments within train/dev, every 500')\n",
    "    plot_lengths(cross_traindev_df, slice(None, None, 500), title='Length of arguments cross train/dev, every 500')\n",
    "    plot_lengths(within_test_df, slice(None, None, 1), title='Length of arguments within test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dev set - 70% 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:09.777282Z",
     "start_time": "2019-12-02T09:12:09.770248Z"
    },
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']]\n",
    "    y = df[['is_same_side']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "**_Base code from [gh:grenwi](https://github.com/grenwi/argmining19-same-side-classification)_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss etc.\n",
    "\n",
    "- [BertForSequenceClassification](https://github.com/huggingface/transformers/blob/master/transformers/modeling_bert.py#L962)\n",
    "- [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss)\n",
    "- [transformers GLUE ..](https://github.com/huggingface/transformers/tree/master/examples#glue)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:10.382742Z",
     "start_time": "2019-12-02T09:12:10.348994Z"
    },
    "code_folding": [
     8,
     101,
     102,
     108,
     120,
     143,
     172,
     173,
     201,
     206
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss, BCEWithLogitsLoss\n",
    "from transformers import BertConfig, BertModel, BertPreTrainedModel\n",
    "\n",
    "# https://huggingface.co/transformers/_modules/transformers/configuration_bert.html\n",
    "\n",
    "\n",
    "# see: BertForSequenceClassification\n",
    "class BertForSameSideClassification(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "\n",
    "    # configClass = BERTSameSideConfig\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertForSameSideClassification, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "\n",
    "        if self.num_labels == 1:\n",
    "            # regression\n",
    "            self.loss_cls = MSELoss\n",
    "            # self.loss_cls = BCEWithLogitsLoss\n",
    "        else:\n",
    "            self.loss_cls = CrossEntropyLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                labels=None):\n",
    "\n",
    "        # forward(input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None)\n",
    "        # outputs = self.bert(input_ids,\n",
    "        #                     attention_mask=attention_mask,\n",
    "        #                     token_type_ids=token_type_ids,\n",
    "        #                     position_ids=position_ids,\n",
    "        #                     head_mask=head_mask,\n",
    "        #                     inputs_embeds=inputs_embeds)\n",
    "        # input_embeds only in newer version of transformers>=2.1.1 (in current master but not in pip)\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        # add hidden states and attention if they are here\n",
    "        outputs = (logits, ) + outputs[2:]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "\n",
    "            if self.num_labels == 1:\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels),\n",
    "                                labels.view(-1))\n",
    "            outputs = (loss, ) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "class BertForSameSideBCEClassification(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "        nn.init.normal(self.classifier.weight, 0, 0.01)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                labels=None):\n",
    "\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits, ) + outputs[2:]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss, ) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "# both?\n",
    "class BertForSameSideI2OBCEClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super(BertForSameSideI2OBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size * 2, self.num_labels)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                head_mask=None,\n",
    "                inputs_embeds=None,\n",
    "                input_ids2=None,\n",
    "                attention_mask2=None,\n",
    "                token_type_ids2=None,\n",
    "                position_ids2=None,\n",
    "                head_mask2=None,\n",
    "                inputs_embeds2=None,\n",
    "                labels=None):\n",
    "\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask)\n",
    "        outputs2 = self.bert(input_ids2,\n",
    "                             attention_mask=attention_mask2,\n",
    "                             token_type_ids=token_type_ids2,\n",
    "                             position_ids=position_ids2,\n",
    "                             head_mask=head_mask2)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output2 = outputs2[1]\n",
    "        pooled_output2_both = torch.cat((pooled_output, pooled_output2), 1)\n",
    "\n",
    "        pooled_output2_both = self.dropout(pooled_output2_both)\n",
    "        logits = self.classifier(pooled_output2_both)\n",
    "        outputs = (logits, ) + outputs[2:]\n",
    "        # for second input? -- (hidden_states), (attentions)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss, ) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:10.593303Z",
     "start_time": "2019-12-02T09:12:10.587100Z"
    },
    "code_folding": [
     5,
     6,
     7,
     11,
     28,
     37
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import XLNetModel, XLNetPreTrainedModel\n",
    "from transformers.modeling_utils import SequenceSummary\n",
    "\n",
    "\n",
    "class XLNetForSameSideBCEClassification(XLNetPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            With ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **mems**: (`optional`, returned when ``config.mem_len > 0``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer):\n",
    "            that contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model\n",
    "            if config.mem_len > 0 else tuple of None. Can be used to speed up sequential decoding and attend to longer context.\n",
    "            See details in the docstring of the `mems` input above.\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = XLNetTokenizer.from_pretrained('xlnet-large-cased')\n",
    "        model = XLNetForSequenceClassification.from_pretrained('xlnet-large-cased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(XLNetForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "        \n",
    "        self.transformer = XLNetModel(config)\n",
    "        self.sequence_summary = SequenceSummary(config)\n",
    "        self.logits_proj = nn.Linear(config.d_model, self.num_labels)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, mems=None, perm_mask=None, target_mapping=None,\n",
    "                token_type_ids=None, input_mask=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "        transformer_outputs = self.transformer(input_ids,\n",
    "                                               attention_mask=attention_mask,\n",
    "                                               mems=mems,\n",
    "                                               perm_mask=perm_mask,\n",
    "                                               target_mapping=target_mapping,\n",
    "                                               token_type_ids=token_type_ids,\n",
    "                                               input_mask=input_mask,\n",
    "                                               head_mask=head_mask)\n",
    "                                               # inputs_embeds=inputs_embeds\n",
    "        output = transformer_outputs[0]\n",
    "\n",
    "        output = self.sequence_summary(output)\n",
    "        logits = self.logits_proj(output)\n",
    "\n",
    "        outputs = (logits,) + transformer_outputs[1:]  # Keep mems, hidden states, attentions if there are in it\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # return (loss), logits, (mems), (hidden states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:10.827175Z",
     "start_time": "2019-12-02T09:12:10.821023Z"
    },
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import DistilBertConfig, DistilBertModel\n",
    "from transformers.modeling_distilbert import DistilBertPreTrainedModel\n",
    "\n",
    "\n",
    "class DistilBertForSameSideBCEClassification(DistilBertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (BCEWithLogitsLoss),\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(DistilBertForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.classifier = nn.Linear(config.dim, self.num_labels)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, head_mask=None, inputs_embeds=None, labels=None, token_type_ids=None):\n",
    "        # ignore token_type_ids\n",
    "        distilbert_output = self.distilbert(input_ids=input_ids,\n",
    "                                            attention_mask=attention_mask,\n",
    "                                            head_mask=head_mask)\n",
    "                                            # inputs_embeds=inputs_embeds\n",
    "        hidden_state = distilbert_output[0]                    # (bs, seq_len, dim)\n",
    "        pooled_output = hidden_state[:, 0]                    # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)   # (bs, dim)\n",
    "        pooled_output = nn.ReLU()(pooled_output)             # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)         # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)              # (bs, dim)\n",
    "\n",
    "        outputs = (logits,) + distilbert_output[1:]\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:10.966053Z",
     "start_time": "2019-12-02T09:12:10.958991Z"
    },
    "code_folding": [
     6,
     7,
     12,
     24,
     36,
     46
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import RobertaConfig, RobertaModel, BertPreTrainedModel\n",
    "from transformers.modeling_roberta import RobertaClassificationHead\n",
    "from transformers import ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n",
    "\n",
    "\n",
    "class RobertaForSameSideBCEClassification(BertPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "    config_class = RobertaConfig\n",
    "    pretrained_model_archive_map = ROBERTA_PRETRAINED_MODEL_ARCHIVE_MAP\n",
    "    base_model_prefix = \"roberta\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(RobertaForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.roberta = RobertaModel(config)\n",
    "        self.classifier = RobertaClassificationHead(config)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None,\n",
    "                labels=None):\n",
    "        outputs = self.roberta(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids,\n",
    "                               head_mask=head_mask)\n",
    "                               # inputs_embeds=inputs_embeds\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:11.221611Z",
     "start_time": "2019-12-02T09:12:11.215904Z"
    },
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from transformers import XLMModel, XLMPreTrainedModel\n",
    "from transformers.modeling_utils import SequenceSummary\n",
    "\n",
    "\n",
    "class XLMForSameSideBCEClassification(XLMPreTrainedModel):\n",
    "    r\"\"\"\n",
    "        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n",
    "            Labels for computing the sequence classification/regression loss.\n",
    "            Indices should be in ``[0, ..., config.num_labels - 1]``.\n",
    "            If ``config.num_labels == 1`` a regression loss is computed (BCEWithLogitsLoss),\n",
    "    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n",
    "        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n",
    "            Classification (or regression if config.num_labels==1) loss.\n",
    "        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n",
    "            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n",
    "        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n",
    "            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n",
    "            of shape ``(batch_size, sequence_length, hidden_size)``:\n",
    "            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n",
    "        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n",
    "            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n",
    "            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n",
    "    Examples::\n",
    "        tokenizer = XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\n",
    "        model = XLMForSequenceClassification.from_pretrained('xlm-mlm-en-2048')\n",
    "        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\")).unsqueeze(0)  # Batch size 1\n",
    "        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss, logits = outputs[:2]\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(XLMForSameSideBCEClassification, self).__init__(config)\n",
    "        self.num_labels = 1\n",
    "\n",
    "        self.transformer = XLMModel(config)\n",
    "        self.sequence_summary = SequenceSummary(config)\n",
    "\n",
    "        self.loss_kwargs = dict()\n",
    "        self.loss_cls = BCEWithLogitsLoss\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, langs=None, token_type_ids=None, position_ids=None,\n",
    "                lengths=None, cache=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "        transformer_outputs = self.transformer(input_ids,\n",
    "                                               attention_mask=attention_mask,\n",
    "                                               langs=langs,\n",
    "                                               token_type_ids=token_type_ids,\n",
    "                                               position_ids=position_ids,\n",
    "                                               lengths=lengths, \n",
    "                                               cache=cache,\n",
    "                                               head_mask=head_mask)\n",
    "                                               # inputs_embeds=inputs_embeds\n",
    "\n",
    "        output = transformer_outputs[0]\n",
    "        logits = self.sequence_summary(output)\n",
    "\n",
    "        outputs = (logits,) + transformer_outputs[1:]  # Keep new_mems and attention/hidden states if they are here\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = self.loss_cls()\n",
    "            loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:11.588240Z",
     "start_time": "2019-12-02T09:12:11.568508Z"
    },
    "code_folding": [
     0,
     66
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    #: set to true tu auto-set some params\n",
    "    'is_ss_bce': True,\n",
    "    'is_i2o': False,\n",
    "    'title-note': \"First try: DistilBert SS BCE epi 512 trim from front\",\n",
    "    'title': None,\n",
    "    'auto_adjust': True,\n",
    "\n",
    "    #: model_type: (bert|bert-ss|bert-ss-bce|...)\n",
    "    'model_type':  'distilbert-ss-bce',\n",
    "    'model_name': 'distilbert-base-uncased',\n",
    "\n",
    "    #: task_name: (binary|binary-bce)\n",
    "    'task_name': 'binary-bce',\n",
    "\n",
    "    #: output dirs\n",
    "    'data_dir': 'data/transformers/',\n",
    "    'cache_dir': 'cache/transformers/',\n",
    "    # 'output_dir': 'outputs/transformers/',\n",
    "    # 'output_dir': 'outputs/transformers/binary-label2-class',\n",
    "    # 'output_dir': 'outputs/transformers/binary-label1-reg',\n",
    "    'output_dir': 'outputs/transformers/binary-label1-class-bce-proepi',\n",
    "    # 'log_dir': 'logs/transformers/',\n",
    "    # 'log_dir': 'logs/transformers/binary-label2-class',\n",
    "    # 'log_dir': 'logs/transformers/binary-label1-reg',\n",
    "    'log_dir': 'logs/transformers/binary-label1-class-bce-proepi',\n",
    "\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "\n",
    "    'fp16': False,\n",
    "    'fp16_opt_level': 'O1',\n",
    "\n",
    "    'max_seq_length': 512,\n",
    "    #: truncate_end: (True|False) -- truncate longer inputs from start (False) or end (True)\n",
    "    'truncate_end': False,\n",
    "    #: num_labels: (1|2)\n",
    "    'num_labels': 1,\n",
    "    #: output_mode: (regression|classification) -- regression := float, classification := labels (multiple)\n",
    "    'output_mode': 'regression',\n",
    "    #: train batch_size: batch/max_seq_len: 6/512, 16/256, 32/128\n",
    "    'train_batch_size': 6,\n",
    "    #: eval batch_size can probably be slightly larger?\n",
    "    'eval_batch_size': 32,  # 128\n",
    "\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_train_epochs': 3,\n",
    "    'weight_decay': 0,      # hmm?\n",
    "    'learning_rate': 5e-6,  # same as MXNet\n",
    "    'adam_epsilon': 1e-9,   # same as MXNet\n",
    "    'warmup_steps': 0,      # hmm?\n",
    "    'max_grad_norm': 1.0,   # same as MXNet\n",
    "\n",
    "    'logging_steps': 50,\n",
    "    'eval_steps': 3000,\n",
    "    'evaluate_during_training': True,\n",
    "    #: save_steps may need to be larger for smaller batch_sizes\n",
    "    'save_steps': 3000,\n",
    "    #: ?\n",
    "    'eval_all_checkpoints': True,\n",
    "    'overwrite_output_dir': False,\n",
    "    #: cache it?\n",
    "    'reprocess_input_data': False,\n",
    "    'notes': 'SameSide argument classification task'\n",
    "}\n",
    "\n",
    "if args.get(\"auto_adjust\", False):\n",
    "    # set some params based on whether we compute same-side with BCE\n",
    "    if args.get('is_ss_bce', False):\n",
    "        if \"-ss-bce\" not in args[\"model_type\"]:\n",
    "            args[\"model_type\"] = args[\"model_type\"] + \"-ss-bce\"\n",
    "        args[\"task_name\"] = \"binary-bce\"\n",
    "        args[\"num_labels\"] = 1\n",
    "        args[\"output_mode\"] = \"regression\"\n",
    "\n",
    "    # double input mode\n",
    "    if args.get('is_i2o', False):\n",
    "        assert args[\"model_type\"].startswith(\"bert\")\n",
    "        args[\"model_type\"] = args[\"model_type\"] + \"-i2o\"\n",
    "        args[\"task_name\"] = args[\"task_name\"] + \"-i2o\"\n",
    "        if args[\"max_seq_length\"] == 512:\n",
    "            args[\"train_batch_size\"] = 4  # TODO: later increase?\n",
    "            args[\"eval_batch_size\"] = 32  # min(6, args[\"eval_batch_size\"])\n",
    "            args[\"save_steps\"] = 10000\n",
    "            args[\"logging_steps\"] = 5000\n",
    "\n",
    "    # build output folder names\n",
    "    title = args.get(\"title\", None)\n",
    "    if not title:\n",
    "        title = args[\"task_name\"]\n",
    "        if args.get('is_i20', False):\n",
    "            title += '-i2o'\n",
    "    args[\"output_dir\"] = \"outputs/transformers/\" + title\n",
    "    args[\"log_dir\"] = \"logs/transformers/\" + title\n",
    "\n",
    "# computation device\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:11.815932Z",
     "start_time": "2019-12-02T09:12:11.812154Z"
    },
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "    XLMConfig, XLMForSequenceClassification, XLMTokenizer, XLNetConfig,\n",
    "    XLNetForSequenceClassification, XLNetTokenizer, RobertaConfig,\n",
    "    RobertaForSequenceClassification, RobertaTokenizer, DistilBertConfig,\n",
    "    DistilBertForSequenceClassification, DistilBertTokenizer)\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    'bert-ss-bce':\n",
    "    (BertConfig, BertForSameSideBCEClassification, BertTokenizer),\n",
    "    'bert-ss-bce-i2o':\n",
    "    (BertConfig, BertForSameSideI2OBCEClassification, BertTokenizer),\n",
    "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "    'xlnet-ss-bce':\n",
    "    (XLNetConfig, XLNetForSameSideBCEClassification, XLNetTokenizer),\n",
    "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "    'xlm-ss-bce': (XLMConfig, XLMForSameSideBCEClassification, XLMTokenizer),\n",
    "    'roberta':\n",
    "    (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "    'roberta-ss-bce': (RobertaConfig, RobertaForSameSideBCEClassification,\n",
    "                       RobertaTokenizer),\n",
    "    'distilbert': (DistilBertConfig, DistilBertForSequenceClassification,\n",
    "                   DistilBertTokenizer),\n",
    "    'distilbert-ss-bce':\n",
    "    (DistilBertConfig, DistilBertForSameSideBCEClassification,\n",
    "     DistilBertTokenizer)\n",
    "}\n",
    "\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:13.001993Z",
     "start_time": "2019-12-02T09:12:12.052981Z"
    },
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 10:12:12.506418 139647375513408 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /home/ekoerner/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I1202 10:12:12.510771 139647375513408 configuration_utils.py:168] Model config {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"binary-bce\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1202 10:12:12.958631 139647375513408 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ekoerner/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "config = config_class.from_pretrained(args['model_name'],\n",
    "                                      num_labels=args['num_labels'],\n",
    "                                      finetuning_task=args['task_name'])\n",
    "tokenizer = tokenizer_class.from_pretrained(args['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:15.271485Z",
     "start_time": "2019-12-02T09:12:13.146787Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 10:12:13.619422 139647375513408 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /home/ekoerner/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I1202 10:12:13.623948 139647375513408 configuration_utils.py:168] Model config {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1202 10:12:14.074944 139647375513408 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-pytorch_model.bin from cache at /home/ekoerner/.cache/torch/transformers/7b8a8f0b21c4e7f6962451c9370a5d9af90372a5f64637a251f2de154d0fc72c.c2015533705b9dff680ae707e205a35e2860e8d148b45d35085419d74fe57ac5\n",
      "I1202 10:12:15.267747 139647375513408 modeling_utils.py:405] Weights of DistilBertForSameSideBCEClassification not initialized from pretrained model: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "I1202 10:12:15.268638 139647375513408 modeling_utils.py:408] Weights from pretrained model not used in DistilBertForSameSideBCEClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n"
     ]
    }
   ],
   "source": [
    "model = model_class.from_pretrained(args['model_name'], num_labels=args['num_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:17.499929Z",
     "start_time": "2019-12-02T09:12:15.420983Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSameSideBCEClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:17.634571Z",
     "start_time": "2019-12-02T09:12:17.625397Z"
    },
    "code_folding": [
     6,
     42,
     79,
     88
    ]
   },
   "outputs": [],
   "source": [
    "from transformers.data import InputExample\n",
    "# from transformers.data import InputFeatures\n",
    "from transformers.data import DataProcessor\n",
    "\n",
    "\n",
    "# TODO: binary? [0, 1] ?\n",
    "class SameSideProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the sameside data set\"\"\"\n",
    "\n",
    "    def __init__(self, trainset, devset):\n",
    "        self.trainset = trainset\n",
    "        self.devset = devset\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self.trainset, \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self.devset, \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [False, True]\n",
    "\n",
    "    def _create_examples(self, items, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "\n",
    "        for (i, item) in enumerate(items):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = item[0]\n",
    "            text_b = item[1]\n",
    "            label = item[2]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid,\n",
    "                             text_a=text_a,\n",
    "                             text_b=text_b,\n",
    "                             label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "class SameSideBinaryProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the sameside data set, label is binary.\"\"\"\n",
    "\n",
    "    def __init__(self, trainset, devset):\n",
    "        self.trainset = trainset\n",
    "        self.devset = devset\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self.trainset, \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(self.devset, \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [0, 1]\n",
    "\n",
    "    def _create_examples(self, items, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "\n",
    "        for (i, item) in enumerate(items):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = item[0]\n",
    "            text_b = item[1]\n",
    "            label = 0 if not item[2] else 1\n",
    "            examples.append(\n",
    "                InputExample(guid=guid,\n",
    "                             text_a=text_a,\n",
    "                             text_b=text_b,\n",
    "                             label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "# different names compared to transformers.data.InputFeatures\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "class InputI2OFeatures(object):\n",
    "    \"\"\"A single set of features of data for double input.\"\"\"\n",
    "\n",
    "    def __init__(self, input_feature1, input_feature2):\n",
    "        self.input_ids = input_feature1.input_ids\n",
    "        self.input_mask = input_feature1.input_mask\n",
    "        self.segment_ids = input_feature1.segment_ids\n",
    "        # shared label\n",
    "        self.label_id = input_feature1.label_id\n",
    "        self.input_ids2 = input_feature2.input_ids\n",
    "        self.input_mask2 = input_feature2.input_mask\n",
    "        self.segment_ids2 = input_feature2.segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:17.774369Z",
     "start_time": "2019-12-02T09:12:17.759513Z"
    },
    "code_folding": [
     0,
     102,
     127,
     150,
     167,
     183,
     204
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def convert_example_to_feature(example,\n",
    "                               label_map,\n",
    "                               max_seq_length,\n",
    "                               tokenizer,\n",
    "                               output_mode,\n",
    "                               cls_token_at_end,\n",
    "                               cls_token,\n",
    "                               sep_token,\n",
    "                               pad_on_left,\n",
    "                               pad_token=0,\n",
    "                               sequence_a_segment_id=0,\n",
    "                               sequence_b_segment_id=1,\n",
    "                               cls_token_segment_id=1,\n",
    "                               pad_token_segment_id=0,\n",
    "                               mask_padding_with_zero=True,\n",
    "                               truncate_end=True):\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a,\n",
    "                           tokens_b,\n",
    "                           max_seq_length - 3,\n",
    "                           from_end=truncate_end)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids:   0   0   0   0  0     0   0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = tokens_a + [sep_token]\n",
    "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "    if tokens_b:\n",
    "        tokens += tokens_b + [sep_token]\n",
    "        segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
    "\n",
    "    if cls_token_at_end:\n",
    "        tokens = tokens + [cls_token]\n",
    "        segment_ids = segment_ids + [cls_token_segment_id]\n",
    "    else:\n",
    "        tokens = [cls_token] + tokens\n",
    "        segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    if pad_on_left:\n",
    "        input_ids = ([pad_token] * padding_length) + input_ids\n",
    "        input_mask = ([0 if mask_padding_with_zero else 1] *\n",
    "                      padding_length) + input_mask\n",
    "        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "    else:\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] *\n",
    "                                   padding_length)\n",
    "        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    if output_mode == \"classification\":\n",
    "        label_id = label_map[example.label]\n",
    "    elif output_mode == \"regression\":\n",
    "        label_id = float(example.label)\n",
    "    else:\n",
    "        raise KeyError(output_mode)\n",
    "\n",
    "    return InputFeatures(input_ids=input_ids,\n",
    "                         input_mask=input_mask,\n",
    "                         segment_ids=segment_ids,\n",
    "                         label_id=label_id)\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples,\n",
    "                                 label_list,\n",
    "                                 max_seq_length,\n",
    "                                 tokenizer,\n",
    "                                 output_mode,\n",
    "                                 cls_token_at_end=False,\n",
    "                                 pad_on_left=False,\n",
    "                                 cls_token='[CLS]',\n",
    "                                 sep_token='[SEP]',\n",
    "                                 pad_token=0,\n",
    "                                 sequence_a_segment_id=0,\n",
    "                                 sequence_b_segment_id=1,\n",
    "                                 cls_token_segment_id=1,\n",
    "                                 pad_token_segment_id=0,\n",
    "                                 mask_padding_with_zero=True,\n",
    "                                 truncate_end=True):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    fn_convert = partial(convert_example_to_feature,\n",
    "                         label_map=label_map,\n",
    "                         max_seq_length=max_seq_length,\n",
    "                         tokenizer=tokenizer,\n",
    "                         output_mode=output_mode,\n",
    "                         cls_token_at_end=cls_token_at_end,\n",
    "                         cls_token=cls_token,\n",
    "                         sep_token=sep_token,\n",
    "                         pad_on_left=pad_on_left,\n",
    "                         cls_token_segment_id=cls_token_segment_id,\n",
    "                         pad_token_segment_id=pad_token_segment_id,\n",
    "                         truncate_end=truncate_end)\n",
    "\n",
    "    process_count = cpu_count() - 2\n",
    "\n",
    "    with Pool(process_count) as p:\n",
    "        features = list(\n",
    "            tqdm(p.imap(fn_convert, examples, chunksize=100),\n",
    "                 total=len(examples)))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def convert_examples_to_features_i2o(examples,\n",
    "                                     label_list,\n",
    "                                     max_seq_length,\n",
    "                                     tokenizer,\n",
    "                                     output_mode,\n",
    "                                     cls_token_at_end=False,\n",
    "                                     pad_on_left=False,\n",
    "                                     cls_token='[CLS]',\n",
    "                                     sep_token='[SEP]',\n",
    "                                     pad_token=0,\n",
    "                                     sequence_a_segment_id=0,\n",
    "                                     sequence_b_segment_id=1,\n",
    "                                     cls_token_segment_id=1,\n",
    "                                     pad_token_segment_id=0,\n",
    "                                     mask_padding_with_zero=True):\n",
    "    # currently only front and end, nothing with random etc.\n",
    "    # we just re-use _truncate_seq_pair in both variants, everything else takes more work\n",
    "    features1 = convert_examples_to_features(examples,\n",
    "                                             label_list,\n",
    "                                             max_seq_length,\n",
    "                                             tokenizer,\n",
    "                                             output_mode,\n",
    "                                             cls_token_at_end=cls_token_at_end,\n",
    "                                             pad_on_left=pad_on_left,\n",
    "                                             cls_token=cls_token,\n",
    "                                             sep_token=sep_token,\n",
    "                                             pad_token=pad_token,\n",
    "                                             sequence_a_segment_id=sequence_a_segment_id,\n",
    "                                             sequence_b_segment_id=sequence_b_segment_id,\n",
    "                                             cls_token_segment_id=cls_token_segment_id,\n",
    "                                             pad_token_segment_id=pad_token_segment_id,\n",
    "                                             mask_padding_with_zero=mask_padding_with_zero,\n",
    "                                             truncate_end=True)\n",
    "    features2 = convert_examples_to_features(examples,\n",
    "                                             label_list,\n",
    "                                             max_seq_length,\n",
    "                                             tokenizer,\n",
    "                                             output_mode,\n",
    "                                             cls_token_at_end=cls_token_at_end,\n",
    "                                             pad_on_left=pad_on_left,\n",
    "                                             cls_token=cls_token,\n",
    "                                             sep_token=sep_token,\n",
    "                                             pad_token=pad_token,\n",
    "                                             sequence_a_segment_id=sequence_a_segment_id,\n",
    "                                             sequence_b_segment_id=sequence_b_segment_id,\n",
    "                                             cls_token_segment_id=cls_token_segment_id,\n",
    "                                             pad_token_segment_id=pad_token_segment_id,\n",
    "                                             mask_padding_with_zero=mask_padding_with_zero,\n",
    "                                             truncate_end=False)\n",
    "    \n",
    "    features = [InputI2OFeatures(f1, f2) for f1, f2 in tqdm(zip(features1, features2))]\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length, from_end=True):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # from where to truncate (-1 (index) is from end, 0 is from the front)\n",
    "    pop_pos = -1 if from_end else 0\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop(pop_pos)\n",
    "        else:\n",
    "            tokens_b.pop(pop_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:17.901651Z",
     "start_time": "2019-12-02T09:12:17.899649Z"
    },
    "code_folding": [
     0,
     7
    ]
   },
   "outputs": [],
   "source": [
    "processors = {\n",
    "    \"binary\": SameSideProcessor,\n",
    "    \"binary-bce\": SameSideBinaryProcessor,\n",
    "    \"binary-bce-i2o\": SameSideBinaryProcessor,\n",
    "}\n",
    "\n",
    "# not used?\n",
    "output_modes = {\n",
    "    \"binary\": \"classification\",\n",
    "    \"binary-bce\": \"regression\",\n",
    "    \"binary-bce-i2o\": \"regression\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:18.043172Z",
     "start_time": "2019-12-02T09:12:18.027682Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [1 - test/train split]: 0:00:00.013231\n"
     ]
    }
   ],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:18.229715Z",
     "start_time": "2019-12-02T09:12:18.225649Z"
    },
    "code_folding": [
     0,
     11,
     22
    ]
   },
   "outputs": [],
   "source": [
    "def df2ds(X, y):\n",
    "    \"\"\"Convert pandas data frames to training data set\"\"\"\n",
    "    # join label to items\n",
    "    df = X.merge(y, left_index=True, right_index=True)\n",
    "    # filter neccessary columns\n",
    "    df = df[[\"argument1\", \"argument2\", \"is_same_side\"]]\n",
    "    # skip id and convert to list\n",
    "    ds = [i[1:] for i in df.itertuples()]\n",
    "    return ds\n",
    "\n",
    "\n",
    "def df2ds_rev(X, y):\n",
    "    \"\"\"Convert pandas data frames to training data set\"\"\"\n",
    "    # join label to items\n",
    "    df = X.merge(y, left_index=True, right_index=True)\n",
    "    # filter neccessary columns\n",
    "    df = df[[\"argument2\", \"argument1\", \"is_same_side\"]]\n",
    "    # skip id and convert to list\n",
    "    ds = [i[1:] for i in df.itertuples()]\n",
    "    return ds\n",
    "\n",
    "\n",
    "def df2ds_test(X):\n",
    "    # TODO: or keep id?\n",
    "    df = df[[\"argument1\", \"argument2\"]]\n",
    "    ds = [i[1:] for i in df.itertuples()]\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:18.470836Z",
     "start_time": "2019-12-02T09:12:18.412433Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [2 - convert train/dev sets input format]: 0:00:00.055957\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"2 - convert train/dev sets input format\"):\n",
    "    task = args['task_name']\n",
    "\n",
    "    ds_train = df2ds(X_train, y_train)\n",
    "    ds_dev = df2ds(X_dev, y_dev)\n",
    "\n",
    "# processor = processors[task](ds_train, ds_dev)\n",
    "# label_list = processor.get_labels()\n",
    "# num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:21.511400Z",
     "start_time": "2019-12-02T09:12:21.496882Z"
    },
    "code_folding": [
     0,
     24,
     62,
     72,
     78,
     86
    ]
   },
   "outputs": [],
   "source": [
    "def load_and_cache_examples(ds_train, ds_dev, args, tokenizer, evaluate=False):\n",
    "    task = args['task_name']\n",
    "    processor = processors[task](ds_train, ds_dev)\n",
    "    output_mode = args['output_mode']\n",
    "\n",
    "    mode = 'dev' if evaluate else 'train'\n",
    "    cached_features_file = os.path.join(\n",
    "        args['cache_dir'],\n",
    "        f\"cached_{mode}_{args['model_name']}_{args['max_seq_length']}_{task}\")\n",
    "\n",
    "    if os.path.exists(cached_features_file) and not args.get(\n",
    "            'reprocess_input_data', False):\n",
    "        logger.info(\"Loading features from cached file %s\",\n",
    "                    cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\",\n",
    "                    args['data_dir'])\n",
    "        label_list = processor.get_labels()\n",
    "        examples = processor.get_dev_examples(\n",
    "            args['data_dir']) if evaluate else processor.get_train_examples(\n",
    "                args['data_dir'])\n",
    "\n",
    "        features = convert_examples_to_features(\n",
    "            examples,\n",
    "            label_list,\n",
    "            args['max_seq_length'],\n",
    "            tokenizer,\n",
    "            output_mode,\n",
    "            # xlnet has a cls token at the end\n",
    "            cls_token_at_end=bool(args['model_type'] in ['xlnet']),\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n",
    "            # pad on the left for xlnet\n",
    "            pad_on_left=bool(args['model_type'] in ['xlnet']),\n",
    "            pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0,\n",
    "            truncate_end=args['truncate_end'])\n",
    "\n",
    "        logger.info(\"Saving features into cached file %s\",\n",
    "                    cached_features_file)\n",
    "        torch.save(features, cached_features_file)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features],\n",
    "                                 dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features],\n",
    "                                  dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features],\n",
    "                                   dtype=torch.long)\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features],\n",
    "                                     dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features],\n",
    "                                     dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                            all_label_ids)\n",
    "    return dataset\n",
    "        \n",
    "        \n",
    "def load_and_cache_examples_i2o(ds_train, ds_dev, args, tokenizer, evaluate=False):\n",
    "    task = args['task_name']\n",
    "    processor = processors[task](ds_train, ds_dev)\n",
    "    output_mode = args['output_mode']\n",
    "\n",
    "    mode = 'dev' if evaluate else 'train'\n",
    "    cached_features_file = os.path.join(\n",
    "        args['cache_dir'],\n",
    "        f\"cached_{mode}_{args['model_name']}_{args['max_seq_length']}_{task}\")\n",
    "\n",
    "    if os.path.exists(cached_features_file) and not args.get(\n",
    "            'reprocess_input_data', False):\n",
    "        logger.info(\"Loading features from cached file %s\",\n",
    "                    cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\",\n",
    "                    args['data_dir'])\n",
    "        label_list = processor.get_labels()\n",
    "        examples = processor.get_dev_examples(\n",
    "            args['data_dir']) if evaluate else processor.get_train_examples(\n",
    "                args['data_dir'])\n",
    "\n",
    "        features = convert_examples_to_features_i2o(\n",
    "            examples,\n",
    "            label_list,\n",
    "            args['max_seq_length'],\n",
    "            tokenizer,\n",
    "            output_mode,\n",
    "            # xlnet has a cls token at the end\n",
    "            cls_token_at_end=bool(args['model_type'] in ['xlnet']),\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n",
    "            # pad on the left for xlnet\n",
    "            pad_on_left=bool(args['model_type'] in ['xlnet']),\n",
    "            pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0)\n",
    "\n",
    "        logger.info(\"Saving features into cached file %s\",\n",
    "                    cached_features_file)\n",
    "        torch.save(features, cached_features_file)\n",
    "\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features],\n",
    "                                 dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features],\n",
    "                                  dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features],\n",
    "                                   dtype=torch.long)\n",
    "    all_input_ids2 = torch.tensor([f.input_ids2 for f in features],\n",
    "                                 dtype=torch.long)\n",
    "    all_input_mask2 = torch.tensor([f.input_mask2 for f in features],\n",
    "                                  dtype=torch.long)\n",
    "    all_segment_ids2 = torch.tensor([f.segment_ids2 for f in features],\n",
    "                                   dtype=torch.long)\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features],\n",
    "                                     dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features],\n",
    "                                     dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                            all_input_ids2, all_input_mask2, all_segment_ids2,\n",
    "                            all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:24.520093Z",
     "start_time": "2019-12-02T09:12:24.517683Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://beta.mxnet.io/api/ndarray/_autogen/mxnet.ndarray.sigmoid.html\n",
    "# https://stackoverflow.com/questions/43024745/applying-a-function-along-a-numpy-array\n",
    "\n",
    "\n",
    "# def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "from scipy.special import expit as sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:27.545607Z",
     "start_time": "2019-12-02T09:12:27.539839Z"
    },
    "code_folding": [
     4,
     13,
     29
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, matthews_corrcoef, confusion_matrix, accuracy_score, f1_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "def get_mismatched(labels, preds, args, ds_train, ds_dev):\n",
    "    mismatched = labels != preds\n",
    "    processor = processors[args['task_name']](ds_train, ds_dev)\n",
    "    examples = processor.get_dev_examples(args['data_dir'])\n",
    "    wrong = [i for (i, v) in zip(examples, mismatched) if v]\n",
    "\n",
    "    return wrong\n",
    "\n",
    "\n",
    "def get_eval_report(labels, preds, args, ds_train, ds_dev):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    return {\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn,\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1\n",
    "    }, get_mismatched(labels, preds, args, ds_train, ds_dev)\n",
    "\n",
    "\n",
    "def compute_metrics(preds, labels, args, ds_train, ds_dev):\n",
    "    assert len(preds) == len(labels)\n",
    "    return get_eval_report(labels, preds, args, ds_train, ds_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:30.585731Z",
     "start_time": "2019-12-02T09:12:30.563551Z"
    },
    "code_folding": [
     0,
     2,
     12,
     41,
     114
    ]
   },
   "outputs": [],
   "source": [
    "def write_eval_setup_args(args, prefix=\"\"):\n",
    "    eval_output_dir = args['output_dir']\n",
    "    if not os.path.exists(eval_output_dir):\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        writer.write(\"***** Experiments params {} *****\\n\".format(prefix))\n",
    "        writer.write(json.dumps(args))\n",
    "        writer.write(\"\\n********************************\\n\\n\")\n",
    "\n",
    "\n",
    "def evaluate(model, tokenizer, args, ds_train, ds_dev, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args['output_dir']\n",
    "    if not os.path.exists(eval_output_dir):\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    results = {}\n",
    "    EVAL_TASK = args['task_name']\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(ds_train,\n",
    "                                           ds_dev,\n",
    "                                           args,\n",
    "                                           tokenizer,\n",
    "                                           evaluate=True)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=args['eval_batch_size'])\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[3]\n",
    "            }\n",
    "            # Distilbert doesn't use token_type_ids\n",
    "            if args['model_type'].split(\"-\")[0] in [\"distilbert\"]:\n",
    "                del inputs[\"token_type_ids\"]\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        preds_ = logits.detach().cpu().numpy()\n",
    "        out_label_ids_ = inputs['labels'].detach().cpu().numpy()\n",
    "        \n",
    "        # if args['output_mode'] == \"classification\" and args['num_labels'] == 1:\n",
    "        #     # preds_ = preds_.sigmoid().round().astype('int32')\n",
    "        #     out_label_ids_ = out_label_ids_.astype('float32')\n",
    "        \n",
    "        if preds is None:\n",
    "            preds = preds_\n",
    "            out_label_ids = out_label_ids_\n",
    "        else:\n",
    "            preds = np.append(preds, preds_, axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, out_label_ids_, axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        if args['num_labels'] == 1:\n",
    "            preds = np.squeeze(preds)\n",
    "        else:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    \n",
    "    # TODO: ?\n",
    "    if args['num_labels'] == 1:\n",
    "        preds = sigmoid(preds).round().astype('int32')\n",
    "        out_label_ids = out_label_ids.astype('int32')\n",
    "\n",
    "    try:\n",
    "        result, wrong = compute_metrics(preds, out_label_ids, args, ds_train,\n",
    "                                        ds_dev)\n",
    "    except:\n",
    "        result = wrong = None\n",
    "\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        writer.write(\"***** Eval results {} *****\\n\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        writer.write(\"\\n\")\n",
    "\n",
    "    return results, wrong\n",
    "\n",
    "\n",
    "def evaluate_i2o(model, tokenizer, args, ds_train, ds_dev, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args['output_dir']\n",
    "    if not os.path.exists(eval_output_dir):\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "    results = {}\n",
    "    EVAL_TASK = args['task_name']\n",
    "\n",
    "    eval_dataset = load_and_cache_examples_i2o(ds_train,\n",
    "                                               ds_dev,\n",
    "                                               args,\n",
    "                                               tokenizer,\n",
    "                                               evaluate=True)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=args['eval_batch_size'])\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'input_ids2':\n",
    "                batch[3],\n",
    "                'attention_mask2':\n",
    "                batch[4],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids2':\n",
    "                batch[5] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[6]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        preds_ = logits.detach().cpu().numpy()\n",
    "        out_label_ids_ = inputs['labels'].detach().cpu().numpy()\n",
    "        \n",
    "        if preds is None:\n",
    "            preds = preds_\n",
    "            out_label_ids = out_label_ids_\n",
    "        else:\n",
    "            preds = np.append(preds, preds_, axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, out_label_ids_, axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        if args['num_labels'] == 1:\n",
    "            preds = np.squeeze(preds)\n",
    "        else:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    \n",
    "    if args['num_labels'] == 1:\n",
    "        preds = sigmoid(preds).round().astype('int32')\n",
    "        out_label_ids = out_label_ids.astype('int32')\n",
    "\n",
    "    try:\n",
    "        result, wrong = compute_metrics(preds, out_label_ids, args, ds_train,\n",
    "                                        ds_dev)\n",
    "    except:\n",
    "        result = wrong = None\n",
    "\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        writer.write(\"***** Eval results {} *****\\n\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        writer.write(\"\\n\")\n",
    "\n",
    "    return results, wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:33.600234Z",
     "start_time": "2019-12-02T09:12:33.591537Z"
    },
    "code_folding": [
     0,
     1,
     8,
     26,
     27,
     56,
     61
    ]
   },
   "outputs": [],
   "source": [
    "def get_train_output(model, tokenizer, args, ds_train, ds_dev, prefix=\"\", evaluate=True):\n",
    "    eval_dataset = load_and_cache_examples(ds_train,\n",
    "                                           ds_dev,\n",
    "                                           args,\n",
    "                                           tokenizer,\n",
    "                                           evaluate=evaluate)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset,\n",
    "                                 sampler=eval_sampler,\n",
    "                                 batch_size=args['eval_batch_size'])\n",
    "\n",
    "    logger.info(\"***** Running model output gen {} *****\".format(prefix))\n",
    "    logger.info(\"  Evaluation mode = %s\", evaluate)\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Get Model outputs\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[3]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        preds_ = logits.detach().cpu().numpy()\n",
    "        out_label_ids_ = inputs['labels'].detach().cpu().numpy()\n",
    "        \n",
    "        if preds is None:\n",
    "            preds = preds_\n",
    "            out_label_ids = out_label_ids_\n",
    "        else:\n",
    "            preds = np.append(preds, preds_, axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, out_label_ids_, axis=0)\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        if args['num_labels'] == 1:\n",
    "            preds = np.squeeze(preds)\n",
    "        else:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    \n",
    "    if args['num_labels'] == 1:\n",
    "        preds = sigmoid(preds).round().astype('int32')\n",
    "        out_label_ids = out_label_ids.astype('int32')\n",
    "        \n",
    "    return preds, out_label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T09:12:36.631755Z",
     "start_time": "2019-12-02T09:12:36.600375Z"
    },
    "code_folding": [
     0,
     14,
     18,
     22,
     26,
     27,
     33,
     41,
     44,
     48,
     98,
     101,
     107,
     121,
     138,
     178
    ]
   },
   "outputs": [],
   "source": [
    "def compute_train_acc(preds, labels, args):\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        if args['num_labels'] == 1:\n",
    "            preds = np.squeeze(preds)\n",
    "        else:\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    if args['num_labels'] == 1:\n",
    "        preds = sigmoid(preds).round().astype('int32')\n",
    "        labels = labels.astype('int32')\n",
    "    return accuracy_score(labels, preds)\n",
    "\n",
    "\n",
    "def train(train_dataset, model, tokenizer, args, ds_train=None, ds_dev=None):\n",
    "    tb_writer = SummaryWriter(args[\"log_dir\"])\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  sampler=train_sampler,\n",
    "                                  batch_size=args['train_batch_size'])\n",
    "\n",
    "    t_total = len(train_dataloader) // args[\n",
    "        'gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [{\n",
    "        'params': [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        args['weight_decay']\n",
    "    }, {\n",
    "        'params': [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        0.0\n",
    "    }]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=args['learning_rate'],\n",
    "                      eps=args['adam_epsilon'])\n",
    "    scheduler = WarmupLinearSchedule(optimizer,\n",
    "                                     warmup_steps=args['warmup_steps'],\n",
    "                                     t_total=t_total)\n",
    "\n",
    "    if args['fp16']:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n",
    "            )\n",
    "        model, optimizer = amp.initialize(model,\n",
    "                                          optimizer,\n",
    "                                          opt_level=args['fp16_opt_level'])\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\",\n",
    "                args['gradient_accumulation_steps'])\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "    \n",
    "    preds = out_label_ids = None  # inline acc computation\n",
    "\n",
    "    for epoch_nr in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration {}/{}\".format(epoch_nr + 1, args['num_train_epochs']))\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[3]\n",
    "            }\n",
    "            # Distilbert doesn't use token_type_ids\n",
    "            if args['model_type'].split(\"-\")[0] in [\"distilbert\"]:\n",
    "                del inputs[\"token_type_ids\"]\n",
    "            outputs = model(**inputs)\n",
    "            # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            loss = outputs[0]\n",
    "            # print(\"\\rLoss: %f\" % loss, end='')  # has no \"real\" meaning for me?\n",
    "\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            if args['fp16']:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer),\n",
    "                                               args['max_grad_norm'])\n",
    "\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                               args['max_grad_norm'])\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "                if args['eval_steps'] > 0 and global_step % args[\n",
    "                        'eval_steps'] == 0:\n",
    "                    # Log metrics\n",
    "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                    if args['evaluate_during_training']:\n",
    "                        results, _ = evaluate(model, tokenizer, args, ds_train,\n",
    "                                              ds_dev)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar('eval_{}'.format(key), value,\n",
    "                                                 global_step)\n",
    "\n",
    "                if args['logging_steps'] > 0 and global_step % args[\n",
    "                        'logging_steps'] == 0:\n",
    "\n",
    "                    # compute acc (see eval)\n",
    "                    preds_ = outputs[1].detach().cpu().numpy()  # logits\n",
    "                    out_label_ids_ = inputs['labels'].detach().cpu().numpy()\n",
    "                    if preds is None:\n",
    "                        preds = preds_\n",
    "                        out_label_ids = out_label_ids_\n",
    "                    else:\n",
    "                        preds = np.append(preds, preds_, axis=0)\n",
    "                        out_label_ids = np.append(out_label_ids, out_label_ids_, axis=0)\n",
    "                    acc = compute_train_acc(preds, out_label_ids, args)\n",
    "                    cur_acc = compute_train_acc(preds_, out_label_ids_, args)\n",
    "                    print(\"train acc: {:.3f} (cur acc: {:.3f})\".format(acc, cur_acc), end=\"\\r\")\n",
    "\n",
    "                    tb_writer.add_scalar('lr',\n",
    "                                         scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar('acc', acc, global_step)\n",
    "                    # tb_writer.add_pr_curve('pr_curve', out_label_ids_, preds_, global_step)\n",
    "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss) /\n",
    "                                         args['logging_steps'], global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args['save_steps'] > 0 and global_step % args[\n",
    "                        'save_steps'] == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(\n",
    "                        args['output_dir'],\n",
    "                        'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    # Take care of distributed/parallel training\n",
    "                    model_to_save = model.module if hasattr(\n",
    "                        model, 'module') else model\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "    try:\n",
    "        tb_writer.close()\n",
    "    except Exception as ex:\n",
    "        logger.exception(\"SummaryWriter.close() error?\")\n",
    "\n",
    "    return global_step, tr_loss / global_step\n",
    "\n",
    "\n",
    "def train_i2o(train_dataset, model, tokenizer, args, ds_train=None, ds_dev=None):\n",
    "    tb_writer = SummaryWriter(args[\"log_dir\"])\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  sampler=train_sampler,\n",
    "                                  batch_size=args['train_batch_size'])\n",
    "\n",
    "    t_total = len(train_dataloader) // args[\n",
    "        'gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [{\n",
    "        'params': [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        args['weight_decay']\n",
    "    }, {\n",
    "        'params': [\n",
    "            p for n, p in model.named_parameters()\n",
    "            if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        'weight_decay':\n",
    "        0.0\n",
    "    }]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                      lr=args['learning_rate'],\n",
    "                      eps=args['adam_epsilon'])\n",
    "    scheduler = WarmupLinearSchedule(optimizer,\n",
    "                                     warmup_steps=args['warmup_steps'],\n",
    "                                     t_total=t_total)\n",
    "\n",
    "    if args['fp16']:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n",
    "            )\n",
    "        model, optimizer = amp.initialize(model,\n",
    "                                          optimizer,\n",
    "                                          opt_level=args['fp16_opt_level'])\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\",\n",
    "                args['gradient_accumulation_steps'])\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "\n",
    "    for epoch_nr in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration {}/{}\".format(epoch_nr + 1, args['num_train_epochs']))\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\n",
    "                'input_ids':\n",
    "                batch[0],\n",
    "                'attention_mask':\n",
    "                batch[1],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids':\n",
    "                batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'input_ids2':\n",
    "                batch[3],\n",
    "                'attention_mask2':\n",
    "                batch[4],\n",
    "                # XLM don't use segment_ids\n",
    "                'token_type_ids2':\n",
    "                batch[5] if args['model_type'] in ['bert', 'xlnet'] else None,\n",
    "                'labels':\n",
    "                batch[6]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            loss = outputs[0]\n",
    "\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            if args['fp16']:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer),\n",
    "                                               args['max_grad_norm'])\n",
    "\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(),\n",
    "                                               args['max_grad_norm'])\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "\n",
    "                global_step += 1\n",
    "\n",
    "                if args['logging_steps'] > 0 and global_step % args[\n",
    "                        'logging_steps'] == 0:\n",
    "                    # Log metrics\n",
    "                    # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                    if args['evaluate_during_training']:\n",
    "                        results, _ = evaluate_i2o(model, tokenizer, args, ds_train,\n",
    "                                                  ds_dev)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar('eval_{}'.format(key), value,\n",
    "                                                 global_step)\n",
    "                    tb_writer.add_scalar('lr',\n",
    "                                         scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss) /\n",
    "                                         args['logging_steps'], global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args['save_steps'] > 0 and global_step % args[\n",
    "                        'save_steps'] == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(\n",
    "                        args['output_dir'],\n",
    "                        'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    # Take care of distributed/parallel training\n",
    "                    model_to_save = model.module if hasattr(\n",
    "                        model, 'module') else model\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "    try:\n",
    "        tb_writer.close()\n",
    "    except Exception as ex:\n",
    "        logger.exception(\"SummaryWriter.close() error?\")\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T10:55:13.205112Z",
     "start_time": "2019-12-02T09:12:46.387945Z"
    },
    "code_folding": [
     4
    ],
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 10:12:46.397599 139647375513408 <ipython-input-40-fa36fc6f9a16>:14] Loading features from cached file cache/transformers/cached_train_distilbert-base-uncased_512_binary-bce\n",
      "I1202 10:12:50.748721 139647375513408 <ipython-input-45-08f342e6a116>:60] ***** Running training *****\n",
      "I1202 10:12:50.749628 139647375513408 <ipython-input-45-08f342e6a116>:61]   Num examples = 57512\n",
      "I1202 10:12:50.750196 139647375513408 <ipython-input-45-08f342e6a116>:62]   Num Epochs = 3\n",
      "I1202 10:12:50.750678 139647375513408 <ipython-input-45-08f342e6a116>:63]   Total train batch size  = 6\n",
      "I1202 10:12:50.751140 139647375513408 <ipython-input-45-08f342e6a116>:65]   Gradient Accumulation steps = 1\n",
      "I1202 10:12:50.751663 139647375513408 <ipython-input-45-08f342e6a116>:66]   Total optimization steps = 28758\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f70813cc75e43faa5e35fd239c73130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration 1/3', max=9586, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.619 (cur acc: 0.500)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 10:22:27.668914 139647375513408 <ipython-input-40-fa36fc6f9a16>:19] Creating features from dataset file at data/transformers/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c901c95ea1c64a539c5ddfc58bf87c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6391), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 10:22:32.624720 139647375513408 <ipython-input-40-fa36fc6f9a16>:42] Saving features into cached file cache/transformers/cached_dev_distilbert-base-uncased_512_binary-bce\n",
      "I1202 10:22:35.358091 139647375513408 <ipython-input-43-1a70036edb06>:34] ***** Running evaluation  *****\n",
      "I1202 10:22:35.359086 139647375513408 <ipython-input-43-1a70036edb06>:35]   Num examples = 6391\n",
      "I1202 10:22:35.359692 139647375513408 <ipython-input-43-1a70036edb06>:36]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029665a5f2d146269d3a9d6e56e54512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 10:23:35.319556 139647375513408 <ipython-input-43-1a70036edb06>:105] ***** Eval results  *****\n",
      "I1202 10:23:35.320501 139647375513408 <ipython-input-43-1a70036edb06>:108]   acc = 0.6240025035205758\n",
      "I1202 10:23:35.321291 139647375513408 <ipython-input-43-1a70036edb06>:108]   f1 = 0.6448943401802866\n",
      "I1202 10:23:35.322016 139647375513408 <ipython-input-43-1a70036edb06>:108]   fn = 1250\n",
      "I1202 10:23:35.322731 139647375513408 <ipython-input-43-1a70036edb06>:108]   fp = 1153\n",
      "I1202 10:23:35.323436 139647375513408 <ipython-input-43-1a70036edb06>:108]   mcc = 0.24568143894044497\n",
      "I1202 10:23:35.324122 139647375513408 <ipython-input-43-1a70036edb06>:108]   tn = 1806\n",
      "I1202 10:23:35.324613 139647375513408 <ipython-input-43-1a70036edb06>:108]   tp = 2182\n",
      "I1202 10:23:35.328754 139647375513408 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-bce/checkpoint-3000/config.json\n",
      "I1202 10:23:35.501348 139647375513408 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-bce/checkpoint-3000/pytorch_model.bin\n",
      "I1202 10:23:35.502315 139647375513408 <ipython-input-45-08f342e6a116>:169] Saving model checkpoint to outputs/transformers/binary-bce/checkpoint-3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.696 (cur acc: 0.833)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 10:33:13.530078 139647375513408 <ipython-input-40-fa36fc6f9a16>:14] Loading features from cached file cache/transformers/cached_dev_distilbert-base-uncased_512_binary-bce\n",
      "I1202 10:33:13.967953 139647375513408 <ipython-input-43-1a70036edb06>:34] ***** Running evaluation  *****\n",
      "I1202 10:33:13.968921 139647375513408 <ipython-input-43-1a70036edb06>:35]   Num examples = 6391\n",
      "I1202 10:33:13.969562 139647375513408 <ipython-input-43-1a70036edb06>:36]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe11ebe9f98d4af9a7625a47bfe12302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 10:34:14.041770 139647375513408 <ipython-input-43-1a70036edb06>:105] ***** Eval results  *****\n",
      "I1202 10:34:14.042230 139647375513408 <ipython-input-43-1a70036edb06>:108]   acc = 0.7942419026756377\n",
      "I1202 10:34:14.042537 139647375513408 <ipython-input-43-1a70036edb06>:108]   f1 = 0.778283594672062\n",
      "I1202 10:34:14.042805 139647375513408 <ipython-input-43-1a70036edb06>:108]   fn = 1124\n",
      "I1202 10:34:14.043051 139647375513408 <ipython-input-43-1a70036edb06>:108]   fp = 191\n",
      "I1202 10:34:14.043314 139647375513408 <ipython-input-43-1a70036edb06>:108]   mcc = 0.6212138232100833\n",
      "I1202 10:34:14.043569 139647375513408 <ipython-input-43-1a70036edb06>:108]   tn = 2768\n",
      "I1202 10:34:14.043857 139647375513408 <ipython-input-43-1a70036edb06>:108]   tp = 2308\n",
      "I1202 10:34:14.046355 139647375513408 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-bce/checkpoint-6000/config.json\n",
      "I1202 10:34:14.200075 139647375513408 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-bce/checkpoint-6000/pytorch_model.bin\n",
      "I1202 10:34:14.200654 139647375513408 <ipython-input-45-08f342e6a116>:169] Saving model checkpoint to outputs/transformers/binary-bce/checkpoint-6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.737 (cur acc: 1.000)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 10:43:50.929927 139647375513408 <ipython-input-40-fa36fc6f9a16>:14] Loading features from cached file cache/transformers/cached_dev_distilbert-base-uncased_512_binary-bce\n",
      "I1202 10:43:51.246610 139647375513408 <ipython-input-43-1a70036edb06>:34] ***** Running evaluation  *****\n",
      "I1202 10:43:51.247572 139647375513408 <ipython-input-43-1a70036edb06>:35]   Num examples = 6391\n",
      "I1202 10:43:51.248205 139647375513408 <ipython-input-43-1a70036edb06>:36]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2d317958404041b10f953d1a8d3918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 10:44:51.313766 139647375513408 <ipython-input-43-1a70036edb06>:105] ***** Eval results  *****\n",
      "I1202 10:44:51.314388 139647375513408 <ipython-input-43-1a70036edb06>:108]   acc = 0.8087936160225316\n",
      "I1202 10:44:51.314891 139647375513408 <ipython-input-43-1a70036edb06>:108]   f1 = 0.7896005509641874\n",
      "I1202 10:44:51.315345 139647375513408 <ipython-input-43-1a70036edb06>:108]   fn = 1139\n",
      "I1202 10:44:51.315810 139647375513408 <ipython-input-43-1a70036edb06>:108]   fp = 83\n",
      "I1202 10:44:51.316227 139647375513408 <ipython-input-43-1a70036edb06>:108]   mcc = 0.660404454749411\n",
      "I1202 10:44:51.316663 139647375513408 <ipython-input-43-1a70036edb06>:108]   tn = 2876\n",
      "I1202 10:44:51.317054 139647375513408 <ipython-input-43-1a70036edb06>:108]   tp = 2293\n",
      "I1202 10:44:51.320492 139647375513408 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-bce/checkpoint-9000/config.json\n",
      "I1202 10:44:51.469225 139647375513408 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-bce/checkpoint-9000/pytorch_model.bin\n",
      "I1202 10:44:51.470164 139647375513408 <ipython-input-45-08f342e6a116>:169] Saving model checkpoint to outputs/transformers/binary-bce/checkpoint-9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.746 (cur acc: 0.667)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███▎      | 1/3 [33:53<1:07:47, 2033.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445fadc2edda44f5a7f03371c720a3cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration 2/3', max=9586, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.757 (cur acc: 0.667)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 10:54:29.222965 139647375513408 <ipython-input-40-fa36fc6f9a16>:14] Loading features from cached file cache/transformers/cached_dev_distilbert-base-uncased_512_binary-bce\n",
      "I1202 10:54:29.642754 139647375513408 <ipython-input-43-1a70036edb06>:34] ***** Running evaluation  *****\n",
      "I1202 10:54:29.643735 139647375513408 <ipython-input-43-1a70036edb06>:35]   Num examples = 6391\n",
      "I1202 10:54:29.644347 139647375513408 <ipython-input-43-1a70036edb06>:36]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2af6c4af6b145618a359607a48c88cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 10:55:29.703753 139647375513408 <ipython-input-43-1a70036edb06>:105] ***** Eval results  *****\n",
      "I1202 10:55:29.704485 139647375513408 <ipython-input-43-1a70036edb06>:108]   acc = 0.8122359568142701\n",
      "I1202 10:55:29.705010 139647375513408 <ipython-input-43-1a70036edb06>:108]   f1 = 0.8003992015968063\n",
      "I1202 10:55:29.705496 139647375513408 <ipython-input-43-1a70036edb06>:108]   fn = 1026\n",
      "I1202 10:55:29.705962 139647375513408 <ipython-input-43-1a70036edb06>:108]   fp = 174\n",
      "I1202 10:55:29.706424 139647375513408 <ipython-input-43-1a70036edb06>:108]   mcc = 0.6527061884256545\n",
      "I1202 10:55:29.706946 139647375513408 <ipython-input-43-1a70036edb06>:108]   tn = 2785\n",
      "I1202 10:55:29.707421 139647375513408 <ipython-input-43-1a70036edb06>:108]   tp = 2406\n",
      "I1202 10:55:29.711538 139647375513408 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-bce/checkpoint-12000/config.json\n",
      "I1202 10:55:29.861761 139647375513408 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-bce/checkpoint-12000/pytorch_model.bin\n",
      "I1202 10:55:29.862730 139647375513408 <ipython-input-45-08f342e6a116>:169] Saving model checkpoint to outputs/transformers/binary-bce/checkpoint-12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.770 (cur acc: 0.833)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:05:03.901622 139647375513408 <ipython-input-40-fa36fc6f9a16>:14] Loading features from cached file cache/transformers/cached_dev_distilbert-base-uncased_512_binary-bce\n",
      "I1202 11:05:04.206497 139647375513408 <ipython-input-43-1a70036edb06>:34] ***** Running evaluation  *****\n",
      "I1202 11:05:04.207448 139647375513408 <ipython-input-43-1a70036edb06>:35]   Num examples = 6391\n",
      "I1202 11:05:04.208168 139647375513408 <ipython-input-43-1a70036edb06>:36]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34fc62b12e2a4f57a1447fb036929b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:06:04.237010 139647375513408 <ipython-input-43-1a70036edb06>:105] ***** Eval results  *****\n",
      "I1202 11:06:04.237845 139647375513408 <ipython-input-43-1a70036edb06>:108]   acc = 0.8102018463464247\n",
      "I1202 11:06:04.238377 139647375513408 <ipython-input-43-1a70036edb06>:108]   f1 = 0.7941625657559817\n",
      "I1202 11:06:04.238867 139647375513408 <ipython-input-43-1a70036edb06>:108]   fn = 1092\n",
      "I1202 11:06:04.239338 139647375513408 <ipython-input-43-1a70036edb06>:108]   fp = 121\n",
      "I1202 11:06:04.239872 139647375513408 <ipython-input-43-1a70036edb06>:108]   mcc = 0.6567527833269\n",
      "I1202 11:06:04.240439 139647375513408 <ipython-input-43-1a70036edb06>:108]   tn = 2838\n",
      "I1202 11:06:04.240919 139647375513408 <ipython-input-43-1a70036edb06>:108]   tp = 2340\n",
      "I1202 11:06:04.245256 139647375513408 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-bce/checkpoint-15000/config.json\n",
      "I1202 11:06:04.410721 139647375513408 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-bce/checkpoint-15000/pytorch_model.bin\n",
      "I1202 11:06:04.411728 139647375513408 <ipython-input-45-08f342e6a116>:169] Saving model checkpoint to outputs/transformers/binary-bce/checkpoint-15000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.782 (cur acc: 1.000)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:15:47.240919 139647375513408 <ipython-input-40-fa36fc6f9a16>:14] Loading features from cached file cache/transformers/cached_dev_distilbert-base-uncased_512_binary-bce\n",
      "I1202 11:15:47.710776 139647375513408 <ipython-input-43-1a70036edb06>:34] ***** Running evaluation  *****\n",
      "I1202 11:15:47.711791 139647375513408 <ipython-input-43-1a70036edb06>:35]   Num examples = 6391\n",
      "I1202 11:15:47.712362 139647375513408 <ipython-input-43-1a70036edb06>:36]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e632fc2c9d400c89376195b86a20b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:16:48.245051 139647375513408 <ipython-input-43-1a70036edb06>:105] ***** Eval results  *****\n",
      "I1202 11:16:48.245957 139647375513408 <ipython-input-43-1a70036edb06>:108]   acc = 0.8158347676419966\n",
      "I1202 11:16:48.246510 139647375513408 <ipython-input-43-1a70036edb06>:108]   f1 = 0.8014172431246837\n",
      "I1202 11:16:48.247011 139647375513408 <ipython-input-43-1a70036edb06>:108]   fn = 1057\n",
      "I1202 11:16:48.247484 139647375513408 <ipython-input-43-1a70036edb06>:108]   fp = 120\n",
      "I1202 11:16:48.248019 139647375513408 <ipython-input-43-1a70036edb06>:108]   mcc = 0.6658716299802083\n",
      "I1202 11:16:48.248487 139647375513408 <ipython-input-43-1a70036edb06>:108]   tn = 2839\n",
      "I1202 11:16:48.248945 139647375513408 <ipython-input-43-1a70036edb06>:108]   tp = 2375\n",
      "I1202 11:16:48.253653 139647375513408 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-bce/checkpoint-18000/config.json\n",
      "I1202 11:16:48.414074 139647375513408 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-bce/checkpoint-18000/pytorch_model.bin\n",
      "I1202 11:16:48.415081 139647375513408 <ipython-input-45-08f342e6a116>:169] Saving model checkpoint to outputs/transformers/binary-bce/checkpoint-18000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.784 (cur acc: 1.000)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████▋   | 2/3 [1:07:44<33:52, 2032.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c149038e7c024d11bc66508cae037756",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration 3/3', max=9586, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.788 (cur acc: 0.833)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:26:18.330991 139647375513408 <ipython-input-40-fa36fc6f9a16>:14] Loading features from cached file cache/transformers/cached_dev_distilbert-base-uncased_512_binary-bce\n",
      "I1202 11:26:18.648135 139647375513408 <ipython-input-43-1a70036edb06>:34] ***** Running evaluation  *****\n",
      "I1202 11:26:18.649097 139647375513408 <ipython-input-43-1a70036edb06>:35]   Num examples = 6391\n",
      "I1202 11:26:18.649753 139647375513408 <ipython-input-43-1a70036edb06>:36]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b92fe25c0048c6b2966c558463387b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:27:18.842978 139647375513408 <ipython-input-43-1a70036edb06>:105] ***** Eval results  *****\n",
      "I1202 11:27:18.843730 139647375513408 <ipython-input-43-1a70036edb06>:108]   acc = 0.8128618369582225\n",
      "I1202 11:27:18.844241 139647375513408 <ipython-input-43-1a70036edb06>:108]   f1 = 0.802835476425981\n",
      "I1202 11:27:18.844650 139647375513408 <ipython-input-43-1a70036edb06>:108]   fn = 997\n",
      "I1202 11:27:18.845010 139647375513408 <ipython-input-43-1a70036edb06>:108]   fp = 199\n",
      "I1202 11:27:18.845391 139647375513408 <ipython-input-43-1a70036edb06>:108]   mcc = 0.6506078624404638\n",
      "I1202 11:27:18.845761 139647375513408 <ipython-input-43-1a70036edb06>:108]   tn = 2760\n",
      "I1202 11:27:18.846809 139647375513408 <ipython-input-43-1a70036edb06>:108]   tp = 2435\n",
      "I1202 11:27:18.849822 139647375513408 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-bce/checkpoint-21000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.788 (cur acc: 0.833)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:27:18.998815 139647375513408 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-bce/checkpoint-21000/pytorch_model.bin\n",
      "I1202 11:27:18.999868 139647375513408 <ipython-input-45-08f342e6a116>:169] Saving model checkpoint to outputs/transformers/binary-bce/checkpoint-21000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.789 (cur acc: 0.667)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:36:56.083919 139647375513408 <ipython-input-40-fa36fc6f9a16>:14] Loading features from cached file cache/transformers/cached_dev_distilbert-base-uncased_512_binary-bce\n",
      "I1202 11:36:56.390810 139647375513408 <ipython-input-43-1a70036edb06>:34] ***** Running evaluation  *****\n",
      "I1202 11:36:56.391785 139647375513408 <ipython-input-43-1a70036edb06>:35]   Num examples = 6391\n",
      "I1202 11:36:56.392370 139647375513408 <ipython-input-43-1a70036edb06>:36]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac9981a0d5943efb0a18bce47eed0d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:37:56.524632 139647375513408 <ipython-input-43-1a70036edb06>:105] ***** Eval results  *****\n",
      "I1202 11:37:56.525334 139647375513408 <ipython-input-43-1a70036edb06>:108]   acc = 0.8139571272101392\n",
      "I1202 11:37:56.525861 139647375513408 <ipython-input-43-1a70036edb06>:108]   f1 = 0.807262117036797\n",
      "I1202 11:37:56.526348 139647375513408 <ipython-input-43-1a70036edb06>:108]   fn = 942\n",
      "I1202 11:37:56.526886 139647375513408 <ipython-input-43-1a70036edb06>:108]   fp = 247\n",
      "I1202 11:37:56.527371 139647375513408 <ipython-input-43-1a70036edb06>:108]   mcc = 0.6469839762623391\n",
      "I1202 11:37:56.527863 139647375513408 <ipython-input-43-1a70036edb06>:108]   tn = 2712\n",
      "I1202 11:37:56.528325 139647375513408 <ipython-input-43-1a70036edb06>:108]   tp = 2490\n",
      "I1202 11:37:56.532478 139647375513408 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-bce/checkpoint-24000/config.json\n",
      "I1202 11:37:56.685458 139647375513408 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-bce/checkpoint-24000/pytorch_model.bin\n",
      "I1202 11:37:56.686423 139647375513408 <ipython-input-45-08f342e6a116>:169] Saving model checkpoint to outputs/transformers/binary-bce/checkpoint-24000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.794 (cur acc: 1.000)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:47:34.895898 139647375513408 <ipython-input-40-fa36fc6f9a16>:14] Loading features from cached file cache/transformers/cached_dev_distilbert-base-uncased_512_binary-bce\n",
      "I1202 11:47:35.207910 139647375513408 <ipython-input-43-1a70036edb06>:34] ***** Running evaluation  *****\n",
      "I1202 11:47:35.208612 139647375513408 <ipython-input-43-1a70036edb06>:35]   Num examples = 6391\n",
      "I1202 11:47:35.209006 139647375513408 <ipython-input-43-1a70036edb06>:36]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da8eabbd05448b5b0a73b86f4dcb0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:48:35.382163 139647375513408 <ipython-input-43-1a70036edb06>:105] ***** Eval results  *****\n",
      "I1202 11:48:35.383088 139647375513408 <ipython-input-43-1a70036edb06>:108]   acc = 0.8184947582537944\n",
      "I1202 11:48:35.383649 139647375513408 <ipython-input-43-1a70036edb06>:108]   f1 = 0.8080741230972865\n",
      "I1202 11:48:35.384167 139647375513408 <ipython-input-43-1a70036edb06>:108]   fn = 990\n",
      "I1202 11:48:35.384637 139647375513408 <ipython-input-43-1a70036edb06>:108]   fp = 170\n",
      "I1202 11:48:35.385097 139647375513408 <ipython-input-43-1a70036edb06>:108]   mcc = 0.6634471838996666\n",
      "I1202 11:48:35.385558 139647375513408 <ipython-input-43-1a70036edb06>:108]   tn = 2789\n",
      "I1202 11:48:35.386013 139647375513408 <ipython-input-43-1a70036edb06>:108]   tp = 2442\n",
      "I1202 11:48:35.390414 139647375513408 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-bce/checkpoint-27000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.794 (cur acc: 0.667)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:48:35.542763 139647375513408 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-bce/checkpoint-27000/pytorch_model.bin\n",
      "I1202 11:48:35.543758 139647375513408 <ipython-input-45-08f342e6a116>:169] Saving model checkpoint to outputs/transformers/binary-bce/checkpoint-27000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.797 (cur acc: 0.833)\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 3/3 [1:41:21<00:00, 2028.21s/it]\n",
      "I1202 11:54:12.752272 139647375513408 <ipython-input-40-fa36fc6f9a16>:14] Loading features from cached file cache/transformers/cached_dev_distilbert-base-uncased_512_binary-bce\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:54:13.070953 139647375513408 <ipython-input-43-1a70036edb06>:34] ***** Running evaluation  *****\n",
      "I1202 11:54:13.071983 139647375513408 <ipython-input-43-1a70036edb06>:35]   Num examples = 6391\n",
      "I1202 11:54:13.072525 139647375513408 <ipython-input-43-1a70036edb06>:36]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb35ceb99044fa2acc44044233aa487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 11:55:13.198882 139647375513408 <ipython-input-43-1a70036edb06>:105] ***** Eval results  *****\n",
      "I1202 11:55:13.199559 139647375513408 <ipython-input-43-1a70036edb06>:108]   acc = 0.8166171178219371\n",
      "I1202 11:55:13.200152 139647375513408 <ipython-input-43-1a70036edb06>:108]   f1 = 0.8080576482148705\n",
      "I1202 11:55:13.200648 139647375513408 <ipython-input-43-1a70036edb06>:108]   fn = 965\n",
      "I1202 11:55:13.201112 139647375513408 <ipython-input-43-1a70036edb06>:108]   fp = 207\n",
      "I1202 11:55:13.201574 139647375513408 <ipython-input-43-1a70036edb06>:108]   mcc = 0.6558804390511694\n",
      "I1202 11:55:13.202028 139647375513408 <ipython-input-43-1a70036edb06>:108]   tn = 2752\n",
      "I1202 11:55:13.202485 139647375513408 <ipython-input-43-1a70036edb06>:108]   tp = 2467\n",
      "I1202 11:55:13.203050 139647375513408 <ipython-input-46-e48efd1a75ca>:16]  global_step = 28758, average loss = 0.37328667078631256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [3 - train (fine-tune) model]: 1:42:26.806061\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "if args['do_train']:\n",
    "    write_eval_setup_args(args)\n",
    "    with Timer(\"3 - train (fine-tune) model\"):\n",
    "        if args.get('is_i2o', False):\n",
    "            _ = evaluate_i2o(model, tokenizer, args, ds_train, ds_dev)\n",
    "            train_dataset = load_and_cache_examples_i2o(ds_train, ds_dev, args, tokenizer)\n",
    "            global_step, tr_loss = train_i2o(train_dataset, model, tokenizer, args, ds_train=ds_train, ds_dev=ds_dev)\n",
    "            _ = evaluate_i2o(model, tokenizer, args, ds_train, ds_dev)\n",
    "\n",
    "        else:\n",
    "            train_dataset = load_and_cache_examples(ds_train, ds_dev, args, tokenizer)\n",
    "            global_step, tr_loss = train(train_dataset, model, tokenizer, args, ds_train=ds_train, ds_dev=ds_dev)\n",
    "            _ = evaluate(model, tokenizer, args, ds_train, ds_dev)\n",
    "\n",
    "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T11:27:10.325053Z",
     "start_time": "2019-12-02T11:27:10.132237Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 12:27:10.138892 139647375513408 <ipython-input-47-94c486dab606>:6] Saving model checkpoint to outputs/transformers/binary-bce\n",
      "I1202 12:27:10.141351 139647375513408 configuration_utils.py:71] Configuration saved in outputs/transformers/binary-bce/config.json\n",
      "I1202 12:27:10.308016 139647375513408 modeling_utils.py:205] Model weights saved in outputs/transformers/binary-bce/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# saving\n",
    "if args['do_train']:\n",
    "    if not os.path.exists(args['output_dir']):\n",
    "        os.makedirs(args['output_dir'])\n",
    "    \n",
    "    logger.info(\"Saving model checkpoint to %s\", args['output_dir'])\n",
    "\n",
    "    model_to_save = model.module if hasattr(\n",
    "        model,\n",
    "        'module') else model  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(args['output_dir'])\n",
    "    tokenizer.save_pretrained(args['output_dir'])\n",
    "    torch.save(args, os.path.join(args['output_dir'], 'training_args.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T11:27:13.537633Z",
     "start_time": "2019-12-02T11:27:13.535108Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(args['output_dir'], 'done.flag'), \"w\") as fp:\n",
    "    fp.write(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T11:30:10.801512Z",
     "start_time": "2019-12-02T11:30:09.114121Z"
    },
    "code_folding": [
     6
    ],
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 12:30:09.601337 139647375513408 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-uncased-config.json from cache at /home/ekoerner/.cache/torch/transformers/a41e817d5c0743e29e86ff85edc8c257e61bc8d88e4271bb1b243b6e7614c633.1ccd1a11c9ff276830e114ea477ea2407100f4a3be7bdc45d37be9e37fa71c7e\n",
      "I1202 12:30:09.603906 139647375513408 configuration_utils.py:168] Model config {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"binary-bce\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1202 12:30:09.605185 139647375513408 tokenization_utils.py:306] Model name 'outputs/transformers/binary-bce' not found in model shortcut name list (distilbert-base-uncased, distilbert-base-uncased-distilled-squad). Assuming 'outputs/transformers/binary-bce' is a path or url to a directory containing tokenizer files.\n",
      "I1202 12:30:09.606924 139647375513408 tokenization_utils.py:371] loading file outputs/transformers/binary-bce/vocab.txt\n",
      "I1202 12:30:09.607881 139647375513408 tokenization_utils.py:371] loading file outputs/transformers/binary-bce/added_tokens.json\n",
      "I1202 12:30:09.608834 139647375513408 tokenization_utils.py:371] loading file outputs/transformers/binary-bce/special_tokens_map.json\n",
      "I1202 12:30:09.609743 139647375513408 tokenization_utils.py:371] loading file outputs/transformers/binary-bce/tokenizer_config.json\n",
      "I1202 12:30:09.649066 139647375513408 configuration_utils.py:148] loading configuration file outputs/transformers/binary-bce/config.json\n",
      "I1202 12:30:09.650470 139647375513408 configuration_utils.py:168] Model config {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I1202 12:30:09.651422 139647375513408 modeling_utils.py:334] loading weights file outputs/transformers/binary-bce/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSameSideBCEClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (dropout): Dropout(p=0.1)\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.2)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading\n",
    "model_dir = args['output_dir']  # this should probably be somehow set to load the args/config\n",
    "args = torch.load(os.path.join(model_dir, 'training_args.bin'))\n",
    "\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]\n",
    "\n",
    "# is not saved?\n",
    "config = config_class.from_pretrained(args['model_name'],\n",
    "                                      num_labels=args['num_labels'],\n",
    "                                      finetuning_task=args['task_name'])\n",
    "\n",
    "# tokenizer.save_pretrained(args['output_dir'])\n",
    "tokenizer = tokenizer_class.from_pretrained(args['output_dir'])\n",
    "\n",
    "# model_to_load = model.module if hasattr(\n",
    "#     model,\n",
    "#     'module') else model  # Take care of distributed/parallel training\n",
    "# model_to_save.save_pretrained(args['output_dir'])\n",
    "model = model_class.from_pretrained(args['output_dir'], num_labels=args['num_labels'])\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T11:31:31.706145Z",
     "start_time": "2019-12-02T11:30:32.090731Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 12:30:32.093506 139647375513408 <ipython-input-40-fa36fc6f9a16>:14] Loading features from cached file cache/transformers/cached_dev_distilbert-base-uncased_512_binary-bce\n",
      "I1202 12:30:32.740238 139647375513408 <ipython-input-43-1a70036edb06>:34] ***** Running evaluation  *****\n",
      "I1202 12:30:32.741215 139647375513408 <ipython-input-43-1a70036edb06>:35]   Num examples = 6391\n",
      "I1202 12:30:32.741795 139647375513408 <ipython-input-43-1a70036edb06>:36]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ecd74ca11df46d89f10a61bf40fb278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 12:31:31.700134 139647375513408 <ipython-input-43-1a70036edb06>:105] ***** Eval results  *****\n",
      "I1202 12:31:31.700829 139647375513408 <ipython-input-43-1a70036edb06>:108]   acc = 0.8166171178219371\n",
      "I1202 12:31:31.701335 139647375513408 <ipython-input-43-1a70036edb06>:108]   f1 = 0.8080576482148705\n",
      "I1202 12:31:31.701820 139647375513408 <ipython-input-43-1a70036edb06>:108]   fn = 965\n",
      "I1202 12:31:31.702291 139647375513408 <ipython-input-43-1a70036edb06>:108]   fp = 207\n",
      "I1202 12:31:31.702757 139647375513408 <ipython-input-43-1a70036edb06>:108]   mcc = 0.6558804390511694\n",
      "I1202 12:31:31.703285 139647375513408 <ipython-input-43-1a70036edb06>:108]   tn = 2752\n",
      "I1202 12:31:31.703779 139647375513408 <ipython-input-43-1a70036edb06>:108]   tp = 2467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_ = evaluate(model, tokenizer, args, ds_train, ds_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-02T11:28:27.805925Z",
     "start_time": "2019-12-02T11:27:28.647468Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 12:27:28.669451 139647375513408 <ipython-input-40-fa36fc6f9a16>:14] Loading features from cached file cache/transformers/cached_dev_distilbert-base-uncased_512_binary-bce\n",
      "I1202 12:27:29.013592 139647375513408 <ipython-input-43-1a70036edb06>:34] ***** Running evaluation  *****\n",
      "I1202 12:27:29.014484 139647375513408 <ipython-input-43-1a70036edb06>:35]   Num examples = 6391\n",
      "I1202 12:27:29.015063 139647375513408 <ipython-input-43-1a70036edb06>:36]   Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec13b7671704bbd814f81f98c731d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=200, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1202 12:28:27.800076 139647375513408 <ipython-input-43-1a70036edb06>:105] ***** Eval results  *****\n",
      "I1202 12:28:27.800774 139647375513408 <ipython-input-43-1a70036edb06>:108]   acc = 0.8166171178219371\n",
      "I1202 12:28:27.801353 139647375513408 <ipython-input-43-1a70036edb06>:108]   f1 = 0.8080576482148705\n",
      "I1202 12:28:27.801861 139647375513408 <ipython-input-43-1a70036edb06>:108]   fn = 965\n",
      "I1202 12:28:27.802338 139647375513408 <ipython-input-43-1a70036edb06>:108]   fp = 207\n",
      "I1202 12:28:27.802808 139647375513408 <ipython-input-43-1a70036edb06>:108]   mcc = 0.6558804390511694\n",
      "I1202 12:28:27.803338 139647375513408 <ipython-input-43-1a70036edb06>:108]   tn = 2752\n",
      "I1202 12:28:27.803905 139647375513408 <ipython-input-43-1a70036edb06>:108]   tp = 2467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# reverse argument order\n",
    "# --> results almost seems the same!\n",
    "\n",
    "ds_dev = df2ds_rev(X_dev, y_dev)\n",
    "# remove cached dev file\n",
    "_ = evaluate(model, tokenizer, args, ds_train, ds_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, wrong = evaluate(model, tokenizer, args, ds_train, ds_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, out_label_ids = get_train_output(model, tokenizer, args, ds_train, ds_dev, prefix=\"\", evaluate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(preds, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = out_label_ids\n",
    "confusion_matrix(labels, preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = np.array([-1.0, -0.1, 0.1, 0.4, 0.5, 0.6, 1.0, 2.0])\n",
    "vals_s = sigmoid(vals)\n",
    "vals_s_r = vals_s.round()\n",
    "\n",
    "vals, vals_s, vals_s_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jfkdöljfkdlasöjfkld\n",
    "# abort here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2
    ],
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # BERT MXNet setup\n",
    "    def setup_bert():\n",
    "        # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "        ctx = mx.gpu(0)\n",
    "        # ctx = [mx.gpu(i) for i in range(2)]\n",
    "        # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "        # ctx = mx.cpu()\n",
    "\n",
    "        bert_base, vocabulary = nlp.model.get_model(\n",
    "            'bert_12_768_12',\n",
    "            dataset_name='book_corpus_wiki_en_uncased',\n",
    "            pretrained=True,\n",
    "            ctx=ctx,\n",
    "            use_pooler=True,\n",
    "            use_decoder=False,\n",
    "            use_classifier=False)\n",
    "        print(bert_base)\n",
    "\n",
    "        #model = BERTProEpiClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "        model = BERTProEpiClassifier(bert_base, num_classes=1, dropout=0.1)\n",
    "        # only need to initialize the classifier layer.\n",
    "        model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "        model.hybridize(static_alloc=True)\n",
    "\n",
    "        # softmax cross entropy loss for classification\n",
    "        #loss_function = gluon.loss.SoftmaxCELoss()\n",
    "        loss_function = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "        loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "        metric = mx.metric.Accuracy()\n",
    "\n",
    "        # use the vocabulary from pre-trained model for tokenization\n",
    "        bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "        # maximum sequence length\n",
    "        # max_len = 128  # + batch_size: 32\n",
    "        # 384 - 12\n",
    "        max_len = 512  # + batch_size: 6 ?\n",
    "        # the labels for the two classes\n",
    "        #all_labels = [\"0\", \"1\"]\n",
    "        all_labels = [0, 1]\n",
    "        # whether to transform the data as sentence pairs.\n",
    "        # for single sentence classification, set pair=False\n",
    "        transform = FirstAndLastPartBERTDatasetTransform(bert_tokenizer,\n",
    "                                                         max_len,\n",
    "                                                         labels=all_labels,\n",
    "                                                         label_dtype='int32',\n",
    "                                                         pad=True,\n",
    "                                                         pair=True)\n",
    "\n",
    "        return model, vocabulary, ctx, bert_tokenizer, transform, loss_function, metric, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2,
     14,
     18,
     21,
     38,
     91
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # BERT MXNet train\n",
    "    def train(model,\n",
    "              data_train,\n",
    "              ctx,\n",
    "              metric,\n",
    "              loss_function,\n",
    "              batch_size=32,\n",
    "              lr=5e-6,\n",
    "              num_epochs=3,\n",
    "              sw=None,\n",
    "              checkpoint_dir=\"data\",\n",
    "              use_checkpoints=True):\n",
    "        with Timer(\"setup training\"):\n",
    "            train_sampler = nlp.data.FixedBucketSampler(\n",
    "                lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True)\n",
    "            bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                       batch_sampler=train_sampler)\n",
    "\n",
    "            trainer = gluon.Trainer(model.collect_params(), 'adam', {\n",
    "                'learning_rate': lr,\n",
    "                'epsilon': 1e-9\n",
    "            })\n",
    "\n",
    "            # collect all differentiable parameters\n",
    "            # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "            # the gradients for these params are clipped later\n",
    "            params = [\n",
    "                p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "            ]\n",
    "\n",
    "        log_interval = 500\n",
    "        global_step = 0\n",
    "        with Timer(\"training\"):\n",
    "            stats = list()\n",
    "            for epoch_id in range(num_epochs):\n",
    "                if use_checkpoints:\n",
    "                    epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                        epoch_id)\n",
    "                    if checkpoint_dir is not None:\n",
    "                        epoch_checkpoint_savefile = os.path.join(\n",
    "                            checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                    if os.path.exists(epoch_checkpoint_savefile):\n",
    "                        model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                        print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                        continue\n",
    "\n",
    "                with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                    metric.reset()\n",
    "                    step_loss = 0\n",
    "                    global_step = epoch_id * len(bert_dataloader)\n",
    "                    t_p = time.time()  # time keeping\n",
    "                    for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                                   token_ids_epi, valid_length_epi,\n",
    "                                   segment_ids_epi,\n",
    "                                   label) in enumerate(tqdm(bert_dataloader)):\n",
    "                        global_step += 1\n",
    "                        with mx.autograd.record():\n",
    "                            # load data to GPU\n",
    "                            token_ids = token_ids.as_in_context(ctx)\n",
    "                            valid_length = valid_length.as_in_context(ctx)\n",
    "                            segment_ids = segment_ids.as_in_context(ctx)\n",
    "                            token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "                            valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "                            segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "                            label = label.as_in_context(ctx)\n",
    "\n",
    "                            # forward computation\n",
    "                            out = model(token_ids, segment_ids,\n",
    "                                        valid_length.astype('float32'),\n",
    "                                        token_ids_epi, segment_ids_epi,\n",
    "                                        valid_length_epi.astype('float32'))\n",
    "                            label = label.astype('float32')\n",
    "                            ls = loss_function(out, label).mean()\n",
    "\n",
    "                        # backward computation\n",
    "                        ls.backward()\n",
    "\n",
    "                        # gradient clipping\n",
    "                        trainer.allreduce_grads()\n",
    "                        nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                        trainer.update(1)\n",
    "\n",
    "                        step_loss += ls.asscalar()\n",
    "                        out = out.sigmoid().round().astype('int32')\n",
    "                        label = label.astype('int32')\n",
    "                        metric.update([label], [out])\n",
    "                        stats.append((metric.get()[1], ls.asscalar()))\n",
    "\n",
    "                        if sw:\n",
    "                            sw.add_scalar(tag='T-ls', value=ls.asscalar(), global_step=global_step)\n",
    "                            sw.add_scalar(tag='T-acc', value=metric.get()[1], global_step=global_step)\n",
    "\n",
    "                        if (batch_id + 1) % (log_interval) == 0:\n",
    "                            print(\n",
    "                                '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                                .format(\n",
    "                                    epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                    step_loss / log_interval,\n",
    "                                    trainer.learning_rate,\n",
    "                                    metric.get()[1],\n",
    "                                    datetime.timedelta(seconds=(time.time() -\n",
    "                                                                t_p))))\n",
    "                            t_p = time.time()\n",
    "                            step_loss = 0\n",
    "\n",
    "                if use_checkpoints:\n",
    "                    model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     2
    ]
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # BERT MXNet predict\n",
    "    def predict_unknown(model, data_predict, ctx, label_map=None, batch_size=32):\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_predict,\n",
    "                                                   batch_size=batch_size)\n",
    "\n",
    "        predictions = list()\n",
    "\n",
    "        with Timer(\"prediction\"):\n",
    "            for batch_id, (token_ids, valid_length, segment_ids, token_ids_epi,\n",
    "                           valid_length_epi,\n",
    "                           segment_ids_epi) in enumerate(tqdm(bert_dataloader)):\n",
    "                global_step = batch_id\n",
    "                # load data to GPU\n",
    "                token_ids = token_ids.as_in_context(ctx)\n",
    "                valid_length = valid_length.as_in_context(ctx)\n",
    "                segment_ids = segment_ids.as_in_context(ctx)\n",
    "                token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "                valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "                segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "\n",
    "                # forward computation\n",
    "                out = model(token_ids, segment_ids, valid_length.astype('float32'),\n",
    "                            token_ids_epi, segment_ids_epi,\n",
    "                            valid_length_epi.astype('float32'))\n",
    "\n",
    "                # to binary: 0/1\n",
    "                out = out.sigmoid().round().astype('int32')\n",
    "                # to numpy (not mxnet)\n",
    "                out = out.asnumpy()\n",
    "                # get mapping type\n",
    "                if label_map:\n",
    "                    out = [label_map[c] for c in list(out)]\n",
    "\n",
    "                predictions.extend(out)\n",
    "\n",
    "        # list to numpy array\n",
    "        predictions = np.array(predictions)\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
