{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets (Sentiment) - Yelp - Per Side"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp\n",
    "\n",
    "- https://www.yelp.com/dataset/documentation/main\n",
    "- https://www.yelp.com/developers/documentation/v3/all_category_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:14.905825Z",
     "start_time": "2020-05-25T09:52:14.315598Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk1/users/ekoerner/miniconda3/envs/argmining19-ssc/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from copy import deepcopy\n",
    "from itertools import combinations, groupby\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import jsonlines\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import scipy.spatial.distance as ssd\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from scipy.cluster import hierarchy\n",
    "from sklearn.cluster import AgglomerativeClustering, SpectralClustering\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from syntok.tokenizer import Tokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({\"font.size\": 25, \"font.weight\": \"bold\"})\n",
    "# https://stackoverflow.com/questions/36622237/jupyter-notebook-inline-plots-as-svg\n",
    "# set_matplotlib_formats(\"svg\")  # glitches it\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ekoerner/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/ekoerner/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ekoerner/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from utils_data import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:15.648956Z",
     "start_time": "2020-05-25T09:52:15.148633Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils_data import init_random\n",
    "init_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:15.790984Z",
     "start_time": "2020-05-25T09:52:15.786910Z"
    }
   },
   "outputs": [],
   "source": [
    "# download + scp to cuda + extract\n",
    "# Path on CUDA2\n",
    "data_yelp_path = Path(\"/disk1/users/ekoerner/argmining19-same-side-classification/data/sentiment/yelp/\")\n",
    "# CUDA1?\n",
    "#data_yelp_path = Path(\"/disk1/users/ekoerner/same-side-classification/argmining19-same-side-classification/data/sentiment/yelp/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load reviews / categories (topics) + filter cats/make combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:19.846925Z",
     "start_time": "2020-05-25T09:52:19.827613Z"
    },
    "code_folding": [
     0,
     28
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_reviews(fn_yelp_reviews):\n",
    "    \"\"\"Load Yelp reviews. Return a Pandas dataframe.\n",
    "    Format: {\"id\": business_id, \"rid\": review_id, \"text\": text, \"rating\": rating}\n",
    "    And add goodness bool (rating > 3).\"\"\"\n",
    "    data = list()\n",
    "\n",
    "    with jsonlines.open(fn_yelp_reviews, \"r\") as fp:\n",
    "        for n, entry in enumerate(tqdm(fp)):\n",
    "            # print(entry)\n",
    "            # break\n",
    "            business_id = entry.pop(\"business_id\")\n",
    "            review_id = entry.pop(\"review_id\")\n",
    "            text = entry.pop(\"text\", \"\")\n",
    "            rating = entry.pop(\"stars\", 0.0)\n",
    "            data.append({\"id\": business_id, \"rid\": review_id, \"text\": text, \"rating\": rating})\n",
    "            # TESTING\n",
    "            # if n > 10000:\n",
    "            #     break\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data)\n",
    "    \n",
    "    # Add goodness value\n",
    "    # TODO: maybe ignore with 3\n",
    "    df[\"goodness\"] = df[\"rating\"] > 3\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_topics(fn_yelp_topics, bids_not_cats=None, filter_cats=None, filter_cat_combis=None):\n",
    "    \"\"\"Load topics (categories).\n",
    "    Optionally filter by giving it a whitelist of allowed categories.\n",
    "    Optionally filter by giving a list of allowed category combinations.\n",
    "    Optionally provide an IN/OUT param ``bids_not_cats`` that accumulates business ids without categories.\"\"\"\n",
    "    inv_bid_cats = dict()\n",
    "    if bids_not_cats is None or not isinstance(bids_not_cats, set):\n",
    "        bids_not_cats = set()\n",
    "    \n",
    "    # load lookup business -> categories\n",
    "    with jsonlines.open(fn_yelp_topics, \"r\") as fp:\n",
    "        for n, entry in enumerate(tqdm(fp)):\n",
    "            business_id = entry.pop(\"business_id\")\n",
    "            categories = entry.pop(\"categories\")\n",
    "\n",
    "            if not categories:\n",
    "                bids_not_cats.add(business_id)\n",
    "                continue\n",
    "\n",
    "            categories = categories.split(\", \")\n",
    "            \n",
    "            if filter_cats:\n",
    "                categories = [c for c in categories if c in filter_cats]\n",
    "                if not categories:\n",
    "                    # bids_not_cats.add(business_id)  # ??\n",
    "                    continue\n",
    "                    \n",
    "            if filter_cat_combis:\n",
    "                # skip if combination is not above threshold (of filter list)\n",
    "                if tuple(sorted(set(categories))) not in filter_cat_combis:\n",
    "                    continue\n",
    "\n",
    "            inv_bid_cats[business_id] = categories\n",
    "    \n",
    "    return inv_bid_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:20.111088Z",
     "start_time": "2020-05-25T09:52:20.098533Z"
    },
    "code_folding": [
     0,
     11,
     30
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_min_cat_combis(inv_cat_combis, min_num=30):\n",
    "    \"\"\"Filter category combinations by minimum amount of occurrences in businesses\"\"\"\n",
    "    f_inv_cat_combis = dict()\n",
    "    \n",
    "    for cats, num in inv_cat_combis.items():\n",
    "        if num >= min_num:\n",
    "            f_inv_cat_combis[cats] = num\n",
    "    \n",
    "    return f_inv_cat_combis\n",
    "\n",
    "\n",
    "def make_map_cats(inv_bid_cats):\n",
    "    \"\"\"Make a map from category to business id\"\"\"\n",
    "    inv_cat_bids = dict()\n",
    "\n",
    "    # reverse lookup: category -> businesses\n",
    "    for bid, cats in tqdm(inv_bid_cats.items()):\n",
    "        for cat in cats:\n",
    "            try:\n",
    "                inv_cat_bids[cat].append(bid)\n",
    "            except KeyError:\n",
    "                inv_cat_bids[cat] = [bid]\n",
    "                \n",
    "    ## TODO: make distinct?\n",
    "    for cat in inv_cat_bids.keys():\n",
    "        inv_cat_bids[cat] = list(set(inv_cat_bids[cat]))\n",
    "                \n",
    "    return inv_cat_bids\n",
    "\n",
    "\n",
    "def make_cat_combis(inv_bid_cats):\n",
    "    \"\"\"Count amount of each category combination occurring in businesses\"\"\"\n",
    "    inv_cat_combis = Counter()\n",
    "\n",
    "    inv_cat_combis.update(\n",
    "        (tuple(sorted(set(cats))) for cats in tqdm(inv_bid_cats.values())))\n",
    "    \n",
    "    return inv_cat_combis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter reviews according to criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:22.183788Z",
     "start_time": "2020-05-25T09:52:22.171415Z"
    },
    "code_folding": [
     0,
     16
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_min_review_freq(df, min_ratings=5):\n",
    "    \"\"\"Filter review dataframe for a minimum of N of each good and bad ratings.\"\"\"\n",
    "    # filter with at least N ratings per goodness\n",
    "    df_filter = df.groupby([\"id\", \"goodness\"])[[\"id\"]].count() < min_ratings\n",
    "    df_filter = df_filter.rename(columns={\"id\": \"filter\"})\n",
    "    df_filter = df_filter[df_filter[\"filter\"] == True]\n",
    "\n",
    "    # build a filter id list\n",
    "    df_filter_list = df_filter.reset_index()[\"id\"].to_list()\n",
    "\n",
    "    # filter with list\n",
    "    df_filtered = df[~df.id.isin(df_filter_list)]\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def filter_both_good_bad(df):\n",
    "    \"\"\"Filter the dataframe to contain only both good and bad reviews for each business.\n",
    "    Dataframe should be the same if minimum filtering above is done.\"\"\"\n",
    "    # build filter for ids that contain both positive and negative samples\n",
    "    df_filter = df.groupby([\"id\", \"goodness\"], as_index=False).count().groupby(\"id\")[[\"id\"]].count() == 2\n",
    "    df_filter = df_filter.rename(columns={\"id\": \"filter\"})\n",
    "    df_filter = df_filter[df_filter[\"filter\"] == True]\n",
    "\n",
    "    # create list of IDs for which this is true\n",
    "    df_filter_list = df_filter.reset_index()[\"id\"].to_list()\n",
    "\n",
    "    # filter with list\n",
    "    df_filtered = df[df.id.isin(df_filter_list)]\n",
    "\n",
    "    # df_filtered.groupby([\"id\", \"goodness\"]).count()\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:22.384186Z",
     "start_time": "2020-05-25T09:52:22.377841Z"
    },
    "code_folding": [
     0,
     7
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_by_businesses(df, lst_business_ids):\n",
    "    # filter with list, keep businesses in list\n",
    "    df_filtered = df[df.id.isin(set(lst_business_ids))]\n",
    "\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "def filter_by_businesses_not_same(df, lst_business_ids):\n",
    "    # filter with list, keep businesses that are not in list\n",
    "    df_filtered = df[~df.id.isin(set(lst_business_ids))]\n",
    "\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get topN categories + make Ntuples from category combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:22.837095Z",
     "start_time": "2020-05-25T09:52:22.827303Z"
    },
    "code_folding": [
     0,
     7
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_Ntop_cats(inv_cat_bids, n=50):\n",
    "    # get most common cats\n",
    "    f_cat_cnt = Counter({k: len(v) for k, v in inv_cat_bids.items()})\n",
    "    f_cats = {c for c, v in f_cat_cnt.most_common(n)}\n",
    "    return f_cats\n",
    "\n",
    "\n",
    "def make_cat_Ntuples(f_inv_cat_combis, n=2):\n",
    "    f_cat_pairs = Counter()\n",
    "\n",
    "    for cat_group in tqdm(f_inv_cat_combis.keys()):\n",
    "        if len(cat_group) < n:\n",
    "            continue\n",
    "        it = combinations(cat_group, n)\n",
    "        # repeat (#num_businesses) + chain combis\n",
    "        f_cat_pairs.update(it)\n",
    "        \n",
    "    return f_cat_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make category graph or NxN map (df + array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:24.807433Z",
     "start_time": "2020-05-25T09:52:24.793721Z"
    },
    "code_folding": [
     0,
     9
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_graph(f_cat_pairs):\n",
    "    g_from, g_to, g_value = zip(*((k1, k2, n) for (k1, k2), n in tqdm(f_cat_pairs.most_common())))\n",
    "\n",
    "    g_df = pd.DataFrame({\"from\": g_from, \"to\": g_to, \"value\": g_value})\n",
    "    G = nx.from_pandas_edgelist(g_df, \"from\", \"to\", create_using=nx.Graph())\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "def make_NxN_map(f_cats, f_cat_pairs):\n",
    "    f_cats = list(f_cats)\n",
    "    array = list()\n",
    "    for i, cat1 in enumerate(tqdm(f_cats)):\n",
    "        array_row = list()\n",
    "        for j, cat2 in enumerate(f_cats):\n",
    "            array_row.append(f_cat_pairs.get((cat1, cat2), f_cat_pairs.get((cat2, cat1), 0)))\n",
    "        array.append(array_row)\n",
    "    df_cm = pd.DataFrame(array, index=list(f_cats), columns=list(f_cats))\n",
    "    \n",
    "    # dataframe, NxN array + labels\n",
    "    return df_cm, array, f_cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load categories hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:25.582403Z",
     "start_time": "2020-05-25T09:52:25.560666Z"
    },
    "code_folding": [
     0,
     30,
     40
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_category_tree(fn_all_category_list):\n",
    "    with open(fn_all_category_list, \"r\") as fp:\n",
    "        content = fp.read()\n",
    "        data = json.loads(content)\n",
    "\n",
    "    map_categories = dict()\n",
    "    map_cat_name2id = dict()\n",
    "    lst_root_categories = list()\n",
    "\n",
    "    # load basic lookups\n",
    "    for item in data:\n",
    "        # .alias (id)\n",
    "        map_categories[item[\"alias\"]] = item\n",
    "        # .title\n",
    "        map_cat_name2id[item[\"title\"]] = item[\"alias\"]\n",
    "        # .parents\n",
    "        # some have multiple parents ...\n",
    "        if not item[\"parents\"]:\n",
    "            lst_root_categories.append(item[\"alias\"])\n",
    "        # add list of children\n",
    "        item[\"children\"] = list()\n",
    "\n",
    "    # add children\n",
    "    for cid, item in map_categories.items():\n",
    "        for parent_cid in item[\"parents\"]:\n",
    "            map_categories[parent_cid][\"children\"].append(item[\"alias\"])\n",
    "\n",
    "    return map_categories, map_cat_name2id, lst_root_categories\n",
    "\n",
    "\n",
    "def get_root_category_items(map_categories):\n",
    "    lst_root_categories = list()\n",
    "\n",
    "    for cid, item in map_categories.items():\n",
    "        if not item[\"parents\"]:\n",
    "            lst_root_categories.append(item)\n",
    "            \n",
    "    return lst_root_categories\n",
    "\n",
    "\n",
    "def get_children_category_item_list(map_categories, parent_cid):\n",
    "    return [\n",
    "        map_categories[child_cid]\n",
    "        for child_cid in map_categories[parent_cid][\"children\"]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get businesses in categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:28.177244Z",
     "start_time": "2020-05-25T09:52:28.167066Z"
    },
    "code_folding": [
     0,
     7
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_businesses_in_category(inv_cat_bids, category):\n",
    "    try:\n",
    "        return list(set(inv_cat_bids[category]))\n",
    "    except KeyError:\n",
    "        return []\n",
    "    \n",
    "    \n",
    "def get_businesses_in_category_branch(inv_cat_bids, category, map_categories, map_cat_name2id):\n",
    "    map_cat_id2name = {cid: name for name, cid in map_cat_name2id.items()}\n",
    "\n",
    "    def _get_recursive_businesses(cat_name):\n",
    "        businesses = get_businesses_in_category(inv_cat_bids, cat_name)\n",
    "\n",
    "        cid = map_cat_name2id[cat_name]\n",
    "        for child_cid in map_categories[cid][\"children\"]:\n",
    "            child_name = map_cat_id2name[child_cid]\n",
    "            businesses.extend(_get_recursive_businesses(child_name))\n",
    "        \n",
    "        return businesses\n",
    "    \n",
    "    return _get_recursive_businesses(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Print category trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:29.242019Z",
     "start_time": "2020-05-25T09:52:29.205730Z"
    },
    "code_folding": [
     0,
     19,
     49,
     85,
     106
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def print_category_tree(map_categories):\n",
    "    root_categories = get_root_category_items(map_categories)\n",
    "    \n",
    "    def _print_cat_list_rec(lst_cats, level=0):\n",
    "        for item in sorted(lst_cats, key=lambda x: x[\"title\"]):\n",
    "            if level:\n",
    "                print(\"  \" * level, end=\"\")\n",
    "            print(f\"\"\"{item[\"title\"]} [{item[\"alias\"]}]\"\"\", end=\"\")\n",
    "            if item[\"children\"]:\n",
    "                print(f\"\"\" [#{len(item[\"children\"])} children]\"\"\")\n",
    "            else:\n",
    "                print()\n",
    "            \n",
    "            children = get_children_category_item_list(map_categories, item[\"alias\"])\n",
    "            _print_cat_list_rec(children, level=level + 1)\n",
    "            \n",
    "    _print_cat_list_rec(root_categories, level=0)\n",
    "\n",
    "\n",
    "def print_category_tree_with_num_businesses(map_categories, inv_cat_bids):\n",
    "    root_categories = get_root_category_items(map_categories)\n",
    "    \n",
    "    def _print_cat_list_rec(lst_cats, level=0):\n",
    "        for item in sorted(lst_cats, key=lambda x: x[\"title\"]):\n",
    "            cur_line = \" .\" * 30\n",
    "            parts = list()\n",
    "\n",
    "            if level:\n",
    "                parts.append(\"  \" * level)\n",
    "            parts.append(f\"\"\"{item[\"title\"]} [{item[\"alias\"]}]\"\"\")\n",
    "            \n",
    "            str_len = sum(len(part) for part in parts)\n",
    "            print(\"\".join(part for part in parts), end=\"\")\n",
    "            print(cur_line[str_len:], end=\"\")\n",
    "            \n",
    "            if item[\"title\"] not in inv_cat_bids:\n",
    "                print(\" No businesses associated!\")\n",
    "            else:\n",
    "                print(f\"\"\" {len((inv_cat_bids[item[\"title\"]])):>5d} businesses\"\"\")\n",
    "            \n",
    "            children = get_children_category_item_list(map_categories, item[\"alias\"])\n",
    "            _print_cat_list_rec(children, level=level + 1)\n",
    "            \n",
    "            if level == 0:\n",
    "                print()\n",
    "            \n",
    "    _print_cat_list_rec(root_categories, level=0)\n",
    "    \n",
    "\n",
    "def print_category_tree_with_num_businesses_rec(map_categories, inv_cat_bids, map_cat_name2id):\n",
    "    root_categories = get_root_category_items(map_categories)\n",
    "    \n",
    "    def _print_cat_list_rec(lst_cats, level=0):\n",
    "        for item in sorted(lst_cats, key=lambda x: x[\"title\"]):\n",
    "            cur_line = \" .\" * 30\n",
    "            parts = list()\n",
    "\n",
    "            if level:\n",
    "                parts.append(\"  \" * level)\n",
    "            parts.append(f\"\"\"{item[\"title\"]} [{item[\"alias\"]}]\"\"\")\n",
    "            \n",
    "            str_len = sum(len(part) for part in parts)\n",
    "            print(\"\".join(part for part in parts), end=\"\")\n",
    "            print(cur_line[str_len:], end=\"\")\n",
    "            \n",
    "            businesses = get_businesses_in_category_branch(inv_cat_bids, item[\"title\"], map_categories, map_cat_name2id)\n",
    "            businesses_self = get_businesses_in_category(inv_cat_bids, item[\"title\"])\n",
    "            if not businesses:\n",
    "                print(\" No businesses associated!\")\n",
    "            else:\n",
    "                businesses = set(businesses)\n",
    "                print(f\"\"\" {len(businesses):>5d} businesses\"\"\", end=\"\")\n",
    "                if len(businesses) != len(businesses_self):\n",
    "                    print(f\"\"\" (self: {len(businesses_self)})\"\"\", end=\"\")\n",
    "                print()\n",
    "            \n",
    "            children = get_children_category_item_list(map_categories, item[\"alias\"])\n",
    "            _print_cat_list_rec(children, level=level + 1)\n",
    "            \n",
    "            if level == 0:\n",
    "                print()\n",
    "            \n",
    "    _print_cat_list_rec(root_categories, level=0)\n",
    "    \n",
    "    \n",
    "def print_category_tree_with_num_businesses_root(map_categories, inv_cat_bids, map_cat_name2id):\n",
    "    root_categories = get_root_category_items(map_categories)\n",
    "    \n",
    "    for item in sorted(root_categories, key=lambda x: x[\"title\"]):\n",
    "        cur_line = \" .\" * 25\n",
    "        parts = [f\"\"\"{item[\"title\"]} [{item[\"alias\"]}] \"\"\"]\n",
    "\n",
    "        str_len = sum(len(part) for part in parts)\n",
    "        print(\"\".join(part for part in parts), end=\"\")\n",
    "        print(cur_line[str_len:], end=\"\")\n",
    "\n",
    "        businesses = get_businesses_in_category_branch(inv_cat_bids, item[\"title\"], map_categories, map_cat_name2id)\n",
    "        businesses_self = get_businesses_in_category(inv_cat_bids, item[\"title\"])\n",
    "\n",
    "        businesses = set(businesses)\n",
    "        print(f\"\"\" {len(businesses):>5d} businesses\"\"\", end=\"\")\n",
    "        if len(businesses) != len(businesses_self):\n",
    "            print(f\"\"\" (self: {len(businesses_self)})\"\"\", end=\"\")\n",
    "        print()\n",
    "        \n",
    "\n",
    "def print_category_tree_with_num_businesses_root2(map_categories, inv_cat_bids, map_cat_name2id):\n",
    "    root_categories = get_root_category_items(map_categories)\n",
    "    for item in root_categories:\n",
    "        item[\"businesses\"] = get_businesses_in_category_branch(inv_cat_bids, item[\"title\"], map_categories, map_cat_name2id)\n",
    "        item[\"businesses_self\"] = get_businesses_in_category(inv_cat_bids, item[\"title\"])\n",
    "    \n",
    "    for item in sorted(root_categories, key=lambda x: len(set(x[\"businesses\"]))):\n",
    "        cur_line = \" .\" * 25\n",
    "        parts = [f\"\"\"{item[\"title\"]} [{item[\"alias\"]}] \"\"\"]\n",
    "\n",
    "        str_len = sum(len(part) for part in parts)\n",
    "        print(\"\".join(part for part in parts), end=\"\")\n",
    "        print(cur_line[str_len:], end=\"\")\n",
    "\n",
    "        businesses = item[\"businesses\"]\n",
    "        businesses_self = item[\"businesses_self\"]\n",
    "\n",
    "        businesses = set(businesses)\n",
    "        print(f\"\"\" {len(businesses):>5d} businesses\"\"\", end=\"\")\n",
    "        if len(businesses) != len(businesses_self):\n",
    "            print(f\"\"\" (self: {len(businesses_self)})\"\"\", end=\"\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make category comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:31.432925Z",
     "start_time": "2020-05-25T09:52:31.423857Z"
    },
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def print_2category_compare(inv_cat_bids, map_categories, map_cat_name2id, cat_name_i, cat_name_j):\n",
    "    businesses_i = get_businesses_in_category_branch(inv_cat_bids, cat_name_i, map_categories, map_cat_name2id)\n",
    "    businesses_j = get_businesses_in_category_branch(inv_cat_bids, cat_name_j, map_categories, map_cat_name2id)\n",
    "    \n",
    "    cat_name_i += \":\"\n",
    "    cat_name_j += \":\"\n",
    "    width = max(12, len(cat_name_i), len(cat_name_j))\n",
    "\n",
    "    print(f\"\"\"{cat_name_i:<{width}} {len(set(businesses_i)):>5d}\"\"\")\n",
    "    print(f\"\"\"{cat_name_j:<{width}} {len(set(businesses_j)):>5d}\"\"\")\n",
    "    print(f\"\"\"Both: {\"same:\":>{width - 6}} {len(set(businesses_i) & set(businesses_j)):>5d}\"\"\")\n",
    "    print(f\"\"\"{\"total:\":>{width}} {len(set(businesses_i) | set(businesses_j)):>5d}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:31.675547Z",
     "start_time": "2020-05-25T09:52:31.663367Z"
    },
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_NxN_category_businesses_overlap(inv_cat_bids, map_categories, map_cat_name2id):\n",
    "    root_categories = get_root_category_items(map_categories)\n",
    "    root_categories = sorted(root_categories, key=lambda x: x[\"title\"])\n",
    "    root_category_labels = [x[\"title\"] for x in root_categories]\n",
    "    \n",
    "    array = list()\n",
    "    for cname_i in root_category_labels:\n",
    "        array_line = list()\n",
    "        for cname_j in root_category_labels:\n",
    "            businesses_i = get_businesses_in_category_branch(inv_cat_bids, cname_i, map_categories, map_cat_name2id)\n",
    "            businesses_j = get_businesses_in_category_branch(inv_cat_bids, cname_j, map_categories, map_cat_name2id)\n",
    "            businesses_i, businesses_j = set(businesses_i), set(businesses_j)\n",
    "            businesses_ij_union = businesses_i | businesses_j\n",
    "            businesses_ij_intersect = businesses_i & businesses_j\n",
    "            num_businesses_ij = len(businesses_ij_intersect)\n",
    "            #array_line.append(num_businesses_ij)\n",
    "            array_line.append(len(businesses_ij_intersect) / len(businesses_ij_union))\n",
    "        array.append(array_line)\n",
    "    \n",
    "    df_cm = pd.DataFrame(array, index=list(root_category_labels), columns=list(root_category_labels))\n",
    "    \n",
    "    return array, root_category_labels, df_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### text length comparisons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:33.641093Z",
     "start_time": "2020-05-25T09:52:33.634436Z"
    },
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_reviews_for_category(df, cat_name, inv_cat_bids, map_categories, map_cat_name2id):\n",
    "    businesses = get_businesses_in_category_branch(inv_cat_bids, cat_name, map_categories, map_cat_name2id)\n",
    "    print(f\"\"\"{cat_name}: {len(businesses)}, uniq: {len(set(businesses))}\"\"\")\n",
    "    businesses = set(businesses)\n",
    "\n",
    "    df_businesses = filter_by_businesses(df, businesses)\n",
    "\n",
    "    return df_businesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cache root category reviews in dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:36.102987Z",
     "start_time": "2020-05-25T09:52:36.087219Z"
    },
    "code_folding": [
     3,
     27
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dn_yelp_cached = data_yelp_path / \"cached\"\n",
    "\n",
    "\n",
    "def cache_root_category_businesses_df(df, inv_cat_bids, map_categories, map_cat_name2id):\n",
    "    root_categories = get_root_category_items(map_categories)\n",
    "    root_categories = sorted(root_categories, key=lambda x: x[\"title\"])\n",
    "    \n",
    "    dn_yelp_cached = data_yelp_path / \"cached\"\n",
    "    if not dn_yelp_cached.exists():\n",
    "        print(f\"Create cache dir: {dn_yelp_cached}\")\n",
    "        dn_yelp_cached.mkdir()\n",
    "        \n",
    "    for root_category in root_categories:\n",
    "        fn_yelp_cached_root_cat_df = dn_yelp_cached / f\"\"\"{root_category[\"alias\"]}_businesses.df.p\"\"\"\n",
    "        if fn_yelp_cached_root_cat_df.exists():\n",
    "            continue\n",
    "\n",
    "        business_ids = set(get_businesses_in_category_branch(inv_cat_bids, root_category[\"title\"], map_categories, map_cat_name2id))\n",
    "        # business_ids = set(root_category[\"businesses\"])\n",
    "        print(f\"\"\"Filter category {root_category[\"title\"]} [{root_category[\"alias\"]}] with {len(set(business_ids))} businesses ...\"\"\")\n",
    "        df_root_cat = filter_by_businesses(df, business_ids)\n",
    "\n",
    "        # df_root_cat = get_reviews_for_category(df, cat_name, inv_cat_bids, map_categories, map_cat_name2id)\n",
    "\n",
    "        df_root_cat.to_pickle(str(fn_yelp_cached_root_cat_df))\n",
    "        \n",
    "        \n",
    "def load_cached_root_category_businesses_df(category_label, map_categories):\n",
    "    root_categories = get_root_category_items(map_categories)\n",
    "    root_categories = sorted(root_categories, key=lambda x: x[\"title\"])\n",
    "\n",
    "    for root_category in root_categories:\n",
    "        if root_category[\"title\"] == category_label:\n",
    "            category_id = root_category[\"alias\"]\n",
    "            break\n",
    "    else:\n",
    "        print(f\"No cached root category businesses found for: {category_label}\")\n",
    "        return None\n",
    "\n",
    "    fn_yelp_cached_root_cat_df = dn_yelp_cached / f\"{category_id}_businesses.df.p\"\n",
    "    if not fn_yelp_cached_root_cat_df.exists():\n",
    "        return None\n",
    "    \n",
    "    df_root_cat = pd.read_pickle(str(fn_yelp_cached_root_cat_df))\n",
    "    return df_root_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filter other category businesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:38.506865Z",
     "start_time": "2020-05-25T09:52:38.497050Z"
    },
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_root_category_businesses_uniq(category_label, inv_cat_bids, map_categories, map_cat_name2id):\n",
    "    df_root_cat = load_cached_root_category_businesses_df(category_label, map_categories)\n",
    "    \n",
    "    root_categories = get_root_category_items(map_categories)\n",
    "    # root_categories = sorted(root_categories, key=lambda x: len(x[\"businesses\"]), reverse=False)\n",
    "    \n",
    "    for root_category in root_categories:\n",
    "        if root_category[\"title\"] == category_label:\n",
    "            # skip, do not trim self\n",
    "            continue\n",
    "            \n",
    "        business_ids = set(get_businesses_in_category_branch(inv_cat_bids, root_category[\"title\"], map_categories, map_cat_name2id))\n",
    "        # business_ids = set(root_category[\"businesses\"])\n",
    "        print(f\"\"\"Filter businesses from category {root_category[\"title\"]} [{root_category[\"alias\"]}] ({len(business_ids)} businesses) ...\"\"\")\n",
    "        n_before = len(df_root_cat)\n",
    "        df_root_cat = filter_by_businesses_not_same(df_root_cat, business_ids)\n",
    "        n_after = len(df_root_cat)\n",
    "        print(f\"\"\"Filtered {n_before - n_after} businesses (overlap with {root_category[\"title\"]})\"\"\")\n",
    "        \n",
    "    return df_root_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:38.926642Z",
     "start_time": "2020-05-25T09:52:38.916413Z"
    },
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_root_category_businesses_not_other(category_label, category_label_filter, inv_cat_bids, map_categories, map_cat_name2id):\n",
    "    assert category_label != category_label_filter, \"do not filter on self\"\n",
    "\n",
    "    df_root_cat = load_cached_root_category_businesses_df(category_label, map_categories)\n",
    "    \n",
    "    root_categories = get_root_category_items(map_categories)\n",
    "    # root_categories = sorted(root_categories, key=lambda x: len(x[\"businesses\"]), reverse=True)\n",
    "    \n",
    "    for root_category in root_categories:\n",
    "        if root_category[\"title\"] == category_label_filter:\n",
    "            break\n",
    "    else:\n",
    "        print(f\"No businesses found for {category_label_filter} -> return unchanged\")\n",
    "        return df_root_cat\n",
    "    \n",
    "    business_ids = set(get_businesses_in_category_branch(inv_cat_bids, root_category[\"title\"], map_categories, map_cat_name2id))\n",
    "    # businesses_ids = root_category[\"businesses\"]\n",
    "    print(f\"\"\"Filter businesses from category {root_category[\"title\"]} [{root_category[\"alias\"]}] ({len(set(business_ids))} businesses) ...\"\"\")\n",
    "    n_before = len(df_root_cat)\n",
    "    df_root_cat = filter_by_businesses_not_same(df_root_cat, business_ids)\n",
    "    n_after = len(df_root_cat)\n",
    "    print(f\"\"\"Filtered {n_before - n_after} businesses (overlap with {root_category[\"title\"]})\"\"\")\n",
    "    \n",
    "    return df_root_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:39.531336Z",
     "start_time": "2020-05-25T09:52:39.521031Z"
    },
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_root_category_businesses_same_other(category_label, category_label_filter, inv_cat_bids, map_categories, map_cat_name2id):\n",
    "    assert category_label != category_label_filter, \"do not filter on self\"\n",
    "\n",
    "    df_root_cat = load_cached_root_category_businesses_df(category_label, map_categories)\n",
    "    \n",
    "    root_categories = get_root_category_items(map_categories)\n",
    "    # root_categories = sorted(root_categories, key=lambda x: len(x[\"businesses\"]), reverse=True)\n",
    "    \n",
    "    for root_category in root_categories:\n",
    "        if root_category[\"title\"] == category_label_filter:\n",
    "            break\n",
    "    else:\n",
    "        print(f\"No businesses found for {category_label_filter} -> return unchanged\")\n",
    "        return df_root_cat\n",
    "    \n",
    "    business_ids = set(get_businesses_in_category_branch(inv_cat_bids, root_category[\"title\"], map_categories, map_cat_name2id))\n",
    "    # businesses_ids = root_category[\"businesses\"]\n",
    "    print(f\"\"\"Filter businesses from category {root_category[\"title\"]} [{root_category[\"alias\"]}] ({len(set(business_ids))} businesses) ...\"\"\")\n",
    "    n_before = len(df_root_cat)\n",
    "    df_same = filter_by_businesses(df_root_cat, business_ids)\n",
    "    n_after = len(df_same)\n",
    "    print(f\"\"\"Filtered {n_before - n_after} businesses ({n_after} same with {root_category[\"title\"]})\"\"\")\n",
    "    \n",
    "    return df_same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "\n",
    "- check similar length -- what strategy to use to combine similar length pairs best?\n",
    "- flip sides (arg1 <-> arg2)\n",
    "- make more pairs (?) -- but keep same ratio of same-sentiment vs. not-same-sentiment (different-sentiment) ??\n",
    "- group by categories (topics) -- more explicit training (uniqueness of single arguments / topic structures)\n",
    "- evaluate on same-argument (S3C) - or amazon reviews\n",
    "- compute mean/avg sequence length\n",
    "- fixed layers in fine-tuning\n",
    "- visualization of trained layers and activation for good / bad inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Positive + negative same-sentiment pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:43.585863Z",
     "start_time": "2020-05-25T09:52:43.583298Z"
    }
   },
   "outputs": [],
   "source": [
    "num_pairs_per_class = 2\n",
    "# N positive + N negative\n",
    "# --> 2N pos+neg (not same-sentiment)\n",
    "\n",
    "# TODO: randomness for positive ss / negative ss - sum is equal / or sum equal compared to not ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:44.026806Z",
     "start_time": "2020-05-25T09:52:44.007949Z"
    },
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_pairs_good_bad(df, num_pairs_per_class=2):\n",
    "    pairs_good = list()\n",
    "    pairs_bad = list()\n",
    "\n",
    "    for id_, group in tqdm(df.groupby(\"id\")):\n",
    "        grouper = group.groupby(\"goodness\")\n",
    "        reviews_good = grouper.get_group(True)\n",
    "        reviews_bad = grouper.get_group(False)\n",
    "\n",
    "        # TESTING\n",
    "        # print(\"id:\", id_)\n",
    "        # print(\"#good:\", len(reviews_good))\n",
    "        # print(\"#bad:\", len(reviews_bad))\n",
    "        # print(group)\n",
    "        # break\n",
    "\n",
    "        # make pairings -- good ss\n",
    "        rg_idx = reviews_good.index.values\n",
    "        # print(\"pos_idx:\", rg_idx)\n",
    "        rg_idx_sel = np.random.choice(rg_idx, 2 * num_pairs_per_class, replace=False)\n",
    "        for id1, id2 in zip(rg_idx_sel[::2], rg_idx_sel[1::2]):\n",
    "            # print(\"pair:\", id1, id2)\n",
    "            r1, r2 = df.loc[id1], df.loc[id2]\n",
    "            pair = {\n",
    "                \"argument1\": r1[\"text\"], \"argument2\": r2[\"text\"],\n",
    "                \"argument1_id\": f\"\"\"{r1[\"id\"]}|{r1[\"rid\"]}\"\"\", \"argument2_id\": f\"\"\"{r2[\"id\"]}|{r2[\"rid\"]}\"\"\",\n",
    "                \"is_same_side\": True, \"is_good_side\": True, \"type\": \"good-good\",\n",
    "                \"topic\": inv_bid_cats.get(r1[\"id\"], None)\n",
    "            }\n",
    "            # print(pair)\n",
    "            pairs_good.append(pair)\n",
    "\n",
    "        # make pairings -- bad ss\n",
    "        rb_idx = reviews_bad.index.values\n",
    "        # print(\"neg_idx:\", rb_idx)\n",
    "        rb_idx_sel = np.random.choice(rb_idx, 2 * num_pairs_per_class, replace=False)\n",
    "        for id1, id2 in zip(rb_idx_sel[::2], rb_idx_sel[1::2]):\n",
    "            r1, r2 = df.loc[id1], df.loc[id2]\n",
    "            pair = {\n",
    "                \"argument1\": r1[\"text\"], \"argument2\": r2[\"text\"],\n",
    "                \"argument1_id\": f\"\"\"{r1[\"id\"]}|{r1[\"rid\"]}\"\"\", \"argument2_id\": f\"\"\"{r2[\"id\"]}|{r2[\"rid\"]}\"\"\",\n",
    "                \"is_same_side\": True, \"is_good_side\": False, \"type\": \"bad-bad\",\n",
    "                \"topic\": inv_bid_cats.get(r1[\"id\"], None)\n",
    "            }\n",
    "            pairs_bad.append(pair)\n",
    "\n",
    "        # break\n",
    "        \n",
    "    return pairs_good, pairs_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### not same-sentiment pairs (combinations positive + negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:45.733584Z",
     "start_time": "2020-05-25T09:52:45.729177Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#: number of negative same-sentiment samples same as positive same-sentiment samples\n",
    "num_pairs_negative = 2 * num_pairs_per_class\n",
    "\n",
    "#: whether for a single side (good or bad) there can be multiple occurrences of the same review\n",
    "#: may need to check afterwared that not by chance same pairing happens ...\n",
    "repeatable_on_side = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:52:51.076476Z",
     "start_time": "2020-05-25T09:52:51.059949Z"
    },
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_pairs_negative(df, num_pairs_negative, repeatable_on_side=False):\n",
    "    pairs_not_ss = list()\n",
    "\n",
    "    for id_, group in tqdm(df.groupby(\"id\")):\n",
    "        grouper = group.groupby(\"goodness\")\n",
    "        reviews_good = grouper.get_group(True)\n",
    "        reviews_bad = grouper.get_group(False)\n",
    "\n",
    "        # find indices for reviews per business\n",
    "        rg_idx = reviews_good.index.values\n",
    "        rb_idx = reviews_bad.index.values\n",
    "\n",
    "        # randomly select from each side\n",
    "        rg_idx_sel = np.random.choice(rg_idx, num_pairs_negative, replace=repeatable_on_side)\n",
    "        rb_idx_sel = np.random.choice(rb_idx, num_pairs_negative, replace=repeatable_on_side)\n",
    "\n",
    "        # pair them together -- good-bad pairs\n",
    "        for idg, idb in zip(rg_idx_sel[::2], rb_idx_sel[::2]):\n",
    "            rg, rb = df.loc[idg], df.loc[idb]\n",
    "            pair = {\n",
    "                \"argument1\": rg[\"text\"], \"argument2\": rb[\"text\"],\n",
    "                \"argument1_id\": f\"\"\"{rg[\"id\"]}|{rg[\"rid\"]}\"\"\", \"argument2_id\": f\"\"\"{rb[\"id\"]}|{rb[\"rid\"]}\"\"\",\n",
    "                \"is_same_side\": False, \"is_good_side\": None, \"type\": \"good-bad\",\n",
    "                \"topic\": inv_bid_cats.get(rg[\"id\"], None)\n",
    "            }\n",
    "            # print(pair)\n",
    "            pairs_not_ss.append(pair)\n",
    "\n",
    "        # bad-good pairs\n",
    "        for idb, idg in zip(rb_idx_sel[1::2], rg_idx_sel[1::2]):\n",
    "            rb, rg = df.loc[idb], df.loc[idg]\n",
    "            pair = {\n",
    "                \"argument1\": rb[\"text\"], \"argument2\": rg[\"text\"],\n",
    "                \"argument1_id\": f\"\"\"{rb[\"id\"]}|{rb[\"rid\"]}\"\"\", \"argument2_id\": f\"\"\"{rg[\"id\"]}|{rg[\"rid\"]}\"\"\",\n",
    "                \"is_same_side\": False, \"is_good_side\": None, \"type\": \"bad-good\",\n",
    "                \"topic\": inv_bid_cats.get(rb[\"id\"], None)\n",
    "            }\n",
    "            # print(pair)\n",
    "            pairs_not_ss.append(pair)\n",
    "            \n",
    "    return pairs_not_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dataframe for training etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:53:30.441277Z",
     "start_time": "2020-05-25T09:53:30.428763Z"
    },
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def make_or_load_pairs(df, fn_yelp_df, num_pairs_per_class=2):\n",
    "    if fn_yelp_df is not None:\n",
    "        if os.path.exists(fn_yelp_df):\n",
    "            with open(fn_yelp_df, \"rb\") as fp:\n",
    "                all_df = pickle.load(fp)\n",
    "            return all_df\n",
    "    \n",
    "    pairs_good, pairs_bad = make_pairs_good_bad(df, num_pairs_per_class=num_pairs_per_class)\n",
    "    print(\"#ss (pos)\", len(pairs_good))\n",
    "    print(\"#ss (neg)\", len(pairs_bad))\n",
    "    \n",
    "    num_pairs_negative = 2 * num_pairs_per_class\n",
    "    pairs_not_ss = make_pairs_negative(df, num_pairs_negative, repeatable_on_side=False)\n",
    "    print(\"#nss\", len(pairs_not_ss))\n",
    "    \n",
    "    pairs_all = pairs_good + pairs_bad + pairs_not_ss\n",
    "    print(\"#~ss\", len(pairs_all))\n",
    "    \n",
    "    np.random.shuffle(pairs_all)\n",
    "    df_traindev = pd.DataFrame.from_dict(pairs_all)\n",
    "    \n",
    "    if fn_yelp_df is not None:\n",
    "        with open(fn_yelp_df, \"wb\") as fp:\n",
    "            pickle.dump(df_traindev, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return df_traindev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SameStance Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T11:56:27.345335Z",
     "start_time": "2020-01-20T11:56:26.894555Z"
    },
    "code_folding": [
     8,
     25,
     31
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v) \n",
    "        \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def lemmatize_stemming(token, pos_tag):\n",
    "    stemmer = SnowballStemmer(\n",
    "        \"english\")  # pOrter, M. \"An algorithm for suffix stripping.\"\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(token, pos=pos_tag))\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    lemma = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        sentence = sentence.replace('\\n', ' ').strip()\n",
    "        tokens = [token for token in word_tokenize(sentence)]\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "        for idx in range(0, len(tokens)):\n",
    "            token = tokens[idx].lower()\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(\n",
    "                    token) > 3:\n",
    "                wordnet_pos = get_wordnet_pos(pos_tags[idx][1])\n",
    "                l_ = lemmatize_stemming(token, wordnet_pos)\n",
    "                lemma.append(l_)\n",
    "    return ' '.join(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T11:56:28.905390Z",
     "start_time": "2020-01-20T11:56:28.902572Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_lemma(row):\n",
    "    row['argument1_lemmas'] = preprocess(row['argument1'])\n",
    "    row['argument2_lemmas'] = preprocess(row['argument2'])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting n grams lemma for argument1 and argument2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T11:56:28.986226Z",
     "start_time": "2020-01-20T11:56:28.974629Z"
    },
    "code_folding": [
     3,
     41
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "\n",
    "def extract_ngrams(X_train, X_dev, col, idx='id'):\n",
    "    vectorizer = CountVectorizer(min_df=600,\n",
    "                                 max_df=0.7,\n",
    "                                 ngram_range=(3, 3),\n",
    "                                 max_features=5000)\n",
    "\n",
    "    vectorizer.fit(X_train[col])\n",
    "    features = vectorizer.transform(X_train[col])\n",
    "    features_dev = vectorizer.transform(X_dev[col])\n",
    "\n",
    "    train_df = pd.DataFrame(features.todense(),\n",
    "                            columns=vectorizer.get_feature_names())\n",
    "    train_df = train_df.add_prefix(col)\n",
    "\n",
    "    aid_df = X_train[[idx]]\n",
    "\n",
    "    train_df = train_df.merge(aid_df,\n",
    "                              left_index=True,\n",
    "                              right_index=True,\n",
    "                              suffixes=(False, False),\n",
    "                              how='inner')\n",
    "    train_df.set_index(idx, inplace=True)\n",
    "\n",
    "    dev_df = pd.DataFrame(features_dev.todense(),\n",
    "                          columns=vectorizer.get_feature_names())\n",
    "    dev_df = dev_df.add_prefix(col)\n",
    "\n",
    "    aid_dev_df = X_dev[[idx]]\n",
    "\n",
    "    dev_df = dev_df.merge(aid_dev_df,\n",
    "                          left_index=True,\n",
    "                          right_index=True,\n",
    "                          suffixes=(False, False),\n",
    "                          how='inner')\n",
    "    dev_df.set_index(idx, inplace=True)\n",
    "    return train_df, dev_df\n",
    "\n",
    "\n",
    "def extract_n_grams_features(X_train, X_dev, columns, idx='id'):\n",
    "    X_train = X_train.reset_index()\n",
    "    result_train_df = X_train[[idx]]\n",
    "    result_train_df.set_index(idx, inplace=True)\n",
    "\n",
    "    X_dev = X_dev.reset_index()\n",
    "    result_dev_df = X_dev[[idx]]\n",
    "    result_dev_df.set_index(idx, inplace=True)\n",
    "\n",
    "    for col in columns:\n",
    "        result_train_df_, result_dev_df_ = extract_ngrams(X_train, X_dev, col, idx=idx)\n",
    "        result_train_df = result_train_df.join(result_train_df_)\n",
    "        result_dev_df = result_dev_df.join(result_dev_df_)\n",
    "    return result_train_df, result_dev_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T13:36:37.150908Z",
     "start_time": "2020-01-20T13:36:37.142275Z"
    },
    "code_folding": [
     6,
     21
    ],
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "def train_test_svm(X_train, y_train, X_test):\n",
    "    scaler = StandardScaler(copy=True, with_mean=False)\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train = scaler.transform(X_train)\n",
    "\n",
    "    svclassifier = SVC(kernel='linear')\n",
    "    svclassifier.fit(X_train, y_train)\n",
    "\n",
    "    X_test = scaler.transform(X_test)\n",
    "    y_pred = svclassifier.predict(X_test)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def report_training_results(y_test, y_pred, digits=3):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    print('Accuracy: ', round(accuracy_score(y_test, y_pred), digits))  #\n",
    "    print()\n",
    "\n",
    "    print('Report:')\n",
    "    print(classification_report(y_test, y_pred, digits=digits))\n",
    "    f1_dic = {}\n",
    "\n",
    "    f1_dic['macro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='macro'), digits)\n",
    "    f1_dic['micro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='micro'), digits)\n",
    "    return f1_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load reviews and topics and category tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load reviews\n",
    "\n",
    "- review (with business reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:54:28.073054Z",
     "start_time": "2020-05-25T09:53:34.206922Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6685900it [00:42, 158595.47it/s]\n"
     ]
    }
   ],
   "source": [
    "fn_yelp_reviews = data_yelp_path / \"review.json\"\n",
    "df = load_reviews(fn_yelp_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load categories for businesses\n",
    "\n",
    "- business (id) with list of topics/categories\n",
    "- lookups (business -> categories, category -> businesses)\n",
    "- list of combinations (with amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:54:30.875744Z",
     "start_time": "2020-05-25T09:54:28.103817Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192609it [00:02, 96190.52it/s]\n",
      "100%|| 192127/192127 [00:00<00:00, 1293491.99it/s]\n",
      "100%|| 192127/192127 [00:00<00:00, 741882.13it/s]\n"
     ]
    }
   ],
   "source": [
    "fn_yelp_topics = data_yelp_path / \"business.json\"\n",
    "bids_not_cats = set()\n",
    "inv_bid_cats = load_topics(fn_yelp_topics, bids_not_cats=bids_not_cats)\n",
    "\n",
    "inv_cat_bids = make_map_cats(inv_bid_cats)\n",
    "\n",
    "inv_cat_combis = make_cat_combis(inv_bid_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load category tree\n",
    "\n",
    "- hierarchy of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:54:32.035522Z",
     "start_time": "2020-05-25T09:54:32.004962Z"
    }
   },
   "outputs": [],
   "source": [
    "fn_yelp_catgory_tree = data_yelp_path / \"all_category_list.json\"\n",
    "map_categories, map_cat_name2id, lst_root_categories = load_category_tree(fn_yelp_catgory_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Cache all root category businesses (reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:54:33.171937Z",
     "start_time": "2020-05-25T09:54:33.169140Z"
    }
   },
   "outputs": [],
   "source": [
    "cache_root_category_businesses_df(df, inv_cat_bids, map_categories, map_cat_name2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataframe for training etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_yelp_df = data_yelp_path / \"df_traindev4_typed.p\"\n",
    "\n",
    "df = filter_min_review_freq(df, min_ratings=8)\n",
    "df = filter_both_good_bad(df)\n",
    "\n",
    "df_traindev = make_or_load_pairs(df, str(fn_yelp_df), num_pairs_per_class=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_yelp_df = data_yelp_path / \"df_traindev4_typed.p\"\n",
    "\n",
    "with open(fn_yelp_df, \"rb\") as fp:\n",
    "    traindev_df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:54:41.165579Z",
     "start_time": "2020-05-25T09:54:40.872743Z"
    }
   },
   "outputs": [],
   "source": [
    "# from utils_gluon import print_infos\n",
    "\n",
    "from utils_gluon import report_training_results\n",
    "from utils_gluon import plot_confusion_matrix\n",
    "from utils_gluon import compute_metrics\n",
    "\n",
    "from utils_data import names_columns_X, names_columns_X_arteval, names_columns_y\n",
    "# names_columns_X = ['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic', 'type']\n",
    "names_columns_X = names_columns_X + ['type']\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:54:43.151309Z",
     "start_time": "2020-05-25T09:54:43.145966Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils_data import Timer\n",
    "\n",
    "from utils_data import configure_logging\n",
    "configure_logging()\n",
    "\n",
    "from utils_data import init_random\n",
    "init_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted to include \"type\" (of pairing)\n",
    "# train dev set - 70% 30%\n",
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[names_columns_X]\n",
    "    y = df[names_columns_y]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T09:58:31.831838Z",
     "start_time": "2020-05-25T09:58:31.361186Z"
    }
   },
   "outputs": [],
   "source": [
    "run_name = \"yelp_baseline\"\n",
    "fn_run_path = Path(f\"data/{run_name}\")\n",
    "fn_traindev_df = fn_run_path / \"traindev.p\"\n",
    "\n",
    "category_name = \"*\"  # just a dummy ...\n",
    "\n",
    "! mkdir data/yelp_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "703760"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(traindev_df)  # pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad-bad 175940\n",
      "bad-good 175940\n",
      "good-bad 175940\n",
      "good-good 175940\n"
     ]
    }
   ],
   "source": [
    "# check how many pairs are per pairing\n",
    "for pairtype, df_grouped in traindev_df.groupby([\"type\"]):\n",
    "    print(pairtype, len(df_grouped))\n",
    "    # df_grouped.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_dummy_tag(row):\n",
    "    row[\"tag\"] = \"NA\"\n",
    "    return row\n",
    "\n",
    "# all_df = all_df.progress_apply(_add_dummy_tag, axis=1)\n",
    "traindev_df[\"tag\"] = \"NA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T14:03:19.728337Z",
     "start_time": "2020-01-20T13:36:39.303617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start on [1 - test/train] ...\n",
      "Time for [1 - test/train]: 0:00:00.606096\n"
     ]
    }
   ],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(traindev_df, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T14:03:19.728337Z",
     "start_time": "2020-01-20T13:36:39.303617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start on [2 - lemmatize] ...\n",
      "Time for [2 - lemmatize]: 2:05:46.049497\n"
     ]
    }
   ],
   "source": [
    "# 2. Lemmatizing argument1 and argument2\n",
    "with Timer(\"2 - lemmatize\"):\n",
    "    X_train = X_train.apply(get_lemma, axis=1)\n",
    "    X_dev = X_dev.apply(get_lemma, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_lemmatized = fn_run_path / \"traindev_lemmatized.p\"\n",
    "\n",
    "with open(fn_lemmatized, \"wb\") as fp:\n",
    "    pickle.dump(X_train, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(X_dev, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(level=0, inplace=True)\n",
    "X_dev.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T14:03:19.728337Z",
     "start_time": "2020-01-20T13:36:39.303617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start on [3 - n-grams] ...\n",
      "Time for [3 - n-grams]: 0:04:55.913188\n"
     ]
    }
   ],
   "source": [
    "# 3. Extracting features - 1-3 grams lemma\n",
    "with Timer(\"3 - n-grams\"):\n",
    "    X_train_, X_dev_ = extract_n_grams_features(\n",
    "        X_train, X_dev, columns=['argument1_lemmas', 'argument2_lemmas'], idx=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_ngrammed = fn_run_path / \"traindev_ngrammed.p\"\n",
    "\n",
    "with open(gn_ngrammed, \"wb\") as fp:\n",
    "    pickle.dump(X_train_, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(X_dev_, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T14:03:19.728337Z",
     "start_time": "2020-01-20T13:36:39.303617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start on [4 - SVM (train -> predict)] ...\n"
     ]
    }
   ],
   "source": [
    "# 4. train\n",
    "with Timer(\"4 - SVM (train -> predict)\"):\n",
    "    y_pred = train_test_svm(X_train_, y_train, X_dev_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-20T14:03:19.728337Z",
     "start_time": "2020-01-20T13:36:39.303617Z"
    }
   },
   "outputs": [],
   "source": [
    "# 5. Evaluate\n",
    "with Timer(\"5 - report\"):\n",
    "    report_training_results(y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_training_results(y_dev, y_pred, name=f\"Baseline - {run_name}\", heatmap=False)\n",
    "plot_confusion_matrix(y_dev, y_pred, labels=[0, 1], title=f\"{run_name} Baseline\", values_format=\"d\", cmap=\"Blues\", include_colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fn_run_path / \"eval_results.p\", \"wb\") as fp:\n",
    "    pickle.dump(y_dev, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(y_pred, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
