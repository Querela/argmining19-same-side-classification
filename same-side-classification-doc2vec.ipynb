{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:00:31.943421Z",
     "start_time": "2019-06-26T11:00:30.883247Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "# from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "# from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:00:32.048713Z",
     "start_time": "2019-06-26T11:00:31.482Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:00:32.050523Z",
     "start_time": "2019-06-26T11:00:31.841Z"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:00:33.332063Z",
     "start_time": "2019-06-26T11:00:33.330171Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply progress bars for pandas .apply() -> .progress_apply()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:00:33.361391Z",
     "start_time": "2019-06-26T11:00:33.356244Z"
    },
    "code_folding": [
     0,
     4
    ]
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        time_end = time.time()\n",
    "        time_delta = datetime.timedelta(seconds=(time_end - self.time_start))\n",
    "        if self.name:\n",
    "            print((\"Time for [{}]: {}\".format(self.name, time_delta)))\n",
    "        else:\n",
    "            print((\"Time: {}\".format(time_delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Same Side Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:00:34.345752Z",
     "start_time": "2019-06-26T11:00:34.342067Z"
    }
   },
   "outputs": [],
   "source": [
    "data_cross_path = 'data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = 'data/same-side-classification/within-topic/{}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load within-topics and cross-topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:00:39.324202Z",
     "start_time": "2019-06-26T11:00:37.370142Z"
    },
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [read cross]: 0:00:00.930528\n",
      "Time for [read within]: 0:00:01.014369\n"
     ]
    }
   ],
   "source": [
    "# escapechar to detect quoting escapes, else it fails\n",
    "\n",
    "# na_filter=False, because pandas automatic \"nan\" detection fails with the topic column, too\n",
    "# cross_test_df['topic'].astype(str)[9270]\n",
    "\n",
    "with Timer(\"read cross\"):\n",
    "    cross_traindev_df = pd.read_csv(data_cross_path.format('training'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    cross_test_df = pd.read_csv(data_cross_path.format('test'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "\n",
    "with Timer(\"read within\"):\n",
    "    within_traindev_df = pd.read_csv(data_within_path.format('training'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    within_test_df = pd.read_csv(data_within_path.format('test'), index_col='id', escapechar='\\\\', na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:02:30.366812Z",
     "start_time": "2019-06-26T11:00:40.007171Z"
    },
    "code_folding": [
     1,
     12,
     14,
     17,
     19
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [tag cross traindev]: 0:00:35.189107\n",
      "Time for [tag cross test]: 0:00:19.834640\n",
      "Time for [tag within traindev]: 0:00:37.309147\n",
      "Time for [tag within test]: 0:00:18.014437\n"
     ]
    }
   ],
   "source": [
    "# Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "def add_tag(row):\n",
    "    title = row['topic'].lower().strip()\n",
    "    if \"abortion\" in title:\n",
    "        row['tag'] = 'abortion'\n",
    "    elif \"gay marriage\"  in title:\n",
    "        row['tag'] = 'gay marriage'\n",
    "    else:\n",
    "        row['tag'] = 'NA'\n",
    "    return row\n",
    "\n",
    "\n",
    "with Timer(\"tag cross traindev\"):\n",
    "    cross_traindev_df = cross_traindev_df.apply(add_tag, axis=1)\n",
    "with Timer(\"tag cross test\"):\n",
    "    cross_test_df = cross_test_df.apply(add_tag, axis=1)\n",
    "\n",
    "with Timer(\"tag within traindev\"):\n",
    "    within_traindev_df = within_traindev_df.apply(add_tag, axis=1)\n",
    "with Timer(\"tag within test\"):\n",
    "    within_test_df = within_test_df.apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview about each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:02:30.562851Z",
     "start_time": "2019-06-26T11:02:30.554538Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_overview(df, task='same-side', class_name='is_same_side'):\n",
    "    # Total instance numbers\n",
    "    total = len(df)\n",
    "    print(\"Task: \", task)\n",
    "    print('=' * 40, '\\n')\n",
    "\n",
    "    print('Total instances: ', total)\n",
    "    print('\\n')\n",
    "\n",
    "    print('For each topic:')\n",
    "    for tag, tag_df in df.groupby(['tag']):\n",
    "        print(tag, ': ', len(tag_df), ' instances')\n",
    "        if class_name in df.columns:\n",
    "            for is_same_side, side_df in tag_df.groupby([class_name]):\n",
    "                print('\\t\\t', is_same_side, ': ', len(side_df), ' instances')\n",
    "    print('\\n')\n",
    "\n",
    "    if class_name in df.columns:\n",
    "        print('For each class value:')\n",
    "        for class_value, class_df in df.groupby([class_name]):\n",
    "            print(class_value, ': ', len(class_df), ' instances')\n",
    "        print('\\n')\n",
    "\n",
    "    print('Unique argument1:', len(df['argument1'].unique()))\n",
    "    print('Unique argument2:', len(df['argument2'].unique()))\n",
    "    arguments = df['argument1'].values\n",
    "    arguments = np.concatenate([arguments, df['argument2'].values])\n",
    "\n",
    "    print('Unique total arguments:', len(set(list(arguments))), '\\n')\n",
    "\n",
    "    print('-' * 40, '\\n')\n",
    "\n",
    "    arguments_length_lst = [\n",
    "        len(word_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_length_lst.extend(\n",
    "        [len(word_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Words:')\n",
    "    print('\\tshortest argument:', min(arguments_length_lst), ' words')\n",
    "    print('\\tlongest argument:', max(arguments_length_lst), ' words')\n",
    "    print('\\targument average length:', np.mean(arguments_length_lst),\n",
    "          ' words')\n",
    "\n",
    "    arguments_sent_length_lst = [\n",
    "        len(sent_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_sent_length_lst.extend(\n",
    "        [len(sent_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Sentences:')\n",
    "    print('\\tshortest argument:', min(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\tlongest argument:', max(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\targument average length:', np.mean(arguments_sent_length_lst),\n",
    "          ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:02:30.732372Z",
     "start_time": "2019-06-26T11:02:30.730309Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# with Timer(\"overview cross\"):\n",
    "#     get_overview(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:02:30.900880Z",
     "start_time": "2019-06-26T11:02:30.899095Z"
    }
   },
   "outputs": [],
   "source": [
    "# with Timer(\"overview within\"):\n",
    "#     get_overview(within_traindev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dev set - 70% 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:02:31.072476Z",
     "start_time": "2019-06-26T11:02:31.069500Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']]\n",
    "    y = df[['is_same_side']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:02:31.244617Z",
     "start_time": "2019-06-26T11:02:31.238033Z"
    },
    "code_folding": [
     0,
     15,
     22,
     40
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v)\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def lemmatize_stemming(token, pos_tag):\n",
    "    '''lemmatize words (with POS information) and then stem'''\n",
    "    stemmer = SnowballStemmer(\n",
    "        \"english\")  # pOrter, M. \"An algorithm for suffix stripping.\"\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(token, pos=pos_tag))\n",
    "\n",
    "\n",
    "def do_segmentation(text):\n",
    "    '''do sentence segmentation, tokenization (with lemmatization&stemming)'''\n",
    "    lemma = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        sentence = sentence.replace('\\n', ' ').strip()\n",
    "        tokens = [token for token in word_tokenize(sentence)]\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "        for idx in range(0, len(tokens)):\n",
    "            token = tokens[idx].lower()\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(\n",
    "                    token) > 3:\n",
    "                wordnet_pos = get_wordnet_pos(pos_tags[idx][1])\n",
    "                l_ = lemmatize_stemming(token, wordnet_pos)\n",
    "                lemma.append(l_)\n",
    "    return lemma\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    '''concat lemmatized words together again'''\n",
    "    lemma = do_segmentation(text)\n",
    "    return ' '.join(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting n grams lemma for argument1 and argument2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:02:31.417931Z",
     "start_time": "2019-06-26T11:02:31.410882Z"
    },
    "code_folding": [
     0,
     38
    ]
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(X_train, X_dev, col, idx='id'):\n",
    "    vectorizer = CountVectorizer(min_df=600,\n",
    "                                 max_df=0.7,\n",
    "                                 ngram_range=(3, 3),\n",
    "                                 max_features=5000)\n",
    "\n",
    "    vectorizer.fit(X_train[col])\n",
    "    features = vectorizer.transform(X_train[col])\n",
    "    features_dev = vectorizer.transform(X_dev[col])\n",
    "\n",
    "    train_df = pd.DataFrame(features.todense(),\n",
    "                            columns=vectorizer.get_feature_names())\n",
    "    train_df = train_df.add_prefix(col)\n",
    "\n",
    "    aid_df = X_train[[idx]]\n",
    "\n",
    "    train_df = train_df.merge(aid_df,\n",
    "                              left_index=True,\n",
    "                              right_index=True,\n",
    "                              suffixes=(False, False),\n",
    "                              how='inner')\n",
    "    train_df.set_index(idx, inplace=True)\n",
    "\n",
    "    dev_df = pd.DataFrame(features_dev.todense(),\n",
    "                          columns=vectorizer.get_feature_names())\n",
    "    dev_df = dev_df.add_prefix(col)\n",
    "\n",
    "    aid_dev_df = X_dev[[idx]]\n",
    "\n",
    "    dev_df = dev_df.merge(aid_dev_df,\n",
    "                          left_index=True,\n",
    "                          right_index=True,\n",
    "                          suffixes=(False, False),\n",
    "                          how='inner')\n",
    "    dev_df.set_index(idx, inplace=True)\n",
    "    return train_df, dev_df\n",
    "\n",
    "\n",
    "def extract_n_grams_features(X_train, X_dev, columns, idx='id'):\n",
    "    X_train = X_train.reset_index()\n",
    "    result_train_df = X_train[[idx]]\n",
    "    result_train_df.set_index(idx, inplace=True)\n",
    "\n",
    "    X_dev = X_dev.reset_index()\n",
    "    result_dev_df = X_dev[[idx]]\n",
    "    result_dev_df.set_index(idx, inplace=True)\n",
    "\n",
    "    for col in columns:\n",
    "        result_train_df_, result_dev_df_ = extract_ngrams(X_train, X_dev, col)\n",
    "        result_train_df = result_train_df.join(result_train_df_)\n",
    "        result_dev_df = result_dev_df.join(result_dev_df_)\n",
    "    return result_train_df, result_dev_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Doc2Vec model and vectorize argument1 and argument2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:02:31.593837Z",
     "start_time": "2019-06-26T11:02:31.585675Z"
    },
    "code_folding": [
     0,
     15,
     33,
     55
    ]
   },
   "outputs": [],
   "source": [
    "def make_d2v_docs(row):\n",
    "    words1 = do_segmentation(row['argument1'])\n",
    "    words2 = do_segmentation(row['argument2'])\n",
    "\n",
    "    row['argument1_doc'] = TaggedDocument(words=words1,\n",
    "                                          tags=[row['argument1_id']])\n",
    "    row['argument2_doc'] = TaggedDocument(words=words2,\n",
    "                                          tags=[row['argument2_id']])\n",
    "\n",
    "    row['argument1_lemmas'] = ' '.join(words1)\n",
    "    row['argument2_lemmas'] = ' '.join(words2)\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "class DatasetIter:\n",
    "    def __init__(self, ds, shuffle=True):\n",
    "        self.ds = ds\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def _make_taggeddocs(self, row):\n",
    "        yield row['argument1_doc']\n",
    "        yield row['argument2_doc']\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.ds = self.ds.sample(frac=1)\n",
    "\n",
    "        for _, row in self.ds.iterrows():\n",
    "            for doc in self._make_taggeddocs(row):\n",
    "                yield doc\n",
    "\n",
    "\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/2024be9053094fbb2e765b9a06b9dc580f55c505/gensim/test/test_doc2vec.py#L501\n",
    "class ConcatenatedDoc2Vec(object):\n",
    "    \"\"\"\n",
    "    Concatenation of multiple models for reproducing the Paragraph Vectors paper.\n",
    "    Models must have exactly-matching vocabulary and document IDs. (Models should\n",
    "    be trained separately; this wrapper just returns concatenated results.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        if hasattr(models[0], 'docvecs'):\n",
    "            self.docvecs = ConcatenatedDocvecs([model.docvecs for model in models])\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return np.concatenate([model[token] for model in self.models])\n",
    "\n",
    "    def infer_vector(self, document, alpha=0.1, min_alpha=0.0001, steps=5):\n",
    "        return np.concatenate([model.infer_vector(document, alpha, min_alpha, steps) for model in self.models])\n",
    "\n",
    "    def train(self, *ignore_args, **ignore_kwargs):\n",
    "        pass  # train subcomponents individually\n",
    "\n",
    "\n",
    "class ConcatenatedDocvecs(object):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return np.concatenate([model[token] for model in self.models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:46:41.508871Z",
     "start_time": "2019-06-26T11:46:41.497868Z"
    },
    "code_folding": [
     6,
     19
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(X_train, X_dev, workers=2, epochs=30):\n",
    "    with Timer(\"doc2vec dbow\"):\n",
    "        # columns=['argument1_lemmas', 'argument2_lemmas']\n",
    "        # pd.concat([X_train[columns], X_dev[columns]])\n",
    "        alpha = 0.025  # https://radimrehurek.com/gensim/models/base_any2vec.html#gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
    "        # %%time\n",
    "        model_dbow = Doc2Vec(DatasetIter(X_train, shuffle=True),\n",
    "                             dm=0,\n",
    "                             vector_size=300,\n",
    "                             negative=5,\n",
    "                             hs=0,\n",
    "                             min_count=2,\n",
    "                             sample=0,\n",
    "                             workers=workers,\n",
    "                             epochs=epochs,\n",
    "                             alpha=alpha,\n",
    "                             min_alpha=alpha - (epochs * 0.002))\n",
    "        \n",
    "    with Timer(\"doc2vec dmm\"):\n",
    "        model_dmm = Doc2Vec(DatasetIter(X_train, shuffle=True),\n",
    "                            dm=1,\n",
    "                            dm_mean=1,\n",
    "                            vector_size=300,\n",
    "                            window=10,\n",
    "                            negative=5,\n",
    "                            min_count=1,\n",
    "                            workers=workers,\n",
    "                            epochs=epochs,\n",
    "                            alpha=0.065,\n",
    "                            min_alpha=0.065 - (epochs * 0.002))\n",
    "        \n",
    "    return model_dbow, model_dmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:02:31.935090Z",
     "start_time": "2019-06-26T11:02:31.932250Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# unused\n",
    "def vec_for_learning(model, df):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:02:32.106479Z",
     "start_time": "2019-06-26T11:02:32.103067Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def make_vectors(X_train, X_dev, model):\n",
    "    def make_d2v_vecs(row):\n",
    "        vec1 = model.infer_vector(row['argument1_doc'].words, steps=20)\n",
    "        vec2 = model.infer_vector(row['argument2_doc'].words, steps=20)\n",
    "\n",
    "        row['argument1_vec'] = vec1\n",
    "        row['argument2_vec'] = vec2\n",
    "        \n",
    "        return row\n",
    "\n",
    "    X_train = X_train.progress_apply(make_d2v_vecs, axis=1)\n",
    "    X_dev = X_dev.progress_apply(make_d2v_vecs, axis=1)\n",
    "    \n",
    "    return X_train, X_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:51:24.766258Z",
     "start_time": "2019-06-26T11:51:24.753098Z"
    },
    "code_folding": [
     0,
     10,
     20
    ]
   },
   "outputs": [],
   "source": [
    "def make_vector_comparison_diff(X_train, X_dev):\n",
    "    def ret_vec_diff(row):\n",
    "        return row['argument1_vec'] - row['argument2_vec']\n",
    "\n",
    "    X_train_diff = X_train.progress_apply(ret_vec_diff, axis=1)\n",
    "    X_dev_diff = X_dev.progress_apply(ret_vec_diff, axis=1)\n",
    "\n",
    "    return X_train_diff, X_dev_diff\n",
    "\n",
    "\n",
    "def make_vector_comparison_concat(X_train, X_dev):\n",
    "    def ret_vec_concat(row):\n",
    "        return np.concatenate((row['argument1_vec'], row['argument2_vec']))\n",
    "\n",
    "    X_train_concat = X_train.progress_apply(ret_vec_concat, axis=1)\n",
    "    X_dev_concat = X_dev.progress_apply(ret_vec_concat, axis=1)\n",
    "\n",
    "    return X_train_concat, X_dev_concat\n",
    "\n",
    "\n",
    "def make_vector_comparison(X_train, X_dev, mode=\"diff\"):\n",
    "    if mode == \"concat\":\n",
    "        X_train, X_dev = make_vector_comparison_concat(X_train, X_dev)\n",
    "    else:\n",
    "        X_train, X_dev = make_vector_comparison_diff(X_train, X_dev)\n",
    "\n",
    "    # array of array to 2d array\n",
    "    X_train = np.array(list(X_train.values))\n",
    "    X_dev = np.array(list(X_dev.values))\n",
    "\n",
    "    return X_train, X_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:02:32.456460Z",
     "start_time": "2019-06-26T11:02:32.450491Z"
    },
    "code_folding": [
     0,
     22,
     33
    ]
   },
   "outputs": [],
   "source": [
    "def train_test_svm(X_train, y_train, X_test):\n",
    "    with Timer(\"StandardScaler fit\"):\n",
    "        scaler = StandardScaler(copy=True, with_mean=False)\n",
    "        scaler.fit(X_train)\n",
    "\n",
    "    with Timer(\"StandardScaler transform\"):\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # ------------------\n",
    "\n",
    "    with Timer(\"SVC (linear) fit\"):\n",
    "        # svclassifier = SVC(kernel='linear')\n",
    "        svclassifier = LinearSVC()        \n",
    "        svclassifier.fit(X_train, y_train)\n",
    "\n",
    "    with Timer(\"SVC predict\"):\n",
    "        y_pred = svclassifier.predict(X_test)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def train_test_logreg(X_train, y_train, X_test):\n",
    "    with Timer(\"LogisticRegression fit\"):\n",
    "        logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "        logreg.fit(X_train, y_train)\n",
    "    \n",
    "    with Timer(\"LogisticRegression predict\"):\n",
    "        y_pred = logreg.predict(X_test)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def report_training_results(y_test, y_pred):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    print('Accuracy: ', round(accuracy_score(y_test, y_pred), 2), '\\n')  #\n",
    "\n",
    "    print('Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    f1_dic = {}\n",
    "\n",
    "    f1_dic['macro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='macro'), 2)\n",
    "    f1_dic['micro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='micro'), 2)\n",
    "    return f1_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:02:32.640713Z",
     "start_time": "2019-06-26T11:02:32.624572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [1 - test/train]: 0:00:00.013205\n"
     ]
    }
   ],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:22:57.491220Z",
     "start_time": "2019-06-26T11:02:32.887970Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42733/42733 [14:09<00:00, 50.30it/s]\n",
      "100%|██████████| 18315/18315 [06:15<00:00, 48.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [2 - tokenize]: 0:20:24.600272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. tokenize (make doc2vec docs + lemma string)\n",
    "# tqdm.pandas()\n",
    "with Timer(\"2 - tokenize\"):\n",
    "    X_train = X_train.progress_apply(make_d2v_docs, axis=1)\n",
    "    X_dev = X_dev.progress_apply(make_d2v_docs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:30:58.164194Z",
     "start_time": "2019-06-26T11:22:58.047243Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [doc2vec dbow]: 0:03:40.805410\n",
      "Time for [doc2vec dbow]: 0:04:19.307701\n",
      "Time for [3 - doc2vec model]: 0:08:00.113433\n"
     ]
    }
   ],
   "source": [
    "# 3. train doc2vec model\n",
    "with Timer(\"3 - doc2vec model\"):\n",
    "    model_dbow, model_dmm = train_model(X_train, X_dev, workers=3, epochs=15)\n",
    "    \n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    model_concat = ConcatenatedDoc2Vec([model_dbow, model_dmm])  # todo: same vocab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:44:18.109609Z",
     "start_time": "2019-06-26T11:30:58.738294Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42733/42733 [09:16<00:00, 76.85it/s] \n",
      "100%|██████████| 18315/18315 [04:03<00:00, 75.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [4 - vectorize arguments]: 0:13:19.368227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. vectorize arguments\n",
    "with Timer(\"4 - vectorize arguments\"):\n",
    "    # X_train, X_dev = make_vectors(X_train, X_dev, model_dbow)\n",
    "    # X_train, X_dev = make_vectors(X_train, X_dev, model_dmm)\n",
    "    X_train, X_dev = make_vectors(X_train, X_dev, model_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:51:53.796365Z",
     "start_time": "2019-06-26T11:51:49.002787Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42733/42733 [00:03<00:00, 13296.97it/s]\n",
      "100%|██████████| 18315/18315 [00:01<00:00, 13273.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [5 - vector comparison of arguments]: 0:00:04.786261\n"
     ]
    }
   ],
   "source": [
    "# 5. combine two argument vectors into a single one\n",
    "# - diff / concat / ...\n",
    "with Timer(\"5 - vector comparison of arguments\"):\n",
    "    X_train_diff, X_dev_diff = make_vector_comparison(X_train, X_dev, mode=\"concat\")\n",
    "\n",
    "X_train_ = X_train_diff\n",
    "X_dev_ = X_dev_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:54:07.161740Z",
     "start_time": "2019-06-26T11:52:00.286951Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [StandardScaler fit]: 0:00:00.756135\n",
      "Time for [StandardScaler transform]: 0:00:00.287616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekoerner/.conda/envs/argmining19-ssc/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [SVC (linear) fit]: 0:02:05.663633\n",
      "Time for [SVC predict]: 0:00:00.093925\n",
      "Time for [6 - SVM (train -> predict)]: 0:02:06.823927\n",
      "Confusion Matrix:\n",
      "[[3672 5264]\n",
      " [2982 6397]]\n",
      "\n",
      "Accuracy:  0.55 \n",
      "\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.55      0.41      0.47      8936\n",
      "        True       0.55      0.68      0.61      9379\n",
      "\n",
      "    accuracy                           0.55     18315\n",
      "   macro avg       0.55      0.55      0.54     18315\n",
      "weighted avg       0.55      0.55      0.54     18315\n",
      "\n",
      "Time for [7 - report]: 0:00:00.045930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekoerner/.conda/envs/argmining19-ssc/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# 6. train\n",
    "with Timer(\"6 - SVM (train -> predict)\"):\n",
    "    y_pred_svm = train_test_svm(X_train_, y_train, X_dev_)\n",
    "\n",
    "# 7. Evaluate\n",
    "with Timer(\"7 - report\"):\n",
    "    report_training_results(y_dev, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:54:40.760566Z",
     "start_time": "2019-06-26T11:54:08.103439Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekoerner/.conda/envs/argmining19-ssc/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ekoerner/.conda/envs/argmining19-ssc/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [LogisticRegression fit]: 0:00:32.509970\n",
      "Time for [LogisticRegression predict]: 0:00:00.094768\n",
      "Time for [6 - LogReg (train -> predict)]: 0:00:32.604980\n",
      "Confusion Matrix:\n",
      "[[5010 3926]\n",
      " [3875 5504]]\n",
      "\n",
      "Accuracy:  0.57 \n",
      "\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.56      0.56      0.56      8936\n",
      "        True       0.58      0.59      0.59      9379\n",
      "\n",
      "    accuracy                           0.57     18315\n",
      "   macro avg       0.57      0.57      0.57     18315\n",
      "weighted avg       0.57      0.57      0.57     18315\n",
      "\n",
      "Time for [7 - report]: 0:00:00.049424\n"
     ]
    }
   ],
   "source": [
    "# 6. train\n",
    "with Timer(\"6 - LogReg (train -> predict)\"):\n",
    "    y_pred_logreg = train_test_logreg(X_train_, y_train, X_dev_)\n",
    "\n",
    "# 7. Evaluate\n",
    "with Timer(\"7 - report\"):\n",
    "    report_training_results(y_dev, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-26T11:45:45.813418Z",
     "start_time": "2019-06-26T11:45:45.808878Z"
    },
    "code_folding": [
     0,
     6,
     11,
     16,
     20
    ]
   },
   "outputs": [],
   "source": [
    "# old\n",
    "return\n",
    "\n",
    "asdf\n",
    "\n",
    "# 2. Lemmatizing argument1 and argument2\n",
    "with Timer(\"2 - lemmatize\"):\n",
    "    X_train = X_train.apply(get_lemma, axis=1)\n",
    "    X_dev = X_dev.apply(get_lemma, axis=1)\n",
    "\n",
    "# 3. Extracting features - 1-3 grams lemma\n",
    "with Timer(\"3 - n-grams\"):\n",
    "    X_train_, X_dev_ = extract_n_grams_features(\n",
    "        X_train, X_dev, columns=['argument1_lemmas', 'argument2_lemmas'])\n",
    "\n",
    "# 4. train\n",
    "with Timer(\"4 - SVM (train -> predict)\"):\n",
    "    y_pred = train_test_svm(X_train_, y_train, X_dev_)\n",
    "\n",
    "# 5. Evaluate\n",
    "with Timer(\"5 - report\"):\n",
    "    report_training_results(y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
