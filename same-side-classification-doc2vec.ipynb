{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T15:03:43.740204Z",
     "start_time": "2019-06-25T15:03:43.731311Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "# from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn import utils\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from tqdm import tqdm_notebook as tqdm\n",
    "# from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:52:58.892573Z",
     "start_time": "2019-06-25T13:52:58.889284Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:52:59.080109Z",
     "start_time": "2019-06-25T13:52:58.894834Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ekoerner/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ekoerner/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ekoerner/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:52:59.084477Z",
     "start_time": "2019-06-25T13:52:59.081982Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply progress bars for pandas .apply() -> .progress_apply()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:52:59.096629Z",
     "start_time": "2019-06-25T13:52:59.086162Z"
    },
    "code_folding": [
     0,
     4
    ]
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        time_end = time.time()\n",
    "        time_delta = datetime.timedelta(seconds=(time_end - self.time_start))\n",
    "        if self.name:\n",
    "            print((\"Time for [{}]: {}\".format(self.name, time_delta)))\n",
    "        else:\n",
    "            print((\"Time: {}\".format(time_delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Same Side Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:52:59.108347Z",
     "start_time": "2019-06-25T13:52:59.098983Z"
    }
   },
   "outputs": [],
   "source": [
    "data_cross_path = 'data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = 'data/same-side-classification/within-topic/{}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load within-topics and cross-topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:53:01.198227Z",
     "start_time": "2019-06-25T13:52:59.112164Z"
    },
    "hide_input": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [read cross]: 0:00:01.027275\n",
      "Time for [read within]: 0:00:01.050664\n"
     ]
    }
   ],
   "source": [
    "# escapechar to detect quoting escapes, else it fails\n",
    "\n",
    "# na_filter=False, because pandas automatic \"nan\" detection fails with the topic column, too\n",
    "# cross_test_df['topic'].astype(str)[9270]\n",
    "\n",
    "with Timer(\"read cross\"):\n",
    "    cross_traindev_df = pd.read_csv(data_cross_path.format('training'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    cross_test_df = pd.read_csv(data_cross_path.format('test'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "\n",
    "with Timer(\"read within\"):\n",
    "    within_traindev_df = pd.read_csv(data_within_path.format('training'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    within_test_df = pd.read_csv(data_within_path.format('test'), index_col='id', escapechar='\\\\', na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:54:56.500997Z",
     "start_time": "2019-06-25T13:53:01.199900Z"
    },
    "code_folding": [
     1,
     12,
     14,
     17,
     19
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [tag cross traindev]: 0:00:37.183995\n",
      "Time for [tag cross test]: 0:00:20.520795\n",
      "Time for [tag within traindev]: 0:00:38.901913\n",
      "Time for [tag within test]: 0:00:18.685280\n"
     ]
    }
   ],
   "source": [
    "# Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "def add_tag(row):\n",
    "    title = row['topic'].lower().strip()\n",
    "    if \"abortion\" in title:\n",
    "        row['tag'] = 'abortion'\n",
    "    elif \"gay marriage\"  in title:\n",
    "        row['tag'] = 'gay marriage'\n",
    "    else:\n",
    "        row['tag'] = 'NA'\n",
    "    return row\n",
    "\n",
    "\n",
    "with Timer(\"tag cross traindev\"):\n",
    "    cross_traindev_df = cross_traindev_df.apply(add_tag, axis=1)\n",
    "with Timer(\"tag cross test\"):\n",
    "    cross_test_df = cross_test_df.apply(add_tag, axis=1)\n",
    "\n",
    "with Timer(\"tag within traindev\"):\n",
    "    within_traindev_df = within_traindev_df.apply(add_tag, axis=1)\n",
    "with Timer(\"tag within test\"):\n",
    "    within_test_df = within_test_df.apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview about each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:54:56.517254Z",
     "start_time": "2019-06-25T13:54:56.503129Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_overview(df, task='same-side', class_name='is_same_side'):\n",
    "    # Total instance numbers\n",
    "    total = len(df)\n",
    "    print(\"Task: \", task)\n",
    "    print('=' * 40, '\\n')\n",
    "\n",
    "    print('Total instances: ', total)\n",
    "    print('\\n')\n",
    "\n",
    "    print('For each topic:')\n",
    "    for tag, tag_df in df.groupby(['tag']):\n",
    "        print(tag, ': ', len(tag_df), ' instances')\n",
    "        if class_name in df.columns:\n",
    "            for is_same_side, side_df in tag_df.groupby([class_name]):\n",
    "                print('\\t\\t', is_same_side, ': ', len(side_df), ' instances')\n",
    "    print('\\n')\n",
    "\n",
    "    if class_name in df.columns:\n",
    "        print('For each class value:')\n",
    "        for class_value, class_df in df.groupby([class_name]):\n",
    "            print(class_value, ': ', len(class_df), ' instances')\n",
    "        print('\\n')\n",
    "\n",
    "    print('Unique argument1:', len(df['argument1'].unique()))\n",
    "    print('Unique argument2:', len(df['argument2'].unique()))\n",
    "    arguments = df['argument1'].values\n",
    "    arguments = np.concatenate([arguments, df['argument2'].values])\n",
    "\n",
    "    print('Unique total arguments:', len(set(list(arguments))), '\\n')\n",
    "\n",
    "    print('-' * 40, '\\n')\n",
    "\n",
    "    arguments_length_lst = [\n",
    "        len(word_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_length_lst.extend(\n",
    "        [len(word_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Words:')\n",
    "    print('\\tshortest argument:', min(arguments_length_lst), ' words')\n",
    "    print('\\tlongest argument:', max(arguments_length_lst), ' words')\n",
    "    print('\\targument average length:', np.mean(arguments_length_lst),\n",
    "          ' words')\n",
    "\n",
    "    arguments_sent_length_lst = [\n",
    "        len(sent_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_sent_length_lst.extend(\n",
    "        [len(sent_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Sentences:')\n",
    "    print('\\tshortest argument:', min(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\tlongest argument:', max(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\targument average length:', np.mean(arguments_sent_length_lst),\n",
    "          ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:54:56.529378Z",
     "start_time": "2019-06-25T13:54:56.518629Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# with Timer(\"overview cross\"):\n",
    "#     get_overview(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:54:56.545986Z",
     "start_time": "2019-06-25T13:54:56.531705Z"
    }
   },
   "outputs": [],
   "source": [
    "# with Timer(\"overview within\"):\n",
    "#     get_overview(within_traindev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dev set - 70% 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:54:56.558139Z",
     "start_time": "2019-06-25T13:54:56.549158Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']]\n",
    "    y = df[['is_same_side']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:54:56.576823Z",
     "start_time": "2019-06-25T13:54:56.561222Z"
    },
    "code_folding": [
     0,
     15,
     22,
     40
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v)\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # As default pos in lemmatization is Noun\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def lemmatize_stemming(token, pos_tag):\n",
    "    '''lemmatize words (with POS information) and then stem'''\n",
    "    stemmer = SnowballStemmer(\n",
    "        \"english\")  # pOrter, M. \"An algorithm for suffix stripping.\"\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(token, pos=pos_tag))\n",
    "\n",
    "\n",
    "def do_segmentation(text):\n",
    "    '''do sentence segmentation, tokenization (with lemmatization&stemming)'''\n",
    "    lemma = []\n",
    "    for sentence in sent_tokenize(text):\n",
    "        sentence = sentence.replace('\\n', ' ').strip()\n",
    "        tokens = [token for token in word_tokenize(sentence)]\n",
    "        pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "        for idx in range(0, len(tokens)):\n",
    "            token = tokens[idx].lower()\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(\n",
    "                    token) > 3:\n",
    "                wordnet_pos = get_wordnet_pos(pos_tags[idx][1])\n",
    "                l_ = lemmatize_stemming(token, wordnet_pos)\n",
    "                lemma.append(l_)\n",
    "    return lemma\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    '''concat lemmatized words together again'''\n",
    "    lemma = do_segmentation(text)\n",
    "    return ' '.join(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting n grams lemma for argument1 and argument2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:54:56.590714Z",
     "start_time": "2019-06-25T13:54:56.579073Z"
    },
    "code_folding": [
     0,
     38
    ]
   },
   "outputs": [],
   "source": [
    "def extract_ngrams(X_train, X_dev, col, idx='id'):\n",
    "    vectorizer = CountVectorizer(min_df=600,\n",
    "                                 max_df=0.7,\n",
    "                                 ngram_range=(3, 3),\n",
    "                                 max_features=5000)\n",
    "\n",
    "    vectorizer.fit(X_train[col])\n",
    "    features = vectorizer.transform(X_train[col])\n",
    "    features_dev = vectorizer.transform(X_dev[col])\n",
    "\n",
    "    train_df = pd.DataFrame(features.todense(),\n",
    "                            columns=vectorizer.get_feature_names())\n",
    "    train_df = train_df.add_prefix(col)\n",
    "\n",
    "    aid_df = X_train[[idx]]\n",
    "\n",
    "    train_df = train_df.merge(aid_df,\n",
    "                              left_index=True,\n",
    "                              right_index=True,\n",
    "                              suffixes=(False, False),\n",
    "                              how='inner')\n",
    "    train_df.set_index(idx, inplace=True)\n",
    "\n",
    "    dev_df = pd.DataFrame(features_dev.todense(),\n",
    "                          columns=vectorizer.get_feature_names())\n",
    "    dev_df = dev_df.add_prefix(col)\n",
    "\n",
    "    aid_dev_df = X_dev[[idx]]\n",
    "\n",
    "    dev_df = dev_df.merge(aid_dev_df,\n",
    "                          left_index=True,\n",
    "                          right_index=True,\n",
    "                          suffixes=(False, False),\n",
    "                          how='inner')\n",
    "    dev_df.set_index(idx, inplace=True)\n",
    "    return train_df, dev_df\n",
    "\n",
    "\n",
    "def extract_n_grams_features(X_train, X_dev, columns, idx='id'):\n",
    "    X_train = X_train.reset_index()\n",
    "    result_train_df = X_train[[idx]]\n",
    "    result_train_df.set_index(idx, inplace=True)\n",
    "\n",
    "    X_dev = X_dev.reset_index()\n",
    "    result_dev_df = X_dev[[idx]]\n",
    "    result_dev_df.set_index(idx, inplace=True)\n",
    "\n",
    "    for col in columns:\n",
    "        result_train_df_, result_dev_df_ = extract_ngrams(X_train, X_dev, col)\n",
    "        result_train_df = result_train_df.join(result_train_df_)\n",
    "        result_dev_df = result_dev_df.join(result_dev_df_)\n",
    "    return result_train_df, result_dev_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Doc2Vec model and vectorize argument1 and argument2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T15:03:21.054386Z",
     "start_time": "2019-06-25T15:03:21.040262Z"
    },
    "code_folding": [
     0,
     15,
     33,
     55
    ]
   },
   "outputs": [],
   "source": [
    "def make_d2v_docs(row):\n",
    "    words1 = do_segmentation(row['argument1'])\n",
    "    words2 = do_segmentation(row['argument2'])\n",
    "\n",
    "    row['argument1_doc'] = TaggedDocument(words=words1,\n",
    "                                          tags=[row['argument1_id']])\n",
    "    row['argument2_doc'] = TaggedDocument(words=words2,\n",
    "                                          tags=[row['argument2_id']])\n",
    "\n",
    "    row['argument1_lemmas'] = ' '.join(words1)\n",
    "    row['argument2_lemmas'] = ' '.join(words2)\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "class DatasetIter:\n",
    "    def __init__(self, ds, shuffle=True):\n",
    "        self.ds = ds\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def _make_taggeddocs(self, row):\n",
    "        yield row['argument1_doc']\n",
    "        yield row['argument2_doc']\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.ds = self.ds.sample(frac=1)\n",
    "\n",
    "        for _, row in self.ds.iterrows():\n",
    "            for doc in self._make_taggeddocs(row):\n",
    "                yield doc\n",
    "\n",
    "\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/2024be9053094fbb2e765b9a06b9dc580f55c505/gensim/test/test_doc2vec.py#L501\n",
    "class ConcatenatedDoc2Vec(object):\n",
    "    \"\"\"\n",
    "    Concatenation of multiple models for reproducing the Paragraph Vectors paper.\n",
    "    Models must have exactly-matching vocabulary and document IDs. (Models should\n",
    "    be trained separately; this wrapper just returns concatenated results.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        if hasattr(models[0], 'docvecs'):\n",
    "            self.docvecs = ConcatenatedDocvecs([model.docvecs for model in models])\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return np.concatenate([model[token] for model in self.models])\n",
    "\n",
    "    def infer_vector(self, document, alpha=0.1, min_alpha=0.0001, steps=5):\n",
    "        return np.concatenate([model.infer_vector(document, alpha, min_alpha, steps) for model in self.models])\n",
    "\n",
    "    def train(self, *ignore_args, **ignore_kwargs):\n",
    "        pass  # train subcomponents individually\n",
    "\n",
    "\n",
    "class ConcatenatedDocvecs(object):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return np.concatenate([model[token] for model in self.models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T14:39:39.620347Z",
     "start_time": "2019-06-25T14:39:39.610123Z"
    },
    "code_folding": [
     6,
     19
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(X_train, X_dev, workers=2, epochs=30):\n",
    "    with Timer(\"doc2vec dbow\"):\n",
    "        # columns=['argument1_lemmas', 'argument2_lemmas']\n",
    "        # pd.concat([X_train[columns], X_dev[columns]])\n",
    "        alpha = 0.025  # https://radimrehurek.com/gensim/models/base_any2vec.html#gensim.models.base_any2vec.BaseWordEmbeddingsModel\n",
    "        # %%time\n",
    "        model_dbow = Doc2Vec(DatasetIter(X_train, shuffle=True),\n",
    "                             dm=0,\n",
    "                             vector_size=300,\n",
    "                             negative=5,\n",
    "                             hs=0,\n",
    "                             min_count=2,\n",
    "                             sample=0,\n",
    "                             workers=workers,\n",
    "                             epochs=epochs,\n",
    "                             alpha=alpha,\n",
    "                             min_alpha=alpha - (epochs * 0.002))\n",
    "        \n",
    "    with Timer(\"doc2vec dbow\"):\n",
    "        model_dmm = Doc2Vec(DatasetIter(X_train, shuffle=True),\n",
    "                            dm=1,\n",
    "                            dm_mean=1,\n",
    "                            vector_size=300,\n",
    "                            window=10,\n",
    "                            negative=5,\n",
    "                            min_count=1,\n",
    "                            workers=workers,\n",
    "                            epochs=epochs,\n",
    "                            alpha=0.065,\n",
    "                            min_alpha=0.065 - (epochs * 0.002))\n",
    "        \n",
    "    return model_dbow, model_dmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:54:56.641230Z",
     "start_time": "2019-06-25T13:54:56.628854Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# unused\n",
    "def vec_for_learning(model, df):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:54:56.657532Z",
     "start_time": "2019-06-25T13:54:56.643798Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def make_vectors(X_train, X_dev, model):\n",
    "    def make_d2v_vecs(row):\n",
    "        vec1 = model.infer_vector(row['argument1_doc'].words, steps=20)\n",
    "        vec2 = model.infer_vector(row['argument2_doc'].words, steps=20)\n",
    "\n",
    "        row['argument1_vec'] = vec1\n",
    "        row['argument2_vec'] = vec2\n",
    "        \n",
    "        return row\n",
    "\n",
    "    X_train = X_train.progress_apply(make_d2v_vecs, axis=1)\n",
    "    X_dev = X_dev.progress_apply(make_d2v_vecs, axis=1)\n",
    "    \n",
    "    return X_train, X_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:54:56.670758Z",
     "start_time": "2019-06-25T13:54:56.659446Z"
    },
    "code_folding": [
     0,
     10,
     20
    ]
   },
   "outputs": [],
   "source": [
    "def make_vector_comparison_diff(X_train, X_dev):\n",
    "    def ret_vec_diff(row):\n",
    "        return row['argument1_vec'] - row['argument2_vec']\n",
    "\n",
    "    X_train_diff = X_train.progress_apply(ret_vec_diff, axis=1)\n",
    "    X_dev_diff = X_dev.progress_apply(ret_vec_diff, axis=1)\n",
    "\n",
    "    return X_train_diff, X_dev_diff\n",
    "\n",
    "\n",
    "def make_vector_comparison_concat(X_train, X_dev):\n",
    "    def ret_vec_concat(row):\n",
    "        return np.concatenate((row['argument1_vec'], row['argument2_vec']))\n",
    "\n",
    "    X_train_concat = X_train.progress_apply(ret_vec_diff, axis=1)\n",
    "    X_dev_concat = X_dev.progress_apply(ret_vec_diff, axis=1)\n",
    "\n",
    "    return X_train_concat, X_dev_concat\n",
    "\n",
    "\n",
    "def make_vector_comparison(X_train, X_dev):\n",
    "    X_train, X_dev = make_vector_comparison_diff(X_train, X_dev)\n",
    "    \n",
    "    # array of array to 2d array\n",
    "    X_train = np.array(list(X_train.values))\n",
    "    X_dev = np.array(list(X_dev.values))\n",
    "\n",
    "    return X_train, X_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T15:03:31.515782Z",
     "start_time": "2019-06-25T15:03:31.505159Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def train_test_svm(X_train, y_train, X_test):\n",
    "    with Timer(\"StandardScaler fit\"):\n",
    "        scaler = StandardScaler(copy=True, with_mean=False)\n",
    "        scaler.fit(X_train)\n",
    "\n",
    "    with Timer(\"StandardScaler transform\"):\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # ------------------\n",
    "\n",
    "    with Timer(\"SVC (linear) fit\"):\n",
    "        # svclassifier = SVC(kernel='linear')\n",
    "        svclassifier = LinearSVC()        \n",
    "        svclassifier.fit(X_train, y_train)\n",
    "\n",
    "    with Timer(\"SVC predict\"):\n",
    "        y_pred = svclassifier.predict(X_test)\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def train_test_logreg(X_train, y_train, X_test):\n",
    "    with Timer(\"LogisticRegression fit\"):\n",
    "        logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "        logreg.fit(X_train, y_train)\n",
    "    \n",
    "    with Timer(\"LogisticRegression predict\"):\n",
    "        y_pred = logreg.predict(X_test)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def report_training_results(y_test, y_pred):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    print('Accuracy: ', round(accuracy_score(y_test, y_pred), 2), '\\n')  #\n",
    "\n",
    "    print('Report:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    f1_dic = {}\n",
    "\n",
    "    f1_dic['macro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='macro'), 2)\n",
    "    f1_dic['micro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='micro'), 2)\n",
    "    return f1_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T13:54:56.724823Z",
     "start_time": "2019-06-25T13:54:56.689393Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [1 - test/train]: 0:00:00.024936\n"
     ]
    }
   ],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T14:18:13.191241Z",
     "start_time": "2019-06-25T13:54:56.726942Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42733/42733 [16:06<00:00, 55.86it/s]\n",
      "100%|██████████| 18315/18315 [07:07<00:00, 42.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [2 - tokenize]: 0:23:16.460667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. tokenize (make doc2vec docs + lemma string)\n",
    "# tqdm.pandas()\n",
    "with Timer(\"2 - tokenize\"):\n",
    "    X_train = X_train.progress_apply(make_d2v_docs, axis=1)\n",
    "    X_dev = X_dev.progress_apply(make_d2v_docs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T14:54:33.134311Z",
     "start_time": "2019-06-25T14:46:29.529843Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-25 16:46:29,531 : INFO : collecting all words and their counts\n",
      "2019-06-25 16:46:29,545 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-06-25 16:46:30,064 : INFO : PROGRESS: at example #10000, processed 753884 words (1456171/s), 22011 word types, 2133 tags\n",
      "2019-06-25 16:46:30,772 : INFO : PROGRESS: at example #20000, processed 1514632 words (1074660/s), 29170 word types, 3482 tags\n",
      "2019-06-25 16:46:31,345 : INFO : PROGRESS: at example #30000, processed 2275148 words (1330942/s), 33152 word types, 4480 tags\n",
      "2019-06-25 16:46:31,922 : INFO : PROGRESS: at example #40000, processed 3052783 words (1348761/s), 35434 word types, 5278 tags\n",
      "2019-06-25 16:46:32,497 : INFO : PROGRESS: at example #50000, processed 3840157 words (1371291/s), 36710 word types, 5897 tags\n",
      "2019-06-25 16:46:33,041 : INFO : PROGRESS: at example #60000, processed 4601355 words (1401749/s), 37353 word types, 6396 tags\n",
      "2019-06-25 16:46:33,592 : INFO : PROGRESS: at example #70000, processed 5387213 words (1428490/s), 37815 word types, 6782 tags\n",
      "2019-06-25 16:46:34,130 : INFO : PROGRESS: at example #80000, processed 6148955 words (1416559/s), 37994 word types, 7077 tags\n",
      "2019-06-25 16:46:34,414 : INFO : collected 38071 word types and 7236 unique tags from a corpus of 85466 examples and 6557240 words\n",
      "2019-06-25 16:46:34,415 : INFO : Loading a fresh vocabulary\n",
      "2019-06-25 16:46:34,461 : INFO : effective_min_count=2 retains 36901 unique words (96% of original 38071, drops 1170)\n",
      "2019-06-25 16:46:34,462 : INFO : effective_min_count=2 leaves 6556070 word corpus (99% of original 6557240, drops 1170)\n",
      "2019-06-25 16:46:34,537 : INFO : deleting the raw counts dictionary of 38071 items\n",
      "2019-06-25 16:46:34,539 : INFO : sample=0 downsamples 0 most-common words\n",
      "2019-06-25 16:46:34,540 : INFO : downsampling leaves estimated 6556070 word corpus (100.0% of prior 6556070)\n",
      "2019-06-25 16:46:34,629 : INFO : estimated required memory for 36901 words and 300 dimensions: 117143300 bytes\n",
      "2019-06-25 16:46:34,630 : INFO : resetting layer weights\n",
      "2019-06-25 16:46:35,004 : INFO : training model with 3 workers on 36901 vocabulary and 300 features, using sg=1 hs=0 sample=0 negative=5 window=5\n",
      "2019-06-25 16:46:36,017 : INFO : EPOCH 1 - PROGRESS: at 5.54% examples, 382330 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:46:37,026 : INFO : EPOCH 1 - PROGRESS: at 12.62% examples, 418560 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:46:38,042 : INFO : EPOCH 1 - PROGRESS: at 20.52% examples, 449653 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:46:39,046 : INFO : EPOCH 1 - PROGRESS: at 28.12% examples, 463756 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:46:40,048 : INFO : EPOCH 1 - PROGRESS: at 34.64% examples, 455080 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:46:41,082 : INFO : EPOCH 1 - PROGRESS: at 41.83% examples, 457846 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:46:42,092 : INFO : EPOCH 1 - PROGRESS: at 48.00% examples, 450331 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:46:43,104 : INFO : EPOCH 1 - PROGRESS: at 55.76% examples, 456835 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:46:44,117 : INFO : EPOCH 1 - PROGRESS: at 62.11% examples, 452048 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:46:45,124 : INFO : EPOCH 1 - PROGRESS: at 68.93% examples, 453464 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:46:46,143 : INFO : EPOCH 1 - PROGRESS: at 76.38% examples, 455021 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:46:47,144 : INFO : EPOCH 1 - PROGRESS: at 84.03% examples, 459397 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:46:48,167 : INFO : EPOCH 1 - PROGRESS: at 91.23% examples, 460865 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:46:49,180 : INFO : EPOCH 1 - PROGRESS: at 98.15% examples, 460297 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:46:49,332 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:46:49,340 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:46:49,354 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:46:49,355 : INFO : EPOCH - 1 : training on 6557240 raw words (6641536 effective words) took 14.3s, 462889 effective words/s\n",
      "2019-06-25 16:46:50,362 : INFO : EPOCH 2 - PROGRESS: at 6.29% examples, 416217 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:46:51,365 : INFO : EPOCH 2 - PROGRESS: at 13.01% examples, 437216 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:46:52,408 : INFO : EPOCH 2 - PROGRESS: at 19.31% examples, 428491 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:46:53,421 : INFO : EPOCH 2 - PROGRESS: at 25.70% examples, 422335 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:46:54,442 : INFO : EPOCH 2 - PROGRESS: at 32.41% examples, 426032 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:46:55,466 : INFO : EPOCH 2 - PROGRESS: at 39.08% examples, 428084 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:46:56,474 : INFO : EPOCH 2 - PROGRESS: at 46.78% examples, 440322 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:46:57,474 : INFO : EPOCH 2 - PROGRESS: at 55.54% examples, 458528 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:46:58,491 : INFO : EPOCH 2 - PROGRESS: at 63.66% examples, 467507 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:46:59,502 : INFO : EPOCH 2 - PROGRESS: at 70.85% examples, 467293 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:00,525 : INFO : EPOCH 2 - PROGRESS: at 79.27% examples, 471938 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:01,529 : INFO : EPOCH 2 - PROGRESS: at 85.70% examples, 469115 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:02,539 : INFO : EPOCH 2 - PROGRESS: at 92.96% examples, 468837 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:03,371 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:47:03,379 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:47:03,389 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:47:03,390 : INFO : EPOCH - 2 : training on 6557240 raw words (6641536 effective words) took 14.0s, 473329 effective words/s\n",
      "2019-06-25 16:47:04,411 : INFO : EPOCH 3 - PROGRESS: at 6.37% examples, 420568 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:05,412 : INFO : EPOCH 3 - PROGRESS: at 12.91% examples, 434127 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:06,425 : INFO : EPOCH 3 - PROGRESS: at 20.41% examples, 453812 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:07,426 : INFO : EPOCH 3 - PROGRESS: at 27.43% examples, 459955 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:08,429 : INFO : EPOCH 3 - PROGRESS: at 34.38% examples, 461421 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:09,439 : INFO : EPOCH 3 - PROGRESS: at 41.82% examples, 465050 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:10,441 : INFO : EPOCH 3 - PROGRESS: at 48.88% examples, 466895 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:11,469 : INFO : EPOCH 3 - PROGRESS: at 56.23% examples, 468938 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:12,488 : INFO : EPOCH 3 - PROGRESS: at 64.20% examples, 471363 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:13,505 : INFO : EPOCH 3 - PROGRESS: at 71.44% examples, 469292 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:14,530 : INFO : EPOCH 3 - PROGRESS: at 78.07% examples, 465564 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:15,547 : INFO : EPOCH 3 - PROGRESS: at 84.29% examples, 461052 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:16,559 : INFO : EPOCH 3 - PROGRESS: at 91.47% examples, 461241 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:17,587 : INFO : EPOCH 3 - PROGRESS: at 98.32% examples, 460276 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:17,697 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:47:17,703 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:47:17,710 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:47:17,711 : INFO : EPOCH - 3 : training on 6557240 raw words (6641536 effective words) took 14.3s, 463822 effective words/s\n",
      "2019-06-25 16:47:18,738 : INFO : EPOCH 4 - PROGRESS: at 5.61% examples, 358971 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:19,744 : INFO : EPOCH 4 - PROGRESS: at 12.97% examples, 432125 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-25 16:47:20,748 : INFO : EPOCH 4 - PROGRESS: at 19.61% examples, 434193 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:21,766 : INFO : EPOCH 4 - PROGRESS: at 26.91% examples, 445776 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:22,777 : INFO : EPOCH 4 - PROGRESS: at 34.24% examples, 449542 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:23,784 : INFO : EPOCH 4 - PROGRESS: at 40.35% examples, 442513 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:24,786 : INFO : EPOCH 4 - PROGRESS: at 46.71% examples, 440411 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:25,789 : INFO : EPOCH 4 - PROGRESS: at 53.67% examples, 440348 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:26,802 : INFO : EPOCH 4 - PROGRESS: at 61.51% examples, 449619 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:27,803 : INFO : EPOCH 4 - PROGRESS: at 68.75% examples, 451523 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:28,827 : INFO : EPOCH 4 - PROGRESS: at 77.41% examples, 462022 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:29,828 : INFO : EPOCH 4 - PROGRESS: at 84.34% examples, 460169 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:30,856 : INFO : EPOCH 4 - PROGRESS: at 90.74% examples, 457513 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:31,868 : INFO : EPOCH 4 - PROGRESS: at 96.87% examples, 454421 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:32,170 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:47:32,172 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:47:32,176 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:47:32,177 : INFO : EPOCH - 4 : training on 6557240 raw words (6641536 effective words) took 14.5s, 459200 effective words/s\n",
      "2019-06-25 16:47:33,190 : INFO : EPOCH 5 - PROGRESS: at 5.85% examples, 375900 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:34,207 : INFO : EPOCH 5 - PROGRESS: at 13.99% examples, 454077 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:35,220 : INFO : EPOCH 5 - PROGRESS: at 20.70% examples, 450547 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:36,233 : INFO : EPOCH 5 - PROGRESS: at 27.44% examples, 451038 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:37,234 : INFO : EPOCH 5 - PROGRESS: at 34.23% examples, 444805 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:38,236 : INFO : EPOCH 5 - PROGRESS: at 43.21% examples, 474947 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:39,246 : INFO : EPOCH 5 - PROGRESS: at 52.48% examples, 497041 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:40,269 : INFO : EPOCH 5 - PROGRESS: at 59.41% examples, 490898 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:41,272 : INFO : EPOCH 5 - PROGRESS: at 65.69% examples, 481558 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:42,296 : INFO : EPOCH 5 - PROGRESS: at 72.62% examples, 477194 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:43,303 : INFO : EPOCH 5 - PROGRESS: at 79.45% examples, 474520 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:44,318 : INFO : EPOCH 5 - PROGRESS: at 85.35% examples, 467652 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:45,318 : INFO : EPOCH 5 - PROGRESS: at 91.69% examples, 463230 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:46,307 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:47:46,308 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:47:46,310 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:47:46,311 : INFO : EPOCH - 5 : training on 6557240 raw words (6641536 effective words) took 14.1s, 469958 effective words/s\n",
      "2019-06-25 16:47:47,326 : INFO : EPOCH 6 - PROGRESS: at 5.80% examples, 403207 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:48,330 : INFO : EPOCH 6 - PROGRESS: at 13.15% examples, 449384 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:49,332 : INFO : EPOCH 6 - PROGRESS: at 21.95% examples, 488753 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:50,353 : INFO : EPOCH 6 - PROGRESS: at 28.94% examples, 486251 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:51,373 : INFO : EPOCH 6 - PROGRESS: at 35.43% examples, 475210 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:52,388 : INFO : EPOCH 6 - PROGRESS: at 42.09% examples, 469661 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:53,393 : INFO : EPOCH 6 - PROGRESS: at 49.15% examples, 467839 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:54,406 : INFO : EPOCH 6 - PROGRESS: at 56.83% examples, 472294 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:55,448 : INFO : EPOCH 6 - PROGRESS: at 64.03% examples, 469836 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:56,473 : INFO : EPOCH 6 - PROGRESS: at 71.30% examples, 469649 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:47:57,511 : INFO : EPOCH 6 - PROGRESS: at 76.98% examples, 459079 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:58,514 : INFO : EPOCH 6 - PROGRESS: at 83.53% examples, 455928 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:47:59,542 : INFO : EPOCH 6 - PROGRESS: at 90.41% examples, 454417 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:00,554 : INFO : EPOCH 6 - PROGRESS: at 96.53% examples, 450926 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:00,957 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:48:00,969 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:48:00,974 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:48:00,975 : INFO : EPOCH - 6 : training on 6557240 raw words (6641536 effective words) took 14.7s, 452978 effective words/s\n",
      "2019-06-25 16:48:01,990 : INFO : EPOCH 7 - PROGRESS: at 5.81% examples, 384379 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:03,016 : INFO : EPOCH 7 - PROGRESS: at 12.27% examples, 400910 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:04,049 : INFO : EPOCH 7 - PROGRESS: at 18.96% examples, 415065 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:05,077 : INFO : EPOCH 7 - PROGRESS: at 27.08% examples, 439965 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:06,088 : INFO : EPOCH 7 - PROGRESS: at 34.22% examples, 445041 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:07,096 : INFO : EPOCH 7 - PROGRESS: at 42.79% examples, 464764 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:08,101 : INFO : EPOCH 7 - PROGRESS: at 50.58% examples, 470761 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:09,117 : INFO : EPOCH 7 - PROGRESS: at 57.23% examples, 467089 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:10,136 : INFO : EPOCH 7 - PROGRESS: at 64.30% examples, 465321 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:11,144 : INFO : EPOCH 7 - PROGRESS: at 71.35% examples, 466359 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:12,149 : INFO : EPOCH 7 - PROGRESS: at 78.82% examples, 467238 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:13,158 : INFO : EPOCH 7 - PROGRESS: at 85.70% examples, 467918 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:14,164 : INFO : EPOCH 7 - PROGRESS: at 93.42% examples, 470138 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:14,870 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:48:14,889 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:48:14,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:48:14,892 : INFO : EPOCH - 7 : training on 6557240 raw words (6641536 effective words) took 13.9s, 477334 effective words/s\n",
      "2019-06-25 16:48:15,906 : INFO : EPOCH 8 - PROGRESS: at 5.71% examples, 383694 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:16,935 : INFO : EPOCH 8 - PROGRESS: at 13.19% examples, 425340 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:17,960 : INFO : EPOCH 8 - PROGRESS: at 20.26% examples, 439451 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:18,977 : INFO : EPOCH 8 - PROGRESS: at 26.74% examples, 435141 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:19,981 : INFO : EPOCH 8 - PROGRESS: at 33.57% examples, 439086 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:21,007 : INFO : EPOCH 8 - PROGRESS: at 41.78% examples, 458330 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-25 16:48:22,034 : INFO : EPOCH 8 - PROGRESS: at 49.06% examples, 459523 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:23,082 : INFO : EPOCH 8 - PROGRESS: at 55.51% examples, 451956 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:24,083 : INFO : EPOCH 8 - PROGRESS: at 62.34% examples, 450664 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:25,095 : INFO : EPOCH 8 - PROGRESS: at 69.95% examples, 454813 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:26,104 : INFO : EPOCH 8 - PROGRESS: at 77.71% examples, 461935 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:27,139 : INFO : EPOCH 8 - PROGRESS: at 84.74% examples, 461061 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:28,140 : INFO : EPOCH 8 - PROGRESS: at 92.27% examples, 463227 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:28,975 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:48:28,994 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:48:28,995 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:48:28,995 : INFO : EPOCH - 8 : training on 6557240 raw words (6641536 effective words) took 14.1s, 470989 effective words/s\n",
      "2019-06-25 16:48:30,010 : INFO : EPOCH 9 - PROGRESS: at 6.25% examples, 413970 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:31,032 : INFO : EPOCH 9 - PROGRESS: at 13.07% examples, 426193 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:32,039 : INFO : EPOCH 9 - PROGRESS: at 19.12% examples, 422846 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:33,054 : INFO : EPOCH 9 - PROGRESS: at 25.98% examples, 423283 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:34,061 : INFO : EPOCH 9 - PROGRESS: at 32.67% examples, 423813 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:35,077 : INFO : EPOCH 9 - PROGRESS: at 39.56% examples, 428396 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:36,079 : INFO : EPOCH 9 - PROGRESS: at 47.67% examples, 446543 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:37,105 : INFO : EPOCH 9 - PROGRESS: at 55.65% examples, 454059 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:38,112 : INFO : EPOCH 9 - PROGRESS: at 62.01% examples, 449898 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:39,139 : INFO : EPOCH 9 - PROGRESS: at 70.21% examples, 458459 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:40,177 : INFO : EPOCH 9 - PROGRESS: at 77.07% examples, 457838 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:41,189 : INFO : EPOCH 9 - PROGRESS: at 83.47% examples, 455840 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:42,222 : INFO : EPOCH 9 - PROGRESS: at 90.62% examples, 456355 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:43,225 : INFO : EPOCH 9 - PROGRESS: at 97.29% examples, 455080 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:43,454 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:48:43,461 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:48:43,464 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:48:43,465 : INFO : EPOCH - 9 : training on 6557240 raw words (6641536 effective words) took 14.5s, 459071 effective words/s\n",
      "2019-06-25 16:48:44,485 : INFO : EPOCH 10 - PROGRESS: at 6.31% examples, 423682 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:45,506 : INFO : EPOCH 10 - PROGRESS: at 12.99% examples, 436302 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:46,520 : INFO : EPOCH 10 - PROGRESS: at 19.68% examples, 437927 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:47,522 : INFO : EPOCH 10 - PROGRESS: at 26.28% examples, 433309 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:48,542 : INFO : EPOCH 10 - PROGRESS: at 33.16% examples, 437007 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:49,556 : INFO : EPOCH 10 - PROGRESS: at 42.01% examples, 460986 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:50,573 : INFO : EPOCH 10 - PROGRESS: at 49.69% examples, 466575 words/s, in_qsize 6, out_qsize 1\n",
      "2019-06-25 16:48:51,586 : INFO : EPOCH 10 - PROGRESS: at 56.34% examples, 462300 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:52,596 : INFO : EPOCH 10 - PROGRESS: at 63.32% examples, 462412 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:53,602 : INFO : EPOCH 10 - PROGRESS: at 70.12% examples, 459840 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:48:54,606 : INFO : EPOCH 10 - PROGRESS: at 77.19% examples, 459520 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:55,635 : INFO : EPOCH 10 - PROGRESS: at 84.26% examples, 460766 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:56,656 : INFO : EPOCH 10 - PROGRESS: at 90.97% examples, 457634 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:57,696 : INFO : EPOCH 10 - PROGRESS: at 98.63% examples, 460622 words/s, in_qsize 5, out_qsize 1\n",
      "2019-06-25 16:48:57,768 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:48:57,772 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:48:57,782 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:48:57,783 : INFO : EPOCH - 10 : training on 6557240 raw words (6641536 effective words) took 14.3s, 463915 effective words/s\n",
      "2019-06-25 16:48:58,786 : INFO : EPOCH 11 - PROGRESS: at 7.27% examples, 478212 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:48:59,797 : INFO : EPOCH 11 - PROGRESS: at 14.36% examples, 470302 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:00,808 : INFO : EPOCH 11 - PROGRESS: at 21.40% examples, 472297 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:01,841 : INFO : EPOCH 11 - PROGRESS: at 29.16% examples, 477951 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:02,843 : INFO : EPOCH 11 - PROGRESS: at 35.88% examples, 470321 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:03,848 : INFO : EPOCH 11 - PROGRESS: at 42.39% examples, 461475 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:04,874 : INFO : EPOCH 11 - PROGRESS: at 49.31% examples, 456783 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:05,895 : INFO : EPOCH 11 - PROGRESS: at 56.86% examples, 462006 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:06,913 : INFO : EPOCH 11 - PROGRESS: at 64.41% examples, 466148 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:07,933 : INFO : EPOCH 11 - PROGRESS: at 71.42% examples, 465394 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:08,935 : INFO : EPOCH 11 - PROGRESS: at 78.35% examples, 465667 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:09,972 : INFO : EPOCH 11 - PROGRESS: at 85.05% examples, 463691 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:10,978 : INFO : EPOCH 11 - PROGRESS: at 93.48% examples, 470798 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:11,786 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:49:11,799 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:49:11,801 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:49:11,802 : INFO : EPOCH - 11 : training on 6557240 raw words (6641536 effective words) took 14.0s, 473792 effective words/s\n",
      "2019-06-25 16:49:12,813 : INFO : EPOCH 12 - PROGRESS: at 6.92% examples, 466184 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:13,825 : INFO : EPOCH 12 - PROGRESS: at 13.71% examples, 454836 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:14,827 : INFO : EPOCH 12 - PROGRESS: at 21.30% examples, 472542 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:15,854 : INFO : EPOCH 12 - PROGRESS: at 28.00% examples, 465678 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:16,870 : INFO : EPOCH 12 - PROGRESS: at 34.47% examples, 458902 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:17,885 : INFO : EPOCH 12 - PROGRESS: at 41.66% examples, 459435 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:18,887 : INFO : EPOCH 12 - PROGRESS: at 48.42% examples, 455043 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:19,891 : INFO : EPOCH 12 - PROGRESS: at 55.13% examples, 450556 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:20,891 : INFO : EPOCH 12 - PROGRESS: at 61.68% examples, 449384 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:21,911 : INFO : EPOCH 12 - PROGRESS: at 68.69% examples, 449411 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-25 16:49:22,914 : INFO : EPOCH 12 - PROGRESS: at 75.44% examples, 449185 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:23,919 : INFO : EPOCH 12 - PROGRESS: at 82.78% examples, 451470 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:24,928 : INFO : EPOCH 12 - PROGRESS: at 89.30% examples, 450130 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:25,946 : INFO : EPOCH 12 - PROGRESS: at 96.64% examples, 453531 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:26,334 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:49:26,355 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:49:26,358 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:49:26,359 : INFO : EPOCH - 12 : training on 6557240 raw words (6641536 effective words) took 14.6s, 456310 effective words/s\n",
      "2019-06-25 16:49:27,367 : INFO : EPOCH 13 - PROGRESS: at 6.09% examples, 406045 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:28,372 : INFO : EPOCH 13 - PROGRESS: at 12.29% examples, 401396 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:29,373 : INFO : EPOCH 13 - PROGRESS: at 19.05% examples, 423564 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:30,384 : INFO : EPOCH 13 - PROGRESS: at 25.52% examples, 425883 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:31,403 : INFO : EPOCH 13 - PROGRESS: at 33.44% examples, 444737 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:32,420 : INFO : EPOCH 13 - PROGRESS: at 41.09% examples, 453967 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:33,430 : INFO : EPOCH 13 - PROGRESS: at 47.85% examples, 455338 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:34,451 : INFO : EPOCH 13 - PROGRESS: at 54.86% examples, 455872 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:35,466 : INFO : EPOCH 13 - PROGRESS: at 61.84% examples, 453374 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:36,485 : INFO : EPOCH 13 - PROGRESS: at 69.68% examples, 458936 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:37,507 : INFO : EPOCH 13 - PROGRESS: at 77.13% examples, 461595 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:38,516 : INFO : EPOCH 13 - PROGRESS: at 84.77% examples, 465884 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:39,527 : INFO : EPOCH 13 - PROGRESS: at 90.66% examples, 459026 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:40,554 : INFO : EPOCH 13 - PROGRESS: at 98.08% examples, 459594 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:40,690 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:49:40,697 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:49:40,698 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:49:40,699 : INFO : EPOCH - 13 : training on 6557240 raw words (6641536 effective words) took 14.3s, 463236 effective words/s\n",
      "2019-06-25 16:49:41,712 : INFO : EPOCH 14 - PROGRESS: at 6.51% examples, 423699 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:42,714 : INFO : EPOCH 14 - PROGRESS: at 13.43% examples, 440662 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:43,738 : INFO : EPOCH 14 - PROGRESS: at 20.42% examples, 452786 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:44,742 : INFO : EPOCH 14 - PROGRESS: at 27.68% examples, 461356 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:45,745 : INFO : EPOCH 14 - PROGRESS: at 35.73% examples, 472321 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:46,773 : INFO : EPOCH 14 - PROGRESS: at 42.54% examples, 466645 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:47,781 : INFO : EPOCH 14 - PROGRESS: at 49.56% examples, 465038 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:48,785 : INFO : EPOCH 14 - PROGRESS: at 57.23% examples, 467541 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:49,810 : INFO : EPOCH 14 - PROGRESS: at 65.04% examples, 471874 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:50,812 : INFO : EPOCH 14 - PROGRESS: at 72.30% examples, 473414 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:51,829 : INFO : EPOCH 14 - PROGRESS: at 78.97% examples, 470582 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:52,845 : INFO : EPOCH 14 - PROGRESS: at 86.34% examples, 471413 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:53,860 : INFO : EPOCH 14 - PROGRESS: at 94.14% examples, 474440 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:54,586 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:49:54,599 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:49:54,601 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:49:54,602 : INFO : EPOCH - 14 : training on 6557240 raw words (6641536 effective words) took 13.9s, 477764 effective words/s\n",
      "2019-06-25 16:49:55,620 : INFO : EPOCH 15 - PROGRESS: at 7.35% examples, 479294 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:56,621 : INFO : EPOCH 15 - PROGRESS: at 14.38% examples, 479176 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:49:57,652 : INFO : EPOCH 15 - PROGRESS: at 21.96% examples, 484214 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:58,652 : INFO : EPOCH 15 - PROGRESS: at 29.28% examples, 482865 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:49:59,661 : INFO : EPOCH 15 - PROGRESS: at 36.32% examples, 477567 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:00,690 : INFO : EPOCH 15 - PROGRESS: at 43.70% examples, 480526 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:01,695 : INFO : EPOCH 15 - PROGRESS: at 51.67% examples, 485750 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:02,712 : INFO : EPOCH 15 - PROGRESS: at 59.18% examples, 483943 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:03,731 : INFO : EPOCH 15 - PROGRESS: at 66.84% examples, 487547 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:04,752 : INFO : EPOCH 15 - PROGRESS: at 74.21% examples, 488694 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:05,781 : INFO : EPOCH 15 - PROGRESS: at 82.21% examples, 491715 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:06,803 : INFO : EPOCH 15 - PROGRESS: at 90.29% examples, 492292 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:07,809 : INFO : EPOCH 15 - PROGRESS: at 97.38% examples, 489632 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:08,111 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:50:08,123 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:50:08,124 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:50:08,125 : INFO : EPOCH - 15 : training on 6557240 raw words (6641536 effective words) took 13.5s, 491204 effective words/s\n",
      "2019-06-25 16:50:08,126 : INFO : training on a 98358600 raw words (99623040 effective words) took 213.1s, 467448 effective words/s\n",
      "2019-06-25 16:50:08,134 : INFO : collecting all words and their counts\n",
      "2019-06-25 16:50:08,153 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [doc2vec dbow]: 0:03:38.603210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-25 16:50:08,639 : INFO : PROGRESS: at example #10000, processed 783936 words (1618307/s), 22151 word types, 2092 tags\n",
      "2019-06-25 16:50:09,115 : INFO : PROGRESS: at example #20000, processed 1536908 words (1582897/s), 29368 word types, 3456 tags\n",
      "2019-06-25 16:50:09,615 : INFO : PROGRESS: at example #30000, processed 2296828 words (1522377/s), 33431 word types, 4511 tags\n",
      "2019-06-25 16:50:10,166 : INFO : PROGRESS: at example #40000, processed 3062193 words (1393030/s), 35456 word types, 5303 tags\n",
      "2019-06-25 16:50:10,717 : INFO : PROGRESS: at example #50000, processed 3838871 words (1410411/s), 36694 word types, 5922 tags\n",
      "2019-06-25 16:50:11,271 : INFO : PROGRESS: at example #60000, processed 4622579 words (1417264/s), 37310 word types, 6403 tags\n",
      "2019-06-25 16:50:11,894 : INFO : PROGRESS: at example #70000, processed 5368365 words (1198921/s), 37687 word types, 6765 tags\n",
      "2019-06-25 16:50:12,485 : INFO : PROGRESS: at example #80000, processed 6121167 words (1275475/s), 37995 word types, 7091 tags\n",
      "2019-06-25 16:50:12,850 : INFO : collected 38071 word types and 7236 unique tags from a corpus of 85466 examples and 6557240 words\n",
      "2019-06-25 16:50:12,851 : INFO : Loading a fresh vocabulary\n",
      "2019-06-25 16:50:13,818 : INFO : effective_min_count=1 retains 38071 unique words (100% of original 38071, drops 0)\n",
      "2019-06-25 16:50:13,819 : INFO : effective_min_count=1 leaves 6557240 word corpus (100% of original 6557240, drops 0)\n",
      "2019-06-25 16:50:13,914 : INFO : deleting the raw counts dictionary of 38071 items\n",
      "2019-06-25 16:50:13,916 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-06-25 16:50:13,916 : INFO : downsampling leaves estimated 5593924 word corpus (85.3% of prior 6557240)\n",
      "2019-06-25 16:50:14,051 : INFO : estimated required memory for 38071 words and 300 dimensions: 120536300 bytes\n",
      "2019-06-25 16:50:14,051 : INFO : resetting layer weights\n",
      "2019-06-25 16:50:14,444 : INFO : training model with 3 workers on 38071 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-06-25 16:50:15,450 : INFO : EPOCH 1 - PROGRESS: at 5.18% examples, 281207 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:16,465 : INFO : EPOCH 1 - PROGRESS: at 11.07% examples, 312816 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:17,480 : INFO : EPOCH 1 - PROGRESS: at 17.53% examples, 331942 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:18,489 : INFO : EPOCH 1 - PROGRESS: at 22.82% examples, 327108 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:19,544 : INFO : EPOCH 1 - PROGRESS: at 28.32% examples, 323030 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:20,561 : INFO : EPOCH 1 - PROGRESS: at 34.18% examples, 322389 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:21,576 : INFO : EPOCH 1 - PROGRESS: at 40.30% examples, 325679 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:22,592 : INFO : EPOCH 1 - PROGRESS: at 46.25% examples, 325940 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:23,611 : INFO : EPOCH 1 - PROGRESS: at 52.21% examples, 324100 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:24,622 : INFO : EPOCH 1 - PROGRESS: at 58.33% examples, 326275 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:25,644 : INFO : EPOCH 1 - PROGRESS: at 63.78% examples, 323275 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:26,661 : INFO : EPOCH 1 - PROGRESS: at 69.63% examples, 322852 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:27,664 : INFO : EPOCH 1 - PROGRESS: at 76.44% examples, 328842 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:28,666 : INFO : EPOCH 1 - PROGRESS: at 81.82% examples, 327168 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:29,677 : INFO : EPOCH 1 - PROGRESS: at 88.44% examples, 329512 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:30,694 : INFO : EPOCH 1 - PROGRESS: at 94.20% examples, 329330 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:31,486 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:50:31,499 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:50:31,510 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:50:31,512 : INFO : EPOCH - 1 : training on 6557240 raw words (5679487 effective words) took 17.1s, 332797 effective words/s\n",
      "2019-06-25 16:50:32,565 : INFO : EPOCH 2 - PROGRESS: at 4.92% examples, 275910 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:33,578 : INFO : EPOCH 2 - PROGRESS: at 10.36% examples, 285296 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:34,588 : INFO : EPOCH 2 - PROGRESS: at 15.73% examples, 294121 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:35,607 : INFO : EPOCH 2 - PROGRESS: at 21.24% examples, 295976 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:36,611 : INFO : EPOCH 2 - PROGRESS: at 26.72% examples, 298238 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:37,648 : INFO : EPOCH 2 - PROGRESS: at 32.75% examples, 306204 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:38,671 : INFO : EPOCH 2 - PROGRESS: at 38.61% examples, 306464 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:39,703 : INFO : EPOCH 2 - PROGRESS: at 44.96% examples, 309562 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:40,704 : INFO : EPOCH 2 - PROGRESS: at 50.53% examples, 311045 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:41,715 : INFO : EPOCH 2 - PROGRESS: at 56.38% examples, 311917 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:42,753 : INFO : EPOCH 2 - PROGRESS: at 61.80% examples, 310530 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:43,763 : INFO : EPOCH 2 - PROGRESS: at 66.87% examples, 308555 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:44,788 : INFO : EPOCH 2 - PROGRESS: at 73.69% examples, 314244 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:45,825 : INFO : EPOCH 2 - PROGRESS: at 79.38% examples, 313538 words/s, in_qsize 4, out_qsize 1\n",
      "2019-06-25 16:50:46,825 : INFO : EPOCH 2 - PROGRESS: at 84.87% examples, 313751 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:47,843 : INFO : EPOCH 2 - PROGRESS: at 90.83% examples, 315701 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:48,844 : INFO : EPOCH 2 - PROGRESS: at 97.18% examples, 318636 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:49,233 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:50:49,234 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:50:49,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:50:49,254 : INFO : EPOCH - 2 : training on 6557240 raw words (5678713 effective words) took 17.7s, 320106 effective words/s\n",
      "2019-06-25 16:50:50,262 : INFO : EPOCH 3 - PROGRESS: at 4.94% examples, 289041 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:51,273 : INFO : EPOCH 3 - PROGRESS: at 10.64% examples, 310380 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:52,303 : INFO : EPOCH 3 - PROGRESS: at 15.64% examples, 300644 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:53,326 : INFO : EPOCH 3 - PROGRESS: at 20.97% examples, 296366 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:54,357 : INFO : EPOCH 3 - PROGRESS: at 27.53% examples, 306909 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:55,387 : INFO : EPOCH 3 - PROGRESS: at 34.01% examples, 315134 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:56,390 : INFO : EPOCH 3 - PROGRESS: at 40.35% examples, 322285 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:50:57,397 : INFO : EPOCH 3 - PROGRESS: at 45.67% examples, 320163 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:58,413 : INFO : EPOCH 3 - PROGRESS: at 51.27% examples, 318053 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:50:59,430 : INFO : EPOCH 3 - PROGRESS: at 57.59% examples, 322378 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:00,440 : INFO : EPOCH 3 - PROGRESS: at 63.52% examples, 323139 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:01,459 : INFO : EPOCH 3 - PROGRESS: at 69.52% examples, 324920 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:02,468 : INFO : EPOCH 3 - PROGRESS: at 74.97% examples, 322753 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:03,469 : INFO : EPOCH 3 - PROGRESS: at 80.51% examples, 321569 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:04,481 : INFO : EPOCH 3 - PROGRESS: at 85.96% examples, 320876 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-25 16:51:05,500 : INFO : EPOCH 3 - PROGRESS: at 91.26% examples, 319175 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:06,507 : INFO : EPOCH 3 - PROGRESS: at 96.79% examples, 318314 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:07,027 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:51:07,028 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:51:07,039 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:51:07,040 : INFO : EPOCH - 3 : training on 6557240 raw words (5679793 effective words) took 17.8s, 319402 effective words/s\n",
      "2019-06-25 16:51:08,064 : INFO : EPOCH 4 - PROGRESS: at 5.95% examples, 326630 words/s, in_qsize 5, out_qsize 1\n",
      "2019-06-25 16:51:09,068 : INFO : EPOCH 4 - PROGRESS: at 11.61% examples, 320566 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:10,078 : INFO : EPOCH 4 - PROGRESS: at 18.07% examples, 329751 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:11,103 : INFO : EPOCH 4 - PROGRESS: at 23.80% examples, 328566 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:12,137 : INFO : EPOCH 4 - PROGRESS: at 29.62% examples, 327356 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:13,179 : INFO : EPOCH 4 - PROGRESS: at 36.94% examples, 337267 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:14,189 : INFO : EPOCH 4 - PROGRESS: at 43.40% examples, 340839 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:15,231 : INFO : EPOCH 4 - PROGRESS: at 49.33% examples, 339046 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:16,263 : INFO : EPOCH 4 - PROGRESS: at 54.89% examples, 336215 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:17,271 : INFO : EPOCH 4 - PROGRESS: at 60.46% examples, 334691 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:18,294 : INFO : EPOCH 4 - PROGRESS: at 65.90% examples, 332272 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:19,311 : INFO : EPOCH 4 - PROGRESS: at 72.78% examples, 334672 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:20,335 : INFO : EPOCH 4 - PROGRESS: at 79.38% examples, 337238 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:21,345 : INFO : EPOCH 4 - PROGRESS: at 84.56% examples, 334331 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:22,368 : INFO : EPOCH 4 - PROGRESS: at 89.99% examples, 332662 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:23,393 : INFO : EPOCH 4 - PROGRESS: at 96.26% examples, 334221 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:23,927 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:51:23,957 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:51:23,959 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:51:23,959 : INFO : EPOCH - 4 : training on 6557240 raw words (5679672 effective words) took 16.9s, 335732 effective words/s\n",
      "2019-06-25 16:51:24,971 : INFO : EPOCH 5 - PROGRESS: at 5.25% examples, 278590 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:26,004 : INFO : EPOCH 5 - PROGRESS: at 10.36% examples, 279765 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:27,011 : INFO : EPOCH 5 - PROGRESS: at 17.18% examples, 310604 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:28,034 : INFO : EPOCH 5 - PROGRESS: at 23.36% examples, 314184 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:29,075 : INFO : EPOCH 5 - PROGRESS: at 29.12% examples, 313603 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:30,084 : INFO : EPOCH 5 - PROGRESS: at 35.25% examples, 319016 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:31,109 : INFO : EPOCH 5 - PROGRESS: at 40.98% examples, 322214 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:32,128 : INFO : EPOCH 5 - PROGRESS: at 47.00% examples, 323914 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:33,132 : INFO : EPOCH 5 - PROGRESS: at 53.43% examples, 328174 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:34,173 : INFO : EPOCH 5 - PROGRESS: at 59.23% examples, 325714 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:35,181 : INFO : EPOCH 5 - PROGRESS: at 64.87% examples, 326288 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:36,207 : INFO : EPOCH 5 - PROGRESS: at 71.45% examples, 329708 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:37,213 : INFO : EPOCH 5 - PROGRESS: at 77.40% examples, 329852 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:38,213 : INFO : EPOCH 5 - PROGRESS: at 83.49% examples, 331163 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:39,224 : INFO : EPOCH 5 - PROGRESS: at 88.92% examples, 330065 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:40,227 : INFO : EPOCH 5 - PROGRESS: at 94.39% examples, 329180 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:41,023 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:51:41,024 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:51:41,036 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:51:41,037 : INFO : EPOCH - 5 : training on 6557240 raw words (5681054 effective words) took 17.1s, 332714 effective words/s\n",
      "2019-06-25 16:51:42,049 : INFO : EPOCH 6 - PROGRESS: at 5.32% examples, 295575 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:43,054 : INFO : EPOCH 6 - PROGRESS: at 11.34% examples, 309438 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:44,069 : INFO : EPOCH 6 - PROGRESS: at 17.26% examples, 324215 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:45,076 : INFO : EPOCH 6 - PROGRESS: at 22.88% examples, 325479 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:46,103 : INFO : EPOCH 6 - PROGRESS: at 29.06% examples, 328554 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:47,114 : INFO : EPOCH 6 - PROGRESS: at 34.87% examples, 324532 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:48,143 : INFO : EPOCH 6 - PROGRESS: at 40.31% examples, 321853 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:49,156 : INFO : EPOCH 6 - PROGRESS: at 46.21% examples, 322672 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:50,179 : INFO : EPOCH 6 - PROGRESS: at 52.09% examples, 322899 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:51,182 : INFO : EPOCH 6 - PROGRESS: at 57.96% examples, 323677 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:52,204 : INFO : EPOCH 6 - PROGRESS: at 63.75% examples, 323178 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:53,217 : INFO : EPOCH 6 - PROGRESS: at 69.54% examples, 324283 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:54,219 : INFO : EPOCH 6 - PROGRESS: at 75.38% examples, 324299 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:55,222 : INFO : EPOCH 6 - PROGRESS: at 81.34% examples, 324899 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:51:56,250 : INFO : EPOCH 6 - PROGRESS: at 87.44% examples, 325474 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:57,272 : INFO : EPOCH 6 - PROGRESS: at 93.33% examples, 325445 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:51:58,270 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:51:58,285 : INFO : EPOCH 6 - PROGRESS: at 99.89% examples, 328878 words/s, in_qsize 1, out_qsize 1\n",
      "2019-06-25 16:51:58,286 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:51:58,295 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:51:58,296 : INFO : EPOCH - 6 : training on 6557240 raw words (5680772 effective words) took 17.3s, 329181 effective words/s\n",
      "2019-06-25 16:51:59,315 : INFO : EPOCH 7 - PROGRESS: at 5.37% examples, 294903 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:00,317 : INFO : EPOCH 7 - PROGRESS: at 11.05% examples, 317605 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:52:01,346 : INFO : EPOCH 7 - PROGRESS: at 16.97% examples, 322533 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:52:02,351 : INFO : EPOCH 7 - PROGRESS: at 23.18% examples, 328771 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:03,364 : INFO : EPOCH 7 - PROGRESS: at 29.03% examples, 330519 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:04,371 : INFO : EPOCH 7 - PROGRESS: at 34.98% examples, 332173 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:05,405 : INFO : EPOCH 7 - PROGRESS: at 40.76% examples, 329296 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-25 16:52:06,432 : INFO : EPOCH 7 - PROGRESS: at 46.51% examples, 326461 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:07,439 : INFO : EPOCH 7 - PROGRESS: at 53.01% examples, 332444 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:08,470 : INFO : EPOCH 7 - PROGRESS: at 58.02% examples, 327326 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:09,523 : INFO : EPOCH 7 - PROGRESS: at 64.06% examples, 324790 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:52:10,531 : INFO : EPOCH 7 - PROGRESS: at 69.63% examples, 323848 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:11,568 : INFO : EPOCH 7 - PROGRESS: at 75.51% examples, 324155 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:12,583 : INFO : EPOCH 7 - PROGRESS: at 81.53% examples, 324399 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:13,596 : INFO : EPOCH 7 - PROGRESS: at 87.26% examples, 324642 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:14,602 : INFO : EPOCH 7 - PROGRESS: at 93.34% examples, 325028 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:15,609 : INFO : EPOCH 7 - PROGRESS: at 99.41% examples, 326238 words/s, in_qsize 4, out_qsize 0\n",
      "2019-06-25 16:52:15,655 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:52:15,662 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:52:15,667 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:52:15,667 : INFO : EPOCH - 7 : training on 6557240 raw words (5680151 effective words) took 17.4s, 327018 effective words/s\n",
      "2019-06-25 16:52:16,672 : INFO : EPOCH 8 - PROGRESS: at 5.01% examples, 280455 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:17,674 : INFO : EPOCH 8 - PROGRESS: at 10.50% examples, 302239 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:18,687 : INFO : EPOCH 8 - PROGRESS: at 15.89% examples, 303133 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:19,698 : INFO : EPOCH 8 - PROGRESS: at 20.75% examples, 294975 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:20,732 : INFO : EPOCH 8 - PROGRESS: at 27.06% examples, 307523 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:52:21,735 : INFO : EPOCH 8 - PROGRESS: at 33.55% examples, 317269 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:22,758 : INFO : EPOCH 8 - PROGRESS: at 40.46% examples, 324493 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:23,770 : INFO : EPOCH 8 - PROGRESS: at 46.80% examples, 328338 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:24,800 : INFO : EPOCH 8 - PROGRESS: at 53.71% examples, 331407 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:25,814 : INFO : EPOCH 8 - PROGRESS: at 59.40% examples, 329374 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:52:26,816 : INFO : EPOCH 8 - PROGRESS: at 65.51% examples, 331949 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:27,845 : INFO : EPOCH 8 - PROGRESS: at 71.46% examples, 331913 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:52:28,846 : INFO : EPOCH 8 - PROGRESS: at 77.06% examples, 330024 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:29,852 : INFO : EPOCH 8 - PROGRESS: at 83.26% examples, 331887 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:30,879 : INFO : EPOCH 8 - PROGRESS: at 88.73% examples, 330235 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:31,879 : INFO : EPOCH 8 - PROGRESS: at 94.27% examples, 329235 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:32,654 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:52:32,666 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:52:32,673 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:52:32,674 : INFO : EPOCH - 8 : training on 6557240 raw words (5679509 effective words) took 17.0s, 333991 effective words/s\n",
      "2019-06-25 16:52:33,679 : INFO : EPOCH 9 - PROGRESS: at 4.99% examples, 298836 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:34,705 : INFO : EPOCH 9 - PROGRESS: at 10.57% examples, 298244 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:35,713 : INFO : EPOCH 9 - PROGRESS: at 16.19% examples, 303388 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:52:36,741 : INFO : EPOCH 9 - PROGRESS: at 21.02% examples, 294186 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:52:37,792 : INFO : EPOCH 9 - PROGRESS: at 26.76% examples, 295526 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:38,823 : INFO : EPOCH 9 - PROGRESS: at 33.68% examples, 308057 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:39,827 : INFO : EPOCH 9 - PROGRESS: at 39.47% examples, 311599 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:40,838 : INFO : EPOCH 9 - PROGRESS: at 45.96% examples, 317880 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:41,874 : INFO : EPOCH 9 - PROGRESS: at 52.26% examples, 320085 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:42,890 : INFO : EPOCH 9 - PROGRESS: at 57.90% examples, 319996 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:43,904 : INFO : EPOCH 9 - PROGRESS: at 64.31% examples, 322916 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:44,910 : INFO : EPOCH 9 - PROGRESS: at 70.46% examples, 324842 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:45,932 : INFO : EPOCH 9 - PROGRESS: at 75.98% examples, 323631 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:46,932 : INFO : EPOCH 9 - PROGRESS: at 81.43% examples, 323138 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:47,936 : INFO : EPOCH 9 - PROGRESS: at 86.67% examples, 322038 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:48,952 : INFO : EPOCH 9 - PROGRESS: at 93.39% examples, 325560 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:49,976 : INFO : EPOCH 9 - PROGRESS: at 99.26% examples, 325997 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:50,005 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:52:50,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:52:50,023 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:52:50,023 : INFO : EPOCH - 9 : training on 6557240 raw words (5679725 effective words) took 17.3s, 327427 effective words/s\n",
      "2019-06-25 16:52:51,056 : INFO : EPOCH 10 - PROGRESS: at 5.42% examples, 282066 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:52,062 : INFO : EPOCH 10 - PROGRESS: at 11.50% examples, 318827 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:53,092 : INFO : EPOCH 10 - PROGRESS: at 17.56% examples, 320000 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:54,143 : INFO : EPOCH 10 - PROGRESS: at 23.68% examples, 317135 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:55,172 : INFO : EPOCH 10 - PROGRESS: at 29.12% examples, 314883 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:56,224 : INFO : EPOCH 10 - PROGRESS: at 34.96% examples, 315141 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:57,232 : INFO : EPOCH 10 - PROGRESS: at 42.05% examples, 327003 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:52:58,273 : INFO : EPOCH 10 - PROGRESS: at 48.36% examples, 327976 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:52:59,276 : INFO : EPOCH 10 - PROGRESS: at 54.58% examples, 330174 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:00,280 : INFO : EPOCH 10 - PROGRESS: at 59.99% examples, 328510 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:01,306 : INFO : EPOCH 10 - PROGRESS: at 65.98% examples, 329667 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:02,324 : INFO : EPOCH 10 - PROGRESS: at 71.25% examples, 326704 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:03,345 : INFO : EPOCH 10 - PROGRESS: at 77.82% examples, 330490 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:04,366 : INFO : EPOCH 10 - PROGRESS: at 84.12% examples, 332673 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:05,404 : INFO : EPOCH 10 - PROGRESS: at 89.58% examples, 331347 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:06,408 : INFO : EPOCH 10 - PROGRESS: at 95.83% examples, 332287 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:07,025 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:53:07,030 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:53:07,038 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:53:07,039 : INFO : EPOCH - 10 : training on 6557240 raw words (5678860 effective words) took 17.0s, 333800 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-25 16:53:08,059 : INFO : EPOCH 11 - PROGRESS: at 6.08% examples, 344146 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:09,067 : INFO : EPOCH 11 - PROGRESS: at 11.67% examples, 328305 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:10,088 : INFO : EPOCH 11 - PROGRESS: at 17.83% examples, 332504 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:11,113 : INFO : EPOCH 11 - PROGRESS: at 23.73% examples, 332751 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:12,143 : INFO : EPOCH 11 - PROGRESS: at 29.76% examples, 335429 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:13,151 : INFO : EPOCH 11 - PROGRESS: at 35.75% examples, 334418 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:14,154 : INFO : EPOCH 11 - PROGRESS: at 41.66% examples, 334127 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:53:15,174 : INFO : EPOCH 11 - PROGRESS: at 47.57% examples, 333221 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:16,215 : INFO : EPOCH 11 - PROGRESS: at 53.08% examples, 330731 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:17,217 : INFO : EPOCH 11 - PROGRESS: at 58.66% examples, 329249 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:53:18,235 : INFO : EPOCH 11 - PROGRESS: at 64.59% examples, 329980 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:19,253 : INFO : EPOCH 11 - PROGRESS: at 71.15% examples, 331842 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:20,253 : INFO : EPOCH 11 - PROGRESS: at 76.49% examples, 329920 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:21,267 : INFO : EPOCH 11 - PROGRESS: at 81.67% examples, 326235 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:53:22,280 : INFO : EPOCH 11 - PROGRESS: at 86.89% examples, 324193 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:23,296 : INFO : EPOCH 11 - PROGRESS: at 92.97% examples, 324985 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:24,302 : INFO : EPOCH 11 - PROGRESS: at 99.10% examples, 326330 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:24,363 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:53:24,366 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:53:24,371 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:53:24,372 : INFO : EPOCH - 11 : training on 6557240 raw words (5680048 effective words) took 17.3s, 327746 effective words/s\n",
      "2019-06-25 16:53:25,387 : INFO : EPOCH 12 - PROGRESS: at 5.32% examples, 304004 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:26,398 : INFO : EPOCH 12 - PROGRESS: at 11.73% examples, 329452 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:27,443 : INFO : EPOCH 12 - PROGRESS: at 17.60% examples, 328226 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:28,466 : INFO : EPOCH 12 - PROGRESS: at 24.16% examples, 334053 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:53:29,470 : INFO : EPOCH 12 - PROGRESS: at 30.46% examples, 340157 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:30,483 : INFO : EPOCH 12 - PROGRESS: at 36.73% examples, 342443 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:53:31,500 : INFO : EPOCH 12 - PROGRESS: at 43.38% examples, 343994 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:32,502 : INFO : EPOCH 12 - PROGRESS: at 49.06% examples, 343537 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:33,521 : INFO : EPOCH 12 - PROGRESS: at 55.25% examples, 344298 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:34,526 : INFO : EPOCH 12 - PROGRESS: at 61.35% examples, 343007 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:35,539 : INFO : EPOCH 12 - PROGRESS: at 67.41% examples, 343331 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:36,552 : INFO : EPOCH 12 - PROGRESS: at 73.23% examples, 340699 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:53:37,559 : INFO : EPOCH 12 - PROGRESS: at 79.74% examples, 342478 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:38,583 : INFO : EPOCH 12 - PROGRESS: at 85.83% examples, 342500 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:39,596 : INFO : EPOCH 12 - PROGRESS: at 91.95% examples, 343298 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:53:40,604 : INFO : EPOCH 12 - PROGRESS: at 97.32% examples, 340949 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:40,909 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:53:40,913 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:53:40,918 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:53:40,919 : INFO : EPOCH - 12 : training on 6557240 raw words (5678631 effective words) took 16.5s, 343210 effective words/s\n",
      "2019-06-25 16:53:41,931 : INFO : EPOCH 13 - PROGRESS: at 5.49% examples, 294843 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:42,940 : INFO : EPOCH 13 - PROGRESS: at 11.10% examples, 303843 words/s, in_qsize 5, out_qsize 1\n",
      "2019-06-25 16:53:43,973 : INFO : EPOCH 13 - PROGRESS: at 16.64% examples, 306965 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:44,975 : INFO : EPOCH 13 - PROGRESS: at 22.92% examples, 315258 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:46,000 : INFO : EPOCH 13 - PROGRESS: at 28.84% examples, 318671 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:47,003 : INFO : EPOCH 13 - PROGRESS: at 34.36% examples, 318044 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:53:48,015 : INFO : EPOCH 13 - PROGRESS: at 39.73% examples, 317110 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:49,024 : INFO : EPOCH 13 - PROGRESS: at 45.16% examples, 314421 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:50,048 : INFO : EPOCH 13 - PROGRESS: at 50.44% examples, 312950 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:53:51,067 : INFO : EPOCH 13 - PROGRESS: at 56.05% examples, 313510 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:52,071 : INFO : EPOCH 13 - PROGRESS: at 62.39% examples, 316569 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:53:53,083 : INFO : EPOCH 13 - PROGRESS: at 67.97% examples, 316819 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:54,102 : INFO : EPOCH 13 - PROGRESS: at 73.91% examples, 318216 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:55,112 : INFO : EPOCH 13 - PROGRESS: at 79.77% examples, 319583 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:56,121 : INFO : EPOCH 13 - PROGRESS: at 86.41% examples, 323054 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:57,147 : INFO : EPOCH 13 - PROGRESS: at 92.58% examples, 324689 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:53:58,153 : INFO : EPOCH 13 - PROGRESS: at 98.52% examples, 324629 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:53:58,307 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:53:58,308 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:53:58,310 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:53:58,310 : INFO : EPOCH - 13 : training on 6557240 raw words (5678211 effective words) took 17.4s, 326558 effective words/s\n",
      "2019-06-25 16:53:59,319 : INFO : EPOCH 14 - PROGRESS: at 4.84% examples, 262058 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:00,320 : INFO : EPOCH 14 - PROGRESS: at 11.18% examples, 305100 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:01,322 : INFO : EPOCH 14 - PROGRESS: at 16.43% examples, 308180 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:54:02,330 : INFO : EPOCH 14 - PROGRESS: at 21.33% examples, 298842 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:03,333 : INFO : EPOCH 14 - PROGRESS: at 27.90% examples, 313769 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:04,359 : INFO : EPOCH 14 - PROGRESS: at 33.98% examples, 318435 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:05,375 : INFO : EPOCH 14 - PROGRESS: at 39.97% examples, 319906 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:06,399 : INFO : EPOCH 14 - PROGRESS: at 46.16% examples, 322651 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:07,419 : INFO : EPOCH 14 - PROGRESS: at 53.66% examples, 333230 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:54:08,460 : INFO : EPOCH 14 - PROGRESS: at 60.70% examples, 338768 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:54:09,508 : INFO : EPOCH 14 - PROGRESS: at 66.47% examples, 337607 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:10,519 : INFO : EPOCH 14 - PROGRESS: at 72.38% examples, 337673 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-25 16:54:11,521 : INFO : EPOCH 14 - PROGRESS: at 77.34% examples, 334009 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:54:12,533 : INFO : EPOCH 14 - PROGRESS: at 83.12% examples, 332512 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:13,555 : INFO : EPOCH 14 - PROGRESS: at 89.36% examples, 333625 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:14,562 : INFO : EPOCH 14 - PROGRESS: at 94.32% examples, 330827 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:15,300 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:54:15,317 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:54:15,324 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:54:15,325 : INFO : EPOCH - 14 : training on 6557240 raw words (5678188 effective words) took 17.0s, 333756 effective words/s\n",
      "2019-06-25 16:54:16,376 : INFO : EPOCH 15 - PROGRESS: at 5.82% examples, 309919 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:17,396 : INFO : EPOCH 15 - PROGRESS: at 11.90% examples, 321399 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:54:18,409 : INFO : EPOCH 15 - PROGRESS: at 17.05% examples, 315078 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:19,434 : INFO : EPOCH 15 - PROGRESS: at 21.59% examples, 296708 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:54:20,453 : INFO : EPOCH 15 - PROGRESS: at 27.09% examples, 295714 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:21,457 : INFO : EPOCH 15 - PROGRESS: at 32.19% examples, 294623 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:22,459 : INFO : EPOCH 15 - PROGRESS: at 38.49% examples, 305923 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:54:23,475 : INFO : EPOCH 15 - PROGRESS: at 43.46% examples, 302221 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:54:24,504 : INFO : EPOCH 15 - PROGRESS: at 48.71% examples, 301014 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:25,516 : INFO : EPOCH 15 - PROGRESS: at 54.37% examples, 302173 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:54:26,527 : INFO : EPOCH 15 - PROGRESS: at 58.82% examples, 297820 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:27,539 : INFO : EPOCH 15 - PROGRESS: at 64.82% examples, 301307 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:28,564 : INFO : EPOCH 15 - PROGRESS: at 70.08% examples, 300446 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:29,570 : INFO : EPOCH 15 - PROGRESS: at 77.33% examples, 308622 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:30,596 : INFO : EPOCH 15 - PROGRESS: at 83.19% examples, 309057 words/s, in_qsize 6, out_qsize 0\n",
      "2019-06-25 16:54:31,602 : INFO : EPOCH 15 - PROGRESS: at 90.13% examples, 314061 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:32,632 : INFO : EPOCH 15 - PROGRESS: at 96.14% examples, 315166 words/s, in_qsize 5, out_qsize 0\n",
      "2019-06-25 16:54:33,096 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-25 16:54:33,104 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-25 16:54:33,116 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-25 16:54:33,117 : INFO : EPOCH - 15 : training on 6557240 raw words (5678209 effective words) took 17.8s, 319188 effective words/s\n",
      "2019-06-25 16:54:33,117 : INFO : training on a 98358600 raw words (85191023 effective words) took 258.7s, 329339 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [doc2vec dbow]: 0:04:24.988739\n",
      "Time for [3 - doc2vec model]: 0:08:03.592220\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ConcatenatedDoc2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5114b9533750>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel_dmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_temporary_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_doctags_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_inference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenatedDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_dbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dmm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# todo: same vocab?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ConcatenatedDoc2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "# 3. train doc2vec model\n",
    "with Timer(\"3 - doc2vec model\"):\n",
    "    model_dbow, model_dmm = train_model(X_train, X_dev, workers=3, epochs=15)\n",
    "    \n",
    "    model_dbow.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model_dmm.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    \n",
    "    model_concat = ConcatenatedDoc2Vec([model_dbow, model_dmm])  # todo: same vocab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T15:03:07.383690Z",
     "start_time": "2019-06-25T14:55:41.245002Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42733/42733 [05:10<00:00, 137.66it/s]\n",
      "100%|██████████| 18315/18315 [02:14<00:00, 146.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [4 - vectorize arguments]: 0:07:26.134867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. vectorize arguments\n",
    "with Timer(\"4 - vectorize arguments\"):\n",
    "    X_train, X_dev = make_vectors(X_train, X_dev, model_dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T15:03:15.696960Z",
     "start_time": "2019-06-25T15:03:10.060751Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 42733/42733 [00:04<00:00, 10348.53it/s]\n",
      "100%|██████████| 18315/18315 [00:01<00:00, 13090.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [5 - vector comparison of arguments]: 0:00:05.629721\n"
     ]
    }
   ],
   "source": [
    "# 5. combine two argument vectors into a single one\n",
    "# - diff / concat / ...\n",
    "with Timer(\"5 - vector comparison of arguments\"):\n",
    "    X_train_diff, X_dev_diff = make_vector_comparison(X_train, X_dev)\n",
    "\n",
    "X_train_ = X_train_diff\n",
    "X_dev_ = X_dev_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T15:05:11.285401Z",
     "start_time": "2019-06-25T15:04:15.054102Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [StandardScaler fit]: 0:00:00.208016\n",
      "Time for [StandardScaler transform]: 0:00:00.075412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekoerner/.conda/envs/argmining19-ssc/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [SVC (linear) fit]: 0:00:55.859789\n",
      "Time for [SVC predict]: 0:00:00.023363\n",
      "Time for [6 - SVM (train -> predict)]: 0:00:56.170831\n",
      "Confusion Matrix:\n",
      "[[4555 4381]\n",
      " [4149 5230]]\n",
      "\n",
      "Accuracy:  0.53 \n",
      "\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.52      0.51      0.52      8936\n",
      "        True       0.54      0.56      0.55      9379\n",
      "\n",
      "    accuracy                           0.53     18315\n",
      "   macro avg       0.53      0.53      0.53     18315\n",
      "weighted avg       0.53      0.53      0.53     18315\n",
      "\n",
      "Time for [7 - report]: 0:00:00.052794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekoerner/.conda/envs/argmining19-ssc/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# 6. train\n",
    "with Timer(\"6 - SVM (train -> predict)\"):\n",
    "    y_pred_svm = train_test_svm(X_train_, y_train, X_dev_)\n",
    "\n",
    "# 7. Evaluate\n",
    "with Timer(\"7 - report\"):\n",
    "    report_training_results(y_dev, y_pred_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T15:03:50.353142Z",
     "start_time": "2019-06-25T15:03:48.037297Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ekoerner/.conda/envs/argmining19-ssc/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ekoerner/.conda/envs/argmining19-ssc/lib/python3.7/site-packages/sklearn/utils/validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [LogisticRegression fit]: 0:00:02.222502\n",
      "Time for [LogisticRegression predict]: 0:00:00.029869\n",
      "Time for [6 - LogReg (train -> predict)]: 0:00:02.252618\n",
      "Confusion Matrix:\n",
      "[[4329 4607]\n",
      " [3928 5451]]\n",
      "\n",
      "Accuracy:  0.53 \n",
      "\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.52      0.48      0.50      8936\n",
      "        True       0.54      0.58      0.56      9379\n",
      "\n",
      "    accuracy                           0.53     18315\n",
      "   macro avg       0.53      0.53      0.53     18315\n",
      "weighted avg       0.53      0.53      0.53     18315\n",
      "\n",
      "Time for [7 - report]: 0:00:00.057793\n"
     ]
    }
   ],
   "source": [
    "# 6. train\n",
    "with Timer(\"6 - LogReg (train -> predict)\"):\n",
    "    y_pred_logreg = train_test_logreg(X_train_, y_train, X_dev_)\n",
    "\n",
    "# 7. Evaluate\n",
    "with Timer(\"7 - report\"):\n",
    "    report_training_results(y_dev, y_pred_logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-25T14:45:52.106770Z",
     "start_time": "2019-06-25T14:39:55.750Z"
    },
    "code_folding": [
     0,
     6,
     11,
     16,
     20
    ]
   },
   "outputs": [],
   "source": [
    "# old\n",
    "return\n",
    "\n",
    "asdf\n",
    "\n",
    "# 2. Lemmatizing argument1 and argument2\n",
    "with Timer(\"2 - lemmatize\"):\n",
    "    X_train = X_train.apply(get_lemma, axis=1)\n",
    "    X_dev = X_dev.apply(get_lemma, axis=1)\n",
    "\n",
    "# 3. Extracting features - 1-3 grams lemma\n",
    "with Timer(\"3 - n-grams\"):\n",
    "    X_train_, X_dev_ = extract_n_grams_features(\n",
    "        X_train, X_dev, columns=['argument1_lemmas', 'argument2_lemmas'])\n",
    "\n",
    "# 4. train\n",
    "with Timer(\"4 - SVM (train -> predict)\"):\n",
    "    y_pred = train_test_svm(X_train_, y_train, X_dev_)\n",
    "\n",
    "# 5. Evaluate\n",
    "with Timer(\"5 - report\"):\n",
    "    report_training_results(y_dev, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
