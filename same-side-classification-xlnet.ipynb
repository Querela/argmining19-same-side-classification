{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/zihangdai/xlnet/blob/master/notebooks/colab_imdb_gpu.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda env create -f environment.yml\n",
    "! conda activate argmining19-ssc && conda install -y -c conda-forge git-lfs && git lfs install && git lfs pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install sentencepiece\n",
    "! pip install absl-py\n",
    "# ! pip install tensorflow-auto-detect\n",
    "! pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('data/xlnet_cased_L-24_H-1024_A-16'):\n",
    "    ! wget https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip\n",
    "    ! mv cased_L-24_H-1024_A-16.zip data/\n",
    "    ! cd data/ && unzip cased_L-24_H-1024_A-16.zip\n",
    "else:\n",
    "    print('Have XLNet model already!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('xlnet'):\n",
    "    ! git clone https://github.com/zihangdai/xlnet.git\n",
    "else:\n",
    "    print('Should have repo already!')\n",
    "    ! cd xlnet && git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data format\n",
    "\n",
    "https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     10,
     18,
     25
    ]
   },
   "outputs": [],
   "source": [
    "data_cross_path = 'data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = 'data/same-side-classification/within-topic/{}.csv'\n",
    "\n",
    "cross_traindev_df = pd.read_csv(data_cross_path.format('training'),\n",
    "                                quotechar='\"',\n",
    "                                quoting=csv.QUOTE_ALL,\n",
    "                                encoding='utf-8',\n",
    "                                escapechar='\\\\',\n",
    "                                doublequote=False,\n",
    "                                index_col='id')\n",
    "cross_test_df = pd.read_csv(data_cross_path.format('test'),\n",
    "                            quotechar='\"',\n",
    "                            quoting=csv.QUOTE_ALL,\n",
    "                            encoding='utf-8',\n",
    "                            escapechar='\\\\',\n",
    "                            doublequote=False,\n",
    "                            index_col='id')\n",
    "\n",
    "within_traindev_df = pd.read_csv(data_within_path.format('training'),\n",
    "                                 quotechar='\"',\n",
    "                                 quoting=csv.QUOTE_ALL,\n",
    "                                 encoding='utf-8',\n",
    "                                 escapechar='\\\\',\n",
    "                                 doublequote=False,\n",
    "                                 index_col='id')\n",
    "within_test_df = pd.read_csv(data_within_path.format('test'),\n",
    "                             quotechar='\"',\n",
    "                             quoting=csv.QUOTE_ALL,\n",
    "                             encoding='utf-8',\n",
    "                             escapechar='\\\\',\n",
    "                             doublequote=False,\n",
    "                             index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "def add_tag(row):\n",
    "    title = row['topic'].lower().strip()\n",
    "    if \"abortion\" in title:\n",
    "        row['tag'] = 'abortion'\n",
    "    elif \"gay marriage\" in title:\n",
    "        row['tag'] = 'gay marriage'\n",
    "    else:\n",
    "        row['tag'] = 'NA'\n",
    "    return row\n",
    "\n",
    "\n",
    "cross_traindev_df = cross_traindev_df.apply(add_tag, axis=1)\n",
    "cross_test_df = cross_test_df.apply(add_tag, axis=1)\n",
    "\n",
    "within_traindev_df = within_traindev_df.apply(add_tag, axis=1)\n",
    "within_test_df = within_test_df.apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']]\n",
    "    y = df[['is_same_side']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=ratio, random_state=random_state, shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df)\n",
    "X_test = within_test_df\n",
    "# X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df)\n",
    "# X_test = cross_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_DIR = 'data/xlnet-in'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "    \n",
    "DATA_DIR = os.path.join(DATA_DIR, 'ssc-within')\n",
    "# DATA_DIR = os.path.join(DATA_DIR, 'ssc-cross')\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "\n",
    "train_df = X_train.join(y_train)\n",
    "dev_df = X_dev.join(y_dev)\n",
    "test_df = X_test\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'train.tsv'), 'w', encoding='utf-8') as fh:\n",
    "    fh.write(\"label\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\")\n",
    "    for _, row in tqdm(train_df.iterrows()):\n",
    "        fh.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(\n",
    "            (1 if row['is_same_side'] else 0), row['argument1_id'],\n",
    "            row['argument2_id'], row['argument1'], row['argument2']))\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'dev.tsv'), 'w', encoding='utf-8') as fh:\n",
    "    fh.write(\"label\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\")\n",
    "    for _, row in tqdm(dev_df.iterrows()):\n",
    "        fh.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(\n",
    "            (1 if row['is_same_side'] else 0), row['argument1_id'],\n",
    "            row['argument2_id'], row['argument1'], row['argument2']))\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'test.tsv'), 'w', encoding='utf-8') as fh:\n",
    "    fh.write(\"index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\")\n",
    "    for id_, row in tqdm(test_df.iterrows()):\n",
    "        fh.write(\"{}\\t{}\\t{}\\t{}\\t{}\\n\".format(id_, row['argument1_id'],\n",
    "                                               row['argument2_id'],\n",
    "                                               row['argument1'],\n",
    "                                               row['argument2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 2 data/xlnet-in/ssc-within/train.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAME = 'ssc-within' #@param{type:\"string\"}\n",
    "SCRIPTS_DIR = 'xlnet' #@param {type:\"string\"}\n",
    "DATA_DIR = 'data/xlnet-in/ssc-within' #@param {type:\"string\"}\n",
    "OUTPUT_DIR = 'data/xlnet-out/ssc-within' #@param {type:\"string\"}\n",
    "PRETRAINED_MODEL_DIR = 'data/xlnet_cased_L-24_H-1024_A-16' #@param {type:\"string\"}\n",
    "CHECKPOINT_DIR = 'data/xlnet-chkp/ssc-within' #@param {type:\"string\"}\n",
    "\n",
    "MAX_SEQ_LEN = '128'\n",
    "BATCH_SIZE = '6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model (training & evaluation)\n",
    "\n",
    "*not sure what **train/dev/test** data split is when using XLNet...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Patch file `xlnet/run_classifier.py` for using our own data\n",
    "\n",
    "See at the end a diff/patch snippet\n",
    "\n",
    "--- \n",
    "\n",
    "```python\n",
    "# at line: 343\n",
    "\n",
    "class SSCProcessor(GLUEProcessor):\n",
    "  def __init__(self):\n",
    "    super(SSCProcessor, self).__init__()\n",
    "    self.label_column = 0\n",
    "    self.text_a_column = 3\n",
    "    self.text_b_column = 4\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# in: def main(_)\n",
    "# variable: processors = {}\n",
    "# at line: 660\n",
    "\n",
    "      'ssc-within': SSCProcessor,\n",
    "      'ssc-cross': SSCProcessor,\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_command = \"python3 \" + SCRIPTS_DIR + \"/run_classifier.py \\\n",
    "  --do_train=True \\\n",
    "  --do_eval=True \\\n",
    "  --eval_all_ckpt=True \\\n",
    "  --task_name=\" + TASK_NAME + \" \\\n",
    "  --data_dir=\" + DATA_DIR + \" \\\n",
    "  --output_dir=\" + OUTPUT_DIR + \" \\\n",
    "  --model_dir=\" + CHECKPOINT_DIR + \" \\\n",
    "  --uncased=False \\\n",
    "  --spiece_model_file=\" + PRETRAINED_MODEL_DIR + \"/spiece.model \\\n",
    "  --model_config_path=\" + PRETRAINED_MODEL_DIR + \"/xlnet_config.json \\\n",
    "  --init_checkpoint=\" + PRETRAINED_MODEL_DIR + \"/xlnet_model.ckpt \\\n",
    "  --max_seq_length=\" + MAX_SEQ_LEN + \" \\\n",
    "  --train_batch_size=\" + BATCH_SIZE + \" \\\n",
    "  --eval_batch_size=\" + BATCH_SIZE + \" \\\n",
    "  --num_hosts=1 \\\n",
    "  --num_core_per_host=1 \\\n",
    "  --learning_rate=2e-5 \\\n",
    "  --train_steps=40000 \\\n",
    "  --warmup_steps=500 \\\n",
    "  --save_steps=500 \\\n",
    "  --iterations=1000\"\n",
    "\n",
    "! {train_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help_command = \"python3 \" + SCRIPTS_DIR + \"/run_classifier.py --help\"\n",
    "\n",
    "! {help_command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-03T10:35:16.401842Z",
     "start_time": "2019-07-03T10:35:16.396365Z"
    }
   },
   "source": [
    "```python\n",
    "# %load xlnet/patch.diff\n",
    "```\n",
    "\n",
    "Patch in https://github.com/zihangdai/xlnet.git at index: a4ea77132e2954a0b3e6d8db5f97cd198b056c3a (origin/master)  \n",
    "Also update some warnings in tensorflow.\n",
    "\n",
    "```diff\n",
    "diff --git a/function_builder.py b/function_builder.py\n",
    "index 54cf894..15cf917 100644\n",
    "--- a/function_builder.py\n",
    "+++ b/function_builder.py\n",
    "@@ -95,7 +95,7 @@ def two_stream_loss(FLAGS, features, labels, mems, is_training):\n",
    " \n",
    "   initializer = xlnet_model.get_initializer()\n",
    " \n",
    "-  with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE):\n",
    "+  with tf.variable_scope(\"model\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "     # LM loss\n",
    "     lm_loss = modeling.lm_loss(\n",
    "         hidden=output,\n",
    "@@ -153,7 +153,7 @@ def get_classification_loss(\n",
    " \n",
    "   summary = xlnet_model.get_pooled_out(FLAGS.summary_type, FLAGS.use_summ_proj)\n",
    " \n",
    "-  with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE):\n",
    "+  with tf.variable_scope(\"model\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    " \n",
    "     if FLAGS.cls_scope is not None and FLAGS.cls_scope:\n",
    "       cls_scope = \"classification_{}\".format(FLAGS.cls_scope)\n",
    "@@ -196,7 +196,7 @@ def get_regression_loss(\n",
    " \n",
    "   summary = xlnet_model.get_pooled_out(FLAGS.summary_type, FLAGS.use_summ_proj)\n",
    " \n",
    "-  with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE):\n",
    "+  with tf.variable_scope(\"model\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "     per_example_loss, logits = modeling.regression_loss(\n",
    "         hidden=summary,\n",
    "         labels=label,\n",
    "diff --git a/model_utils.py b/model_utils.py\n",
    "index c8e4295..a6a4d40 100644\n",
    "--- a/model_utils.py\n",
    "+++ b/model_utils.py\n",
    "@@ -24,20 +24,20 @@ def configure_tpu(FLAGS):\n",
    "     tpu_cluster = None\n",
    "     master = FLAGS.master\n",
    " \n",
    "-  session_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "+  session_config = tf.compat.v1.ConfigProto(allow_soft_placement=True)\n",
    "   # Uncomment the following line if you hope to monitor GPU RAM growth\n",
    "   # session_config.gpu_options.allow_growth = True\n",
    " \n",
    "   if FLAGS.use_tpu:\n",
    "     strategy = None\n",
    "-    tf.logging.info('Use TPU without distribute strategy.')\n",
    "+    tf.compat.v1.logging.info('Use TPU without distribute strategy.')\n",
    "   elif FLAGS.num_core_per_host == 1:\n",
    "     strategy = None\n",
    "-    tf.logging.info('Single device mode.')\n",
    "+    tf.compat.v1.logging.info('Single device mode.')\n",
    "   else:\n",
    "     strategy = tf.contrib.distribute.MirroredStrategy(\n",
    "         num_gpus=FLAGS.num_core_per_host)\n",
    "-    tf.logging.info('Use MirroredStrategy with %d devices.',\n",
    "+    tf.compat.v1.logging.info('Use MirroredStrategy with %d devices.',\n",
    "                     strategy.num_replicas_in_sync)\n",
    " \n",
    "   per_host_input = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "@@ -64,36 +64,36 @@ def init_from_checkpoint(FLAGS, global_vars=False):\n",
    "   if FLAGS.init_checkpoint is not None:\n",
    "     if FLAGS.init_checkpoint.endswith(\"latest\"):\n",
    "       ckpt_dir = os.path.dirname(FLAGS.init_checkpoint)\n",
    "-      init_checkpoint = tf.train.latest_checkpoint(ckpt_dir)\n",
    "+      init_checkpoint = tf.compat.v1.train.latest_checkpoint(ckpt_dir)\n",
    "     else:\n",
    "       init_checkpoint = FLAGS.init_checkpoint\n",
    " \n",
    "-    tf.logging.info(\"Initialize from the ckpt {}\".format(init_checkpoint))\n",
    "+    tf.compat.v1.logging.info(\"Initialize from the ckpt {}\".format(init_checkpoint))\n",
    " \n",
    "     (assignment_map, initialized_variable_names\n",
    "     ) = get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "     if FLAGS.use_tpu:\n",
    "       def tpu_scaffold():\n",
    "-        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "-        return tf.train.Scaffold()\n",
    "+        tf.compat.v1.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "+        return tf.compat.v1.train.Scaffold()\n",
    " \n",
    "       scaffold_fn = tpu_scaffold\n",
    "     else:\n",
    "-      tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "+      tf.compat.v1.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    " \n",
    "     # Log customized initialization\n",
    "-    tf.logging.info(\"**** Global Variables ****\")\n",
    "+    tf.compat.v1.logging.info(\"**** Global Variables ****\")\n",
    "     for var in tvars:\n",
    "       init_string = \"\"\n",
    "       if var.name in initialized_variable_names:\n",
    "         init_string = \", *INIT_FROM_CKPT*\"\n",
    "-      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "+      tf.compat.v1.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                       init_string)\n",
    "   return scaffold_fn\n",
    " \n",
    " \n",
    " def get_train_op(FLAGS, total_loss, grads_and_vars=None):\n",
    "-  global_step = tf.train.get_or_create_global_step()\n",
    "+  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    " \n",
    "   # increase the learning rate linearly\n",
    "   if FLAGS.warmup_steps > 0:\n",
    "@@ -105,13 +105,13 @@ def get_train_op(FLAGS, total_loss, grads_and_vars=None):\n",
    " \n",
    "   # decay the learning rate\n",
    "   if FLAGS.decay_method == \"poly\":\n",
    "-    decay_lr = tf.train.polynomial_decay(\n",
    "+    decay_lr = tf.compat.v1.train.polynomial_decay(\n",
    "         FLAGS.learning_rate,\n",
    "         global_step=global_step - FLAGS.warmup_steps,\n",
    "         decay_steps=FLAGS.train_steps - FLAGS.warmup_steps,\n",
    "         end_learning_rate=FLAGS.learning_rate * FLAGS.min_lr_ratio)\n",
    "   elif FLAGS.decay_method == \"cos\":\n",
    "-    decay_lr = tf.train.cosine_decay(\n",
    "+    decay_lr = tf.compat.v1.train.cosine_decay(\n",
    "         FLAGS.learning_rate,\n",
    "         global_step=global_step - FLAGS.warmup_steps,\n",
    "         decay_steps=FLAGS.train_steps - FLAGS.warmup_steps,\n",
    "@@ -128,7 +128,7 @@ def get_train_op(FLAGS, total_loss, grads_and_vars=None):\n",
    "                      \"training so far.\")\n",
    " \n",
    "   if FLAGS.weight_decay == 0:\n",
    "-    optimizer = tf.train.AdamOptimizer(\n",
    "+    optimizer = tf.compat.v1.train.AdamOptimizer(\n",
    "         learning_rate=learning_rate,\n",
    "         epsilon=FLAGS.adam_epsilon)\n",
    "   else:\n",
    "@@ -158,7 +158,7 @@ def get_train_op(FLAGS, total_loss, grads_and_vars=None):\n",
    "         if \"model/transformer/layer_{}/\".format(l) in variables[i].name:\n",
    "           abs_rate = FLAGS.lr_layer_decay_rate ** (n_layer - 1 - l)\n",
    "           clipped[i] *= abs_rate\n",
    "-          tf.logging.info(\"Apply mult {:.4f} to layer-{} grad of {}\".format(\n",
    "+          tf.compat.v1.logging.info(\"Apply mult {:.4f} to layer-{} grad of {}\".format(\n",
    "               abs_rate, l, variables[i].name))\n",
    "           break\n",
    " \n",
    "@@ -184,11 +184,11 @@ def clean_ckpt(_):\n",
    "   for (name, shape) in var_list:\n",
    "     if not name.startswith(\"global_step\") and \"adam\" not in name.lower():\n",
    "       var_values[name] = None\n",
    "-      tf.logging.info(\"Include {}\".format(name))\n",
    "+      tf.compat.v1.logging.info(\"Include {}\".format(name))\n",
    "     else:\n",
    "-      tf.logging.info(\"Exclude {}\".format(name))\n",
    "+      tf.compat.v1.logging.info(\"Exclude {}\".format(name))\n",
    " \n",
    "-  tf.logging.info(\"Loading from {}\".format(input_ckpt))\n",
    "+  tf.compat.v1.logging.info(\"Loading from {}\".format(input_ckpt))\n",
    "   reader = tf.contrib.framework.load_checkpoint(input_ckpt)\n",
    "   for name in var_values:\n",
    "     tensor = reader.get_tensor(name)\n",
    "@@ -204,7 +204,7 @@ def clean_ckpt(_):\n",
    "   assign_ops = [tf.assign(v, p) for (v, p) in zip(tf_vars, placeholders)]\n",
    "   global_step = tf.Variable(\n",
    "       0, name=\"global_step\", trainable=False, dtype=tf.int64)\n",
    "-  saver = tf.train.Saver(tf.all_variables())\n",
    "+  saver = tf.compat.v1.train.Saver(tf.all_variables())\n",
    " \n",
    "   if not tf.gfile.Exists(output_model_dir):\n",
    "     tf.gfile.MakeDirs(output_model_dir)\n",
    "@@ -224,7 +224,7 @@ def clean_ckpt(_):\n",
    " def avg_checkpoints(model_dir, output_model_dir, last_k):\n",
    "   tf.reset_default_graph()\n",
    " \n",
    "-  checkpoint_state = tf.train.get_checkpoint_state(model_dir)\n",
    "+  checkpoint_state = tf.compat.v1.train.get_checkpoint_state(model_dir)\n",
    "   checkpoints = checkpoint_state.all_model_checkpoint_paths[- last_k:]\n",
    "   var_list = tf.contrib.framework.list_variables(checkpoints[0])\n",
    "   var_values, var_dtypes = {}, {}\n",
    "@@ -237,7 +237,7 @@ def avg_checkpoints(model_dir, output_model_dir, last_k):\n",
    "       tensor = reader.get_tensor(name)\n",
    "       var_dtypes[name] = tensor.dtype\n",
    "       var_values[name] += tensor\n",
    "-    tf.logging.info(\"Read from checkpoint %s\", checkpoint)\n",
    "+    tf.compat.v1.logging.info(\"Read from checkpoint %s\", checkpoint)\n",
    "   for name in var_values:  # Average.\n",
    "     var_values[name] /= len(checkpoints)\n",
    " \n",
    "@@ -250,7 +250,7 @@ def avg_checkpoints(model_dir, output_model_dir, last_k):\n",
    "   assign_ops = [tf.assign(v, p) for (v, p) in zip(tf_vars, placeholders)]\n",
    "   global_step = tf.Variable(\n",
    "       0, name=\"global_step\", trainable=False, dtype=tf.int64)\n",
    "-  saver = tf.train.Saver(tf.all_variables())\n",
    "+  saver = tf.compat.v1.train.Saver(tf.all_variables())\n",
    " \n",
    "   # Build a model consisting only of variables, set them to the average values.\n",
    "   with tf.Session() as sess:\n",
    "@@ -276,12 +276,12 @@ def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
    "       name = m.group(1)\n",
    "     name_to_variable[name] = var\n",
    " \n",
    "-  init_vars = tf.train.list_variables(init_checkpoint)\n",
    "+  init_vars = tf.compat.v1.train.list_variables(init_checkpoint)\n",
    " \n",
    "   assignment_map = collections.OrderedDict()\n",
    "   for x in init_vars:\n",
    "     (name, var) = (x[0], x[1])\n",
    "-    # tf.logging.info('original name: %s', name)\n",
    "+    # tf.compat.v1.logging.info('original name: %s', name)\n",
    "     if name not in name_to_variable:\n",
    "       continue\n",
    "     # assignment_map[name] = name\n",
    "@@ -292,7 +292,7 @@ def get_assignment_map_from_checkpoint(tvars, init_checkpoint):\n",
    "   return (assignment_map, initialized_variable_names)\n",
    " \n",
    " \n",
    "-class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
    "+class AdamWeightDecayOptimizer(tf.compat.v1.train.Optimizer):\n",
    "   \"\"\"A basic Adam optimizer that includes \"correct\" L2 weight decay.\"\"\"\n",
    " \n",
    "   def __init__(self,\n",
    "@@ -378,7 +378,7 @@ class AdamWeightDecayOptimizer(tf.train.Optimizer):\n",
    "     if self.exclude_from_weight_decay:\n",
    "       for r in self.exclude_from_weight_decay:\n",
    "         if re.search(r, param_name) is not None:\n",
    "-          tf.logging.info('Adam WD excludes {}'.format(param_name))\n",
    "+          tf.compat.v1.logging.info('Adam WD excludes {}'.format(param_name))\n",
    "           return False\n",
    "     return True\n",
    " \n",
    "diff --git a/modeling.py b/modeling.py\n",
    "index a7d719c..d9b24b8 100644\n",
    "--- a/modeling.py\n",
    "+++ b/modeling.py\n",
    "@@ -25,8 +25,8 @@ def gelu(x):\n",
    " def embedding_lookup(x, n_token, d_embed, initializer, use_tpu=True,\n",
    "                      scope='embedding', reuse=None, dtype=tf.float32):\n",
    "   \"\"\"TPU and GPU embedding_lookup function.\"\"\"\n",
    "-  with tf.variable_scope(scope, reuse=reuse):\n",
    "-    lookup_table = tf.get_variable('lookup_table', [n_token, d_embed],\n",
    "+  with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "+    lookup_table = tf.compat.v1.get_variable('lookup_table', [n_token, d_embed],\n",
    "                                    dtype=dtype, initializer=initializer)\n",
    "     if use_tpu:\n",
    "       one_hot_idx = tf.one_hot(x, n_token, dtype=dtype)\n",
    "@@ -61,7 +61,7 @@ def positionwise_ffn(inp, d_model, d_inner, dropout, kernel_initializer,\n",
    "     raise ValueError('Unsupported activation type {}'.format(activation_type))\n",
    " \n",
    "   output = inp\n",
    "-  with tf.variable_scope(scope, reuse=reuse):\n",
    "+  with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "     output = tf.layers.dense(output, d_inner, activation=activation,\n",
    "                              kernel_initializer=kernel_initializer,\n",
    "                              name='layer_1')\n",
    "@@ -79,7 +79,7 @@ def positionwise_ffn(inp, d_model, d_inner, dropout, kernel_initializer,\n",
    " \n",
    " def head_projection(h, d_model, n_head, d_head, kernel_initializer, name):\n",
    "   \"\"\"Project hidden states to a specific head with a 4D-shape.\"\"\"\n",
    "-  proj_weight = tf.get_variable('{}/kernel'.format(name),\n",
    "+  proj_weight = tf.compat.v1.get_variable('{}/kernel'.format(name),\n",
    "                                 [d_model, n_head, d_head], dtype=h.dtype,\n",
    "                                 initializer=kernel_initializer)\n",
    "   head = tf.einsum('ibh,hnd->ibnd', h, proj_weight)\n",
    "@@ -91,7 +91,7 @@ def post_attention(h, attn_vec, d_model, n_head, d_head, dropout, is_training,\n",
    "                    kernel_initializer, residual=True):\n",
    "   \"\"\"Post-attention processing.\"\"\"\n",
    "   # post-attention projection (back to `d_model`)\n",
    "-  proj_o = tf.get_variable('o/kernel', [d_model, n_head, d_head],\n",
    "+  proj_o = tf.compat.v1.get_variable('o/kernel', [d_model, n_head, d_head],\n",
    "                            dtype=h.dtype, initializer=kernel_initializer)\n",
    "   attn_out = tf.einsum('ibnd,hnd->ibh', attn_vec, proj_o)\n",
    " \n",
    "@@ -258,7 +258,7 @@ def multihead_attn(q, k, v, attn_mask, d_model, n_head, d_head, dropout,\n",
    "   \"\"\"Standard multi-head attention with absolute positional embedding.\"\"\"\n",
    " \n",
    "   scale = 1 / (d_head ** 0.5)\n",
    "-  with tf.variable_scope(scope, reuse=reuse):\n",
    "+  with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "     # attention heads\n",
    "     q_head = head_projection(\n",
    "         q, d_model, n_head, d_head, kernel_initializer, 'q')\n",
    "@@ -286,7 +286,7 @@ def rel_multihead_attn(h, r, r_w_bias, r_r_bias, seg_mat, r_s_bias, seg_embed,\n",
    "   \"\"\"Multi-head attention with relative positional encoding.\"\"\"\n",
    " \n",
    "   scale = 1 / (d_head ** 0.5)\n",
    "-  with tf.variable_scope(scope, reuse=reuse):\n",
    "+  with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "     if mems is not None and mems.shape.ndims > 1:\n",
    "       cat = tf.concat([mems, h], 0)\n",
    "     else:\n",
    "@@ -323,7 +323,7 @@ def two_stream_rel_attn(h, g, r, mems, r_w_bias, r_r_bias, seg_mat, r_s_bias,\n",
    "   \"\"\"Two-stream attention with relative positional encoding.\"\"\"\n",
    " \n",
    "   scale = 1 / (d_head ** 0.5)\n",
    "-  with tf.variable_scope(scope, reuse=False):\n",
    "+  with tf.compat.v1.variable_scope(scope, reuse=False):\n",
    " \n",
    "     # content based attention score\n",
    "     if mems is not None and mems.shape.ndims > 1:\n",
    "@@ -357,7 +357,7 @@ def two_stream_rel_attn(h, g, r, mems, r_w_bias, r_r_bias, seg_mat, r_s_bias,\n",
    "     output_h = post_attention(h, attn_vec_h, d_model, n_head, d_head, dropout,\n",
    "                               is_training, kernel_initializer)\n",
    " \n",
    "-  with tf.variable_scope(scope, reuse=True):\n",
    "+  with tf.compat.v1.variable_scope(scope, reuse=True):\n",
    "     ##### g-stream\n",
    "     # query-stream query head\n",
    "     q_head_g = head_projection(\n",
    "@@ -450,21 +450,21 @@ def transformer_xl(inp_k, n_token, n_layer, d_model, n_head,\n",
    "     initializer: A tf initializer.\n",
    "     scope: scope name for the computation graph.\n",
    "   \"\"\"\n",
    "-  tf.logging.info('memory input {}'.format(mems))\n",
    "+  tf.compat.v1.logging.info('memory input {}'.format(mems))\n",
    "   tf_float = tf.bfloat16 if use_bfloat16 else tf.float32\n",
    "-  tf.logging.info('Use float type {}'.format(tf_float))\n",
    "+  tf.compat.v1.logging.info('Use float type {}'.format(tf_float))\n",
    " \n",
    "   new_mems = []\n",
    "-  with tf.variable_scope(scope):\n",
    "+  with tf.compat.v1.variable_scope(scope):\n",
    "     if untie_r:\n",
    "-      r_w_bias = tf.get_variable('r_w_bias', [n_layer, n_head, d_head],\n",
    "+      r_w_bias = tf.compat.v1.get_variable('r_w_bias', [n_layer, n_head, d_head],\n",
    "                                  dtype=tf_float, initializer=initializer)\n",
    "-      r_r_bias = tf.get_variable('r_r_bias', [n_layer, n_head, d_head],\n",
    "+      r_r_bias = tf.compat.v1.get_variable('r_r_bias', [n_layer, n_head, d_head],\n",
    "                                  dtype=tf_float, initializer=initializer)\n",
    "     else:\n",
    "-      r_w_bias = tf.get_variable('r_w_bias', [n_head, d_head],\n",
    "+      r_w_bias = tf.compat.v1.get_variable('r_w_bias', [n_head, d_head],\n",
    "                                  dtype=tf_float, initializer=initializer)\n",
    "-      r_r_bias = tf.get_variable('r_r_bias', [n_head, d_head],\n",
    "+      r_r_bias = tf.compat.v1.get_variable('r_r_bias', [n_head, d_head],\n",
    "                                  dtype=tf_float, initializer=initializer)\n",
    " \n",
    "     bsz = tf.shape(inp_k)[1]\n",
    "@@ -525,8 +525,8 @@ def transformer_xl(inp_k, n_token, n_layer, d_model, n_head,\n",
    "         scope='word_embedding')\n",
    " \n",
    "     if inp_q is not None:\n",
    "-      with tf.variable_scope('mask_emb'):\n",
    "-        mask_emb = tf.get_variable('mask_emb', [1, 1, d_model], dtype=tf_float)\n",
    "+      with tf.compat.v1.variable_scope('mask_emb'):\n",
    "+        mask_emb = tf.compat.v1.get_variable('mask_emb', [1, 1, d_model], dtype=tf_float)\n",
    "         if target_mapping is not None:\n",
    "           word_emb_q = tf.tile(mask_emb, [tf.shape(target_mapping)[0], bsz, 1])\n",
    "         else:\n",
    "@@ -539,14 +539,14 @@ def transformer_xl(inp_k, n_token, n_layer, d_model, n_head,\n",
    "     ##### Segment embedding\n",
    "     if seg_id is not None:\n",
    "       if untie_r:\n",
    "-        r_s_bias = tf.get_variable('r_s_bias', [n_layer, n_head, d_head],\n",
    "+        r_s_bias = tf.compat.v1.get_variable('r_s_bias', [n_layer, n_head, d_head],\n",
    "                                    dtype=tf_float, initializer=initializer)\n",
    "       else:\n",
    "         # default case (tie)\n",
    "-        r_s_bias = tf.get_variable('r_s_bias', [n_head, d_head],\n",
    "+        r_s_bias = tf.compat.v1.get_variable('r_s_bias', [n_head, d_head],\n",
    "                                    dtype=tf_float, initializer=initializer)\n",
    " \n",
    "-      seg_embed = tf.get_variable('seg_embed', [n_layer, 2, n_head, d_head],\n",
    "+      seg_embed = tf.compat.v1.get_variable('seg_embed', [n_layer, 2, n_head, d_head],\n",
    "                                   dtype=tf_float, initializer=initializer)\n",
    " \n",
    "       # Convert `seg_id` to one-hot `seg_mat`\n",
    "@@ -583,7 +583,7 @@ def transformer_xl(inp_k, n_token, n_layer, d_model, n_head,\n",
    "         r_s_bias_i = r_s_bias if not untie_r else r_s_bias[i]\n",
    "         seg_embed_i = seg_embed[i]\n",
    " \n",
    "-      with tf.variable_scope('layer_{}'.format(i)):\n",
    "+      with tf.compat.v1.variable_scope('layer_{}'.format(i)):\n",
    "         if inp_q is not None:\n",
    "           output_h, output_g = two_stream_rel_attn(\n",
    "               h=output_h,\n",
    "@@ -660,16 +660,16 @@ def lm_loss(hidden, target, n_token, d_model, initializer, lookup_table=None,\n",
    "             tie_weight=False, bi_data=True, use_tpu=False):\n",
    "   \"\"\"doc.\"\"\"\n",
    " \n",
    "-  with tf.variable_scope('lm_loss'):\n",
    "+  with tf.compat.v1.variable_scope('lm_loss'):\n",
    "     if tie_weight:\n",
    "       assert lookup_table is not None, \\\n",
    "           'lookup_table cannot be None for tie_weight'\n",
    "       softmax_w = lookup_table\n",
    "     else:\n",
    "-      softmax_w = tf.get_variable('weight', [n_token, d_model],\n",
    "+      softmax_w = tf.compat.v1.get_variable('weight', [n_token, d_model],\n",
    "                                   dtype=hidden.dtype, initializer=initializer)\n",
    " \n",
    "-    softmax_b = tf.get_variable('bias', [n_token], dtype=hidden.dtype,\n",
    "+    softmax_b = tf.compat.v1.get_variable('bias', [n_token], dtype=hidden.dtype,\n",
    "                                 initializer=tf.zeros_initializer())\n",
    " \n",
    "     logits = tf.einsum('ibd,nd->ibn', hidden, softmax_w) + softmax_b\n",
    "@@ -696,7 +696,7 @@ def summarize_sequence(summary_type, hidden, d_model, n_head, d_head, dropout,\n",
    "       Otherwise, one should specify a different `scope` for each task.\n",
    "   \"\"\"\n",
    " \n",
    "-  with tf.variable_scope(scope, 'sequnece_summary', reuse=reuse):\n",
    "+  with tf.compat.v1.variable_scope(scope, 'sequnece_summary', reuse=reuse):\n",
    "     if summary_type == 'last':\n",
    "       summary = hidden[-1]\n",
    "     elif summary_type == 'first':\n",
    "@@ -706,7 +706,7 @@ def summarize_sequence(summary_type, hidden, d_model, n_head, d_head, dropout,\n",
    "     elif summary_type == 'attn':\n",
    "       bsz = tf.shape(hidden)[1]\n",
    " \n",
    "-      summary_bias = tf.get_variable('summary_bias', [d_model],\n",
    "+      summary_bias = tf.compat.v1.get_variable('summary_bias', [d_model],\n",
    "                                      dtype=hidden.dtype,\n",
    "                                      initializer=initializer)\n",
    "       summary_bias = tf.tile(summary_bias[None, None], [1, bsz, 1])\n",
    "@@ -748,7 +748,7 @@ def classification_loss(hidden, labels, n_class, initializer, scope, reuse=None,\n",
    "       the classification weights.\n",
    "   \"\"\"\n",
    " \n",
    "-  with tf.variable_scope(scope, reuse=reuse):\n",
    "+  with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "     logits = tf.layers.dense(\n",
    "         hidden,\n",
    "         n_class,\n",
    "@@ -766,7 +766,7 @@ def classification_loss(hidden, labels, n_class, initializer, scope, reuse=None,\n",
    " \n",
    " def regression_loss(hidden, labels, initializer, scope, reuse=None,\n",
    "                     return_logits=False):\n",
    "-  with tf.variable_scope(scope, reuse=reuse):\n",
    "+  with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "     logits = tf.layers.dense(\n",
    "         hidden,\n",
    "         1,\n",
    "diff --git a/run_classifier.py b/run_classifier.py\n",
    "index c6eb1ba..ed0a700 100644\n",
    "--- a/run_classifier.py\n",
    "+++ b/run_classifier.py\n",
    "@@ -183,7 +183,7 @@ class DataProcessor(object):\n",
    "   @classmethod\n",
    "   def _read_tsv(cls, input_file, quotechar=None):\n",
    "     \"\"\"Reads a tab separated value file.\"\"\"\n",
    "-    with tf.gfile.Open(input_file, \"r\") as f:\n",
    "+    with tf.io.gfile.GFile(input_file, \"r\") as f:\n",
    "       reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "       lines = []\n",
    "       for line in reader:\n",
    "@@ -246,13 +246,13 @@ class GLUEProcessor(DataProcessor):\n",
    " \n",
    "       # there are some incomplete lines in QNLI\n",
    "       if len(line) <= a_column:\n",
    "-        tf.logging.warning('Incomplete line, ignored.')\n",
    "+        tf.compat.v1.logging.warning('Incomplete line, ignored.')\n",
    "         continue\n",
    "       text_a = line[a_column]\n",
    " \n",
    "       if b_column is not None:\n",
    "         if len(line) <= b_column:\n",
    "-          tf.logging.warning('Incomplete line, ignored.')\n",
    "+          tf.compat.v1.logging.warning('Incomplete line, ignored.')\n",
    "           continue\n",
    "         text_b = line[b_column]\n",
    "       else:\n",
    "@@ -262,7 +262,7 @@ class GLUEProcessor(DataProcessor):\n",
    "         label = self.get_labels()[0]\n",
    "       else:\n",
    "         if len(line) <= self.label_column:\n",
    "-          tf.logging.warning('Incomplete line, ignored.')\n",
    "+          tf.compat.v1.logging.warning('Incomplete line, ignored.')\n",
    "           continue\n",
    "         label = line[self.label_column]\n",
    "       examples.append(\n",
    "@@ -309,7 +309,7 @@ class ImdbProcessor(DataProcessor):\n",
    "     examples = []\n",
    "     for label in [\"neg\", \"pos\"]:\n",
    "       cur_dir = os.path.join(data_dir, label)\n",
    "-      for filename in tf.gfile.ListDirectory(cur_dir):\n",
    "+      for filename in tf.io.gfile.listdir(cur_dir):\n",
    "         if not filename.endswith(\"txt\"): continue\n",
    " \n",
    "         path = os.path.join(cur_dir, filename)\n",
    "@@ -340,6 +340,14 @@ class MnliMismatchedProcessor(MnliMatchedProcessor):\n",
    "     self.test_file = \"test_mismatched.tsv\"\n",
    " \n",
    " \n",
    "+class SSCProcessor(GLUEProcessor):\n",
    "+  def __init__(self):\n",
    "+    super(SSCProcessor, self).__init__()\n",
    "+    self.label_column = 0\n",
    "+    self.text_a_column = 3\n",
    "+    self.text_b_column = 4\n",
    "+\n",
    "+\n",
    " class StsbProcessor(GLUEProcessor):\n",
    "   def __init__(self):\n",
    "     super(StsbProcessor, self).__init__()\n",
    "@@ -367,13 +375,13 @@ class StsbProcessor(GLUEProcessor):\n",
    " \n",
    "       # there are some incomplete lines in QNLI\n",
    "       if len(line) <= a_column:\n",
    "-        tf.logging.warning('Incomplete line, ignored.')\n",
    "+        tf.compat.v1.logging.warning('Incomplete line, ignored.')\n",
    "         continue\n",
    "       text_a = line[a_column]\n",
    " \n",
    "       if b_column is not None:\n",
    "         if len(line) <= b_column:\n",
    "-          tf.logging.warning('Incomplete line, ignored.')\n",
    "+          tf.compat.v1.logging.warning('Incomplete line, ignored.')\n",
    "           continue\n",
    "         text_b = line[b_column]\n",
    "       else:\n",
    "@@ -383,7 +391,7 @@ class StsbProcessor(GLUEProcessor):\n",
    "         label = self.get_labels()[0]\n",
    "       else:\n",
    "         if len(line) <= self.label_column:\n",
    "-          tf.logging.warning('Incomplete line, ignored.')\n",
    "+          tf.compat.v1.logging.warning('Incomplete line, ignored.')\n",
    "           continue\n",
    "         label = float(line[self.label_column])\n",
    "       examples.append(\n",
    "@@ -398,20 +406,20 @@ def file_based_convert_examples_to_features(\n",
    "   \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
    " \n",
    "   # do not create duplicated records\n",
    "-  if tf.gfile.Exists(output_file) and not FLAGS.overwrite_data:\n",
    "-    tf.logging.info(\"Do not overwrite tfrecord {} exists.\".format(output_file))\n",
    "+  if tf.io.gfile.exists(output_file) and not FLAGS.overwrite_data:\n",
    "+    tf.compat.v1.logging.info(\"Do not overwrite tfrecord {} exists.\".format(output_file))\n",
    "     return\n",
    " \n",
    "-  tf.logging.info(\"Create new tfrecord {}.\".format(output_file))\n",
    "+  tf.compat.v1.logging.info(\"Create new tfrecord {}.\".format(output_file))\n",
    " \n",
    "-  writer = tf.python_io.TFRecordWriter(output_file)\n",
    "+  writer = tf.io.TFRecordWriter(output_file)\n",
    " \n",
    "   if num_passes > 1:\n",
    "     examples *= num_passes\n",
    " \n",
    "   for (ex_index, example) in enumerate(examples):\n",
    "     if ex_index % 10000 == 0:\n",
    "-      tf.logging.info(\"Writing example {} of {}\".format(ex_index,\n",
    "+      tf.compat.v1.logging.info(\"Writing example {} of {}\".format(ex_index,\n",
    "                                                         len(examples)))\n",
    " \n",
    "     feature = convert_single_example(ex_index, example, label_list,\n",
    "@@ -447,20 +455,20 @@ def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
    " \n",
    " \n",
    "   name_to_features = {\n",
    "-      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "-      \"input_mask\": tf.FixedLenFeature([seq_length], tf.float32),\n",
    "-      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "-      \"label_ids\": tf.FixedLenFeature([], tf.int64),\n",
    "-      \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
    "+      \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "+      \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.float32),\n",
    "+      \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "+      \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "+      \"is_real_example\": tf.io.FixedLenFeature([], tf.int64),\n",
    "   }\n",
    "   if FLAGS.is_regression:\n",
    "-    name_to_features[\"label_ids\"] = tf.FixedLenFeature([], tf.float32)\n",
    "+    name_to_features[\"label_ids\"] = tf.io.FixedLenFeature([], tf.float32)\n",
    " \n",
    "-  tf.logging.info(\"Input tfrecord file {}\".format(input_file))\n",
    "+  tf.compat.v1.logging.info(\"Input tfrecord file {}\".format(input_file))\n",
    " \n",
    "   def _decode_record(record, name_to_features):\n",
    "     \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "-    example = tf.parse_single_example(record, name_to_features)\n",
    "+    example = tf.io.parse_single_example(record, name_to_features)\n",
    " \n",
    "     # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "     # So cast all int64 to int32.\n",
    "@@ -486,7 +494,7 @@ def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
    "     d = tf.data.TFRecordDataset(input_file)\n",
    "     # Shard the dataset to difference devices\n",
    "     if input_context is not None:\n",
    "-      tf.logging.info(\"Input pipeline id %d out of %d\",\n",
    "+      tf.compat.v1.logging.info(\"Input pipeline id %d out of %d\",\n",
    "           input_context.input_pipeline_id, input_context.num_replicas_in_sync)\n",
    "       d = d.shard(input_context.num_input_pipelines,\n",
    "                   input_context.input_pipeline_id)\n",
    "@@ -523,8 +531,8 @@ def get_model_fn(n_class):\n",
    "           FLAGS, features, n_class, is_training)\n",
    " \n",
    "     #### Check model parameters\n",
    "-    num_params = sum([np.prod(v.shape) for v in tf.trainable_variables()])\n",
    "-    tf.logging.info('#params: {}'.format(num_params))\n",
    "+    num_params = sum([np.prod(v.shape) for v in tf.compat.v1.trainable_variables()])\n",
    "+    tf.compat.v1.logging.info('#params: {}'.format(num_params))\n",
    " \n",
    "     #### load pretrained models\n",
    "     scaffold_fn = model_utils.init_from_checkpoint(FLAGS)\n",
    "@@ -540,16 +548,16 @@ def get_model_fn(n_class):\n",
    "             'predictions': predictions,\n",
    "             'weights': is_real_example\n",
    "         }\n",
    "-        accuracy = tf.metrics.accuracy(**eval_input_dict)\n",
    "+        accuracy = tf.compat.v1.metrics.accuracy(**eval_input_dict)\n",
    " \n",
    "-        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
    "+        loss = tf.compat.v1.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
    "         return {\n",
    "             'eval_accuracy': accuracy,\n",
    "             'eval_loss': loss}\n",
    " \n",
    "       def regression_metric_fn(\n",
    "           per_example_loss, label_ids, logits, is_real_example):\n",
    "-        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
    "+        loss = tf.compat.v1.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
    "         pearsonr = tf.contrib.metrics.streaming_pearson_correlation(\n",
    "             logits, label_ids, weights=is_real_example)\n",
    "         return {'eval_loss': loss, 'eval_pearsonr': pearsonr}\n",
    "@@ -634,7 +642,7 @@ def get_model_fn(n_class):\n",
    " \n",
    " \n",
    " def main(_):\n",
    "-  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "+  tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    " \n",
    "   #### Validate flags\n",
    "   if FLAGS.save_steps is not None:\n",
    "@@ -642,13 +650,15 @@ def main(_):\n",
    " \n",
    "   if FLAGS.do_predict:\n",
    "     predict_dir = FLAGS.predict_dir\n",
    "-    if not tf.gfile.Exists(predict_dir):\n",
    "+    if not tf.io.gfile.exists(predict_dir):\n",
    "       tf.gfile.MakeDirs(predict_dir)\n",
    " \n",
    "   processors = {\n",
    "       \"mnli_matched\": MnliMatchedProcessor,\n",
    "       \"mnli_mismatched\": MnliMismatchedProcessor,\n",
    "       'sts-b': StsbProcessor,\n",
    "+      'ssc-within': SSCProcessor,\n",
    "+      'ssc-cross': SSCProcessor,\n",
    "       'imdb': ImdbProcessor,\n",
    "       \"yelp5\": Yelp5Processor\n",
    "   }\n",
    "@@ -658,7 +668,7 @@ def main(_):\n",
    "         \"At least one of `do_train`, `do_eval, `do_predict` or \"\n",
    "         \"`do_submit` must be True.\")\n",
    " \n",
    "-  if not tf.gfile.Exists(FLAGS.output_dir):\n",
    "+  if not tf.io.gfile.exists(FLAGS.output_dir):\n",
    "     tf.gfile.MakeDirs(FLAGS.output_dir)\n",
    " \n",
    "   task_name = FLAGS.task_name.lower()\n",
    "@@ -700,11 +710,11 @@ def main(_):\n",
    "     train_file_base = \"{}.len-{}.train.tf_record\".format(\n",
    "         spm_basename, FLAGS.max_seq_length)\n",
    "     train_file = os.path.join(FLAGS.output_dir, train_file_base)\n",
    "-    tf.logging.info(\"Use tfrecord file {}\".format(train_file))\n",
    "+    tf.compat.v1.logging.info(\"Use tfrecord file {}\".format(train_file))\n",
    " \n",
    "     train_examples = processor.get_train_examples(FLAGS.data_dir)\n",
    "     np.random.shuffle(train_examples)\n",
    "-    tf.logging.info(\"Num of train samples: {}\".format(len(train_examples)))\n",
    "+    tf.compat.v1.logging.info(\"Num of train samples: {}\".format(len(train_examples)))\n",
    " \n",
    "     file_based_convert_examples_to_features(\n",
    "         train_examples, label_list, FLAGS.max_seq_length, tokenize_fn,\n",
    "@@ -724,7 +734,7 @@ def main(_):\n",
    "     else:\n",
    "       eval_examples = processor.get_test_examples(FLAGS.data_dir)\n",
    " \n",
    "-    tf.logging.info(\"Num of eval samples: {}\".format(len(eval_examples)))\n",
    "+    tf.compat.v1.logging.info(\"Num of eval samples: {}\".format(len(eval_examples)))\n",
    " \n",
    "   if FLAGS.do_eval:\n",
    "     # TPU requires a fixed batch size for all batches, therefore the number\n",
    "@@ -756,14 +766,14 @@ def main(_):\n",
    " \n",
    "     # Filter out all checkpoints in the directory\n",
    "     steps_and_files = []\n",
    "-    filenames = tf.gfile.ListDirectory(FLAGS.model_dir)\n",
    "+    filenames = tf.io.gfile.listdir(FLAGS.model_dir)\n",
    " \n",
    "     for filename in filenames:\n",
    "       if filename.endswith(\".index\"):\n",
    "         ckpt_name = filename[:-6]\n",
    "         cur_filename = join(FLAGS.model_dir, ckpt_name)\n",
    "         global_step = int(cur_filename.split(\"-\")[-1])\n",
    "-        tf.logging.info(\"Add {} to eval list.\".format(cur_filename))\n",
    "+        tf.compat.v1.logging.info(\"Add {} to eval list.\".format(cur_filename))\n",
    "         steps_and_files.append([global_step, cur_filename])\n",
    "     steps_and_files = sorted(steps_and_files, key=lambda x: x[0])\n",
    " \n",
    "@@ -783,20 +793,20 @@ def main(_):\n",
    " \n",
    "       eval_results.append(ret)\n",
    " \n",
    "-      tf.logging.info(\"=\" * 80)\n",
    "+      tf.compat.v1.logging.info(\"=\" * 80)\n",
    "       log_str = \"Eval result | \"\n",
    "       for key, val in sorted(ret.items(), key=lambda x: x[0]):\n",
    "         log_str += \"{} {} | \".format(key, val)\n",
    "-      tf.logging.info(log_str)\n",
    "+      tf.compat.v1.logging.info(log_str)\n",
    " \n",
    "     key_name = \"eval_pearsonr\" if FLAGS.is_regression else \"eval_accuracy\"\n",
    "     eval_results.sort(key=lambda x: x[key_name], reverse=True)\n",
    " \n",
    "-    tf.logging.info(\"=\" * 80)\n",
    "+    tf.compat.v1.logging.info(\"=\" * 80)\n",
    "     log_str = \"Best result | \"\n",
    "     for key, val in sorted(eval_results[0].items(), key=lambda x: x[0]):\n",
    "       log_str += \"{} {} | \".format(key, val)\n",
    "-    tf.logging.info(log_str)\n",
    "+    tf.compat.v1.logging.info(log_str)\n",
    " \n",
    "   if FLAGS.do_predict:\n",
    "     eval_file_base = \"{}.len-{}.{}.predict.tf_record\".format(\n",
    "@@ -823,7 +833,7 @@ def main(_):\n",
    "           yield_single_examples=True,\n",
    "           checkpoint_path=FLAGS.predict_ckpt)):\n",
    "         if pred_cnt % 1000 == 0:\n",
    "-          tf.logging.info(\"Predicting submission for example: {}\".format(\n",
    "+          tf.compat.v1.logging.info(\"Predicting submission for example: {}\".format(\n",
    "               pred_cnt))\n",
    " \n",
    "         logits = [float(x) for x in result[\"logits\"].flat]\n",
    "@@ -852,4 +862,4 @@ def main(_):\n",
    " \n",
    " \n",
    " if __name__ == \"__main__\":\n",
    "-  tf.app.run()\n",
    "+  tf.compat.v1.app.run()\n",
    "diff --git a/xlnet.py b/xlnet.py\n",
    "index 4341e24..dfd8885 100644\n",
    "--- a/xlnet.py\n",
    "+++ b/xlnet.py\n",
    "@@ -60,7 +60,7 @@ class XLNetConfig(object):\n",
    "       setattr(self, key, getattr(FLAGS, key))\n",
    " \n",
    "   def init_from_json(self, json_path):\n",
    "-    with tf.gfile.Open(json_path) as f:\n",
    "+    with tf.io.gfile.GFile(json_path) as f:\n",
    "       json_data = json.load(f)\n",
    "       for key in self.keys:\n",
    "         setattr(self, key, json_data[key])\n",
    "@@ -74,7 +74,7 @@ class XLNetConfig(object):\n",
    "     json_dir = os.path.dirname(json_path)\n",
    "     if not tf.gfile.Exists(json_dir):\n",
    "       tf.gfile.MakeDirs(json_dir)\n",
    "-    with tf.gfile.Open(json_path, \"w\") as f:\n",
    "+    with tf.io.gfile.GFile(json_path, \"w\") as f:\n",
    "       json.dump(json_data, f, indent=4, sort_keys=True)\n",
    " \n",
    " \n",
    "@@ -217,7 +217,7 @@ class XLNetModel(object):\n",
    "         inp_q=inp_q)\n",
    "     tfm_args.update(input_args)\n",
    " \n",
    "-    with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE):\n",
    "+    with tf.compat.v1.variable_scope(\"model\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "       (self.output, self.new_mems, self.lookup_table\n",
    "           ) = modeling.transformer_xl(**tfm_args)\n",
    " \n",
    "@@ -240,7 +240,7 @@ class XLNetModel(object):\n",
    "     xlnet_config = self.xlnet_config\n",
    "     run_config = self.run_config\n",
    " \n",
    "-    with tf.variable_scope(\"model\", reuse=tf.AUTO_REUSE):\n",
    "+    with tf.compat.v1.variable_scope(\"model\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "       summary = modeling.summarize_sequence(\n",
    "           summary_type=summary_type,\n",
    "           hidden=self.output,\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
