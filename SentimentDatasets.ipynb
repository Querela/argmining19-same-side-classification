{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets (Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB\n",
    "\n",
    "- http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "&rarr; **BAD** - only single pos + neg review for each film\n",
    "\n",
    "- https://www.imdb.com/interfaces/\n",
    "\n",
    "&rarr; no review text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp\n",
    "\n",
    "- https://www.yelp.com/dataset/documentation/main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T11:31:24.066172Z",
     "start_time": "2020-03-23T11:31:23.599690Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import jsonlines\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from tqdm import tqdm\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({\"font.size\": 25, \"font.weight\": \"bold\"})\n",
    "# https://stackoverflow.com/questions/36622237/jupyter-notebook-inline-plots-as-svg\n",
    "# set_matplotlib_formats(\"svg\")  # glitches it\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_data import init_random\n",
    "init_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T11:31:29.183647Z",
     "start_time": "2020-03-23T11:31:29.180361Z"
    }
   },
   "outputs": [],
   "source": [
    "# download + scp to cuda + extract\n",
    "data_yelp_path = Path(\"/home/ekoerner/same-side-classification/data/sentiment/yelp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T11:32:26.584086Z",
     "start_time": "2020-03-23T11:31:30.382309Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6685900it [00:48, 137163.13it/s]\n"
     ]
    }
   ],
   "source": [
    "fn_yelp_reviews = data_yelp_path / \"review.json\"\n",
    "\n",
    "data = list()\n",
    "\n",
    "with jsonlines.open(fn_yelp_reviews, \"r\") as fp:\n",
    "    for n, entry in enumerate(tqdm(fp)):\n",
    "        # print(entry)\n",
    "        # break\n",
    "        business_id = entry.pop(\"business_id\")\n",
    "        review_id = entry.pop(\"review_id\")\n",
    "        text = entry.pop(\"text\", \"\")\n",
    "        rating = entry.pop(\"stars\", 0.0)\n",
    "        data.append({\"id\": business_id, \"rid\": review_id, \"text\": text, \"rating\": rating})\n",
    "        # TESTING\n",
    "        # if n > 10000:\n",
    "        #     break\n",
    "        \n",
    "df = pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T11:35:05.825431Z",
     "start_time": "2020-03-23T11:35:02.981079Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "192609it [00:02, 73694.20it/s]\n",
      "100%|██████████| 192127/192127 [00:00<00:00, 861900.50it/s]\n"
     ]
    }
   ],
   "source": [
    "fn_yelp_topics = data_yelp_path / \"business.json\"\n",
    "\n",
    "inv_bid_cats = dict()\n",
    "inv_cat_bids = dict()\n",
    "bids_not_cats = set()\n",
    "\n",
    "\n",
    "# load lookup business -> categories\n",
    "with jsonlines.open(fn_yelp_topics, \"r\") as fp:\n",
    "    for n, entry in enumerate(tqdm(fp)):\n",
    "        business_id = entry.pop(\"business_id\")\n",
    "        categories = entry.pop(\"categories\")\n",
    "\n",
    "        if not categories:\n",
    "            bids_not_cats.add(business_id)\n",
    "            continue\n",
    "\n",
    "        categories = categories.split(\", \")\n",
    "\n",
    "        inv_bid_cats[business_id] = categories\n",
    "\n",
    "# reverse lookup: category -> businesses\n",
    "for bid, cats in tqdm(inv_bid_cats.items()):\n",
    "    for cat in cats:\n",
    "        try:\n",
    "            inv_cat_bids[cat].append(bid)\n",
    "        except KeyError:\n",
    "            inv_cat_bids[cat] = [bid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T11:37:43.683277Z",
     "start_time": "2020-03-23T11:37:43.674745Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add goodness value\n",
    "# TODO: maybe ignore with 3\n",
    "df[\"goodness\"] = df[\"rating\"] > 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T11:40:21.843012Z",
     "start_time": "2020-03-23T11:40:21.840312Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of businesses with categories: 192127\n",
      "                  without categories:    482\n",
      "Number of categories: len(inv_cat_bids) = 1300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of businesses with categories: {len(inv_bid_cats):>6}\")\n",
    "print(f\"                  without categories: {len(bids_not_cats):>6}\")\n",
    "print(f\"Number of categories: len(inv_cat_bids) = {len(inv_cat_bids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T11:43:00.289600Z",
     "start_time": "2020-03-23T11:42:59.609394Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192127/192127 [00:00<00:00, 284411.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# make combinations with amount\n",
    "inv_cat_combis = Counter()\n",
    "\n",
    "inv_cat_combis.update(\n",
    "    (tuple(sorted(set(cats))) for cats in tqdm(inv_bid_cats.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T11:45:37.171561Z",
     "start_time": "2020-03-23T11:45:37.155441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Pizza', 'Restaurants'), 2035),\n",
       " (('Apartments', 'Home Services', 'Real Estate'), 1978),\n",
       " (('Beauty & Spas', 'Nail Salons'), 1978),\n",
       " (('Coffee & Tea', 'Food'), 1753),\n",
       " (('Mexican', 'Restaurants'), 1738),\n",
       " (('Beauty & Spas', 'Hair Salons'), 1650),\n",
       " (('Chinese', 'Restaurants'), 1629),\n",
       " (('Event Planning & Services', 'Hotels', 'Hotels & Travel'), 1426),\n",
       " (('Auto Repair', 'Automotive'), 1119),\n",
       " (('Food', 'Grocery'), 983),\n",
       " (('Italian', 'Restaurants'), 920),\n",
       " (('Burgers', 'Fast Food', 'Restaurants'), 885),\n",
       " (('Banks & Credit Unions', 'Financial Services'), 840),\n",
       " (('Fast Food', 'Restaurants', 'Sandwiches'), 800),\n",
       " (('Food', 'Ice Cream & Frozen Yogurt'), 781),\n",
       " (('Barbers', 'Beauty & Spas'), 745),\n",
       " (('Italian', 'Pizza', 'Restaurants'), 677),\n",
       " (('Auto Repair', 'Automotive', 'Oil Change Stations', 'Tires'), 635),\n",
       " (('Japanese', 'Restaurants', 'Sushi Bars'), 619),\n",
       " (('Dry Cleaning & Laundry', 'Laundry Services', 'Local Services'), 612)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_cat_combis.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T11:48:13.063858Z",
     "start_time": "2020-03-23T11:48:13.059391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Restaurants', 59371),\n",
       " ('Shopping', 31878),\n",
       " ('Food', 29989),\n",
       " ('Home Services', 19729),\n",
       " ('Beauty & Spas', 19370),\n",
       " ('Health & Medical', 17171),\n",
       " ('Local Services', 13932),\n",
       " ('Automotive', 13203),\n",
       " ('Nightlife', 13095),\n",
       " ('Bars', 11341),\n",
       " ('Event Planning & Services', 10371),\n",
       " ('Active Life', 9521),\n",
       " ('Fashion', 7798),\n",
       " ('Sandwiches', 7332),\n",
       " ('Coffee & Tea', 7321),\n",
       " ('Fast Food', 7257),\n",
       " ('American (Traditional)', 7107),\n",
       " ('Hair Salons', 6955),\n",
       " ('Pizza', 6804),\n",
       " ('Home & Garden', 6489)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count single categories\n",
    "cnt = Counter({k: len(v) for k, v in inv_cat_bids.items()})\n",
    "cnt.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T11:50:50.619657Z",
     "start_time": "2020-03-23T11:50:50.233350Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192127/192127 [00:00<00:00, 508649.43it/s]\n",
      "100%|██████████| 1300/1300 [00:00<00:00, 1701277.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# Check for subgraphs naive\n",
    "\n",
    "## build mapping for occurrence of each categories with other categories\n",
    "map_cat_cats = dict()\n",
    "\n",
    "for cats in tqdm(inv_bid_cats.values()):\n",
    "    for cat in cats:\n",
    "        try:\n",
    "            map_cat_cats[cat].update(cats)\n",
    "        except KeyError:\n",
    "            map_cat_cats[cat] = set(cats)\n",
    "\n",
    "cnt_cat_cats = Counter({cat: len(cats) for cat, cats in tqdm(map_cat_cats.items())})\n",
    "            \n",
    "set_all_cats = set(map_cat_cats.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T11:53:29.415696Z",
     "start_time": "2020-03-23T11:53:29.412023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of all cats: 1300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Shopping', 993),\n",
       " ('Local Services', 870),\n",
       " ('Event Planning & Services', 826),\n",
       " ('Professional Services', 800),\n",
       " ('Active Life', 782),\n",
       " ('Restaurants', 761),\n",
       " ('Arts & Entertainment', 740),\n",
       " ('Home Services', 736),\n",
       " ('Food', 731),\n",
       " ('Education', 671),\n",
       " ('Health & Medical', 645),\n",
       " ('Nightlife', 614),\n",
       " ('Hotels & Travel', 589),\n",
       " ('Beauty & Spas', 558),\n",
       " ('Local Flavor', 549),\n",
       " ('Home & Garden', 542),\n",
       " ('Automotive', 541),\n",
       " ('Fashion', 533),\n",
       " ('Specialty Schools', 517),\n",
       " ('Venues & Event Spaces', 502)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of all cats: {len(set_all_cats)}\")\n",
    "cnt_cat_cats.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### search for smallest any-connected subgraph ?\n",
    "\n",
    "merge all groups of related categories to find any two groups that are fully distinct over n edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T11:56:06.906898Z",
     "start_time": "2020-03-23T11:56:06.860847Z"
    },
    "code_folding": [
     6,
     12,
     20,
     45,
     46,
     57,
     89
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new group?: Shopping\n"
     ]
    }
   ],
   "source": [
    "## Try to compute distinct sub groups of categories\n",
    "inv_cat_group_id = dict()  # Dict[str, int]\n",
    "lst_cat_subgroups = list()  # List[List[str]]  # TODO: better Dict[int, Set[str]]\n",
    "\n",
    "def _has_related_cat_in_group_already(cat):\n",
    "    for rcat in map_cat_cats[cat]:\n",
    "        if rcat in inv_cat_group_id:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _find_related_cat_groups(cat):\n",
    "    rcat_groups = list()\n",
    "    for rcat in map_cat_cats[cat]:\n",
    "        gid = inv_cat_group_id.get(rcat, None)\n",
    "        if gid is not None:\n",
    "            rcat_groups.append((rcat, gid))\n",
    "    return rcat_groups\n",
    "\n",
    "def _trim_group_ids():\n",
    "    # search for empty groups (None or empty set)\n",
    "    lst_shifting = list()\n",
    "    for n, ele in enumerate(lst_cat_subgroups):\n",
    "        if not ele:\n",
    "            lst_shifting.append(n)\n",
    "\n",
    "    if not lst_shifting:\n",
    "        return\n",
    "\n",
    "    # move larger current groups over empty groups with lower id\n",
    "    for cat, old_gid in inv_cat_group_id.items():\n",
    "        new_gid = old_gid\n",
    "        for n in lst_shifting:\n",
    "            if n < new_gid:\n",
    "                # empty group before current group id,\n",
    "                # reduce current group id (--> skip empty group)\n",
    "                new_gid -= 1\n",
    "        # update on change (check should not really matter)\n",
    "        if new_gid != old_gid:\n",
    "            inv_cat_group_id[cat] = new_gid\n",
    "\n",
    "    for n in reversed(lst_shifting):\n",
    "        del lst_cat_subgroups[n]\n",
    "\n",
    "def _merge_related_cat_groups(cat, target_gid=None):\n",
    "    if target_gid is None:\n",
    "        # find first best gid\n",
    "        # (a) check category\n",
    "        target_gid = inv_cat_group_id.get(cat, None)\n",
    "        # (b) check related categories\n",
    "        if target_gid is None:\n",
    "            for rcat in map_cat_cats[cat]:\n",
    "                target_gid = inv_cat_group_id.get(rcat, None)\n",
    "                if target_gid is not None:\n",
    "                    break\n",
    "\n",
    "    if target_gid is None:\n",
    "        print(f\"No category to merge for {cat}!\")\n",
    "        return\n",
    "\n",
    "    # search for different group ids in related categories of cat\n",
    "    set_old_gid = set()\n",
    "    for rcat in map_cat_cats[cat]:\n",
    "        other_gid = inv_cat_group_id.get(rcat, None)\n",
    "        if other_gid is not None and other_gid != target_gid:\n",
    "            set_old_gid.add(other_gid)\n",
    "\n",
    "    # add non group related categories to target group\n",
    "    for rcat in map_cat_cats[cat]:\n",
    "        if rcat not in inv_cat_group_id:\n",
    "            inv_cat_group_id[rcat] = target_gid\n",
    "            lst_cat_subgroups[target_gid].add(rcat)\n",
    "\n",
    "    # rewrite other group ids to target group id\n",
    "    if not set_old_gid:\n",
    "        return\n",
    "\n",
    "    # rewrite inverse cat-gid mapping, empty old groups\n",
    "    for old_gid in set_old_gid:\n",
    "        old_rcats = lst_cat_subgroups[old_gid]\n",
    "        for old_rcat in old_rcats:\n",
    "            inv_cat_group_id[old_rcat] = target_gid\n",
    "        lst_cat_subgroups[old_gid] = None\n",
    "\n",
    "    _trim_group_ids()\n",
    "\n",
    "# -----\n",
    "            \n",
    "for cat, _ in cnt_cat_cats.most_common(None):\n",
    "    if cat in inv_cat_group_id:\n",
    "        # an existing group has this category already\n",
    "        # merge groups from related categories together\n",
    "        # additionally add new elements to target group (merged group)\n",
    "        _merge_related_cat_groups(cat)\n",
    "        _trim_group_ids()\n",
    "    else:\n",
    "        # new group possible\n",
    "        print(f\"new group?: {cat}\")\n",
    "        if not _has_related_cat_in_group_already(cat):\n",
    "            # open new group\n",
    "            gid = len(lst_cat_subgroups)\n",
    "            rcats = map_cat_cats[cat]\n",
    "            lst_cat_subgroups.append(set(rcats))\n",
    "            for icat in rcats:\n",
    "                inv_cat_group_id[icat] = gid\n",
    "        else:\n",
    "            # merge groups\n",
    "            _merge_related_cat_groups(cat)\n",
    "            _trim_group_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T11:58:45.779369Z",
     "start_time": "2020-03-23T11:58:45.775854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct groups: 1\n",
      "Number of elements in first group: 1300\n",
      "Number of all categories: 1300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of distinct groups: {len(lst_cat_subgroups)}\")\n",
    "print(f\"Number of elements in first group: {len(lst_cat_subgroups[0])}\")\n",
    "print(f\"Number of all categories: {len(set_all_cats)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### search for main groups \n",
    "\n",
    "groups can share related categories but can no directly appear together:  \n",
    "`{A, b} - {C, b}` -- ok, `C` and `A` are not together but share related elements  \n",
    "`{A, b, C} - {C, b}` -- not ok, as `C` is with `A`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T12:01:24.333138Z",
     "start_time": "2020-03-23T12:01:24.318420Z"
    }
   },
   "outputs": [],
   "source": [
    "lst_cat_dist_candidates = [cat for cat, _ in cnt_cat_cats.most_common(None)]\n",
    "# TODO: maybe random shuffle of candidates?\n",
    "set_cats_distinct = set()\n",
    "\n",
    "# lst_cat_dist_candidates.pop(0)\n",
    "\n",
    "while lst_cat_dist_candidates:\n",
    "    cat = lst_cat_dist_candidates.pop(0)\n",
    "    set_cats_distinct.add(cat)\n",
    "\n",
    "    # remove directly related categories, i.e. categories that appear together\n",
    "    # TODO: maybe compute frequency of co-occurrence of categories and make a cut-off???\n",
    "    for rcat in map_cat_cats[cat]:\n",
    "        try:\n",
    "            lst_cat_dist_candidates.remove(rcat)\n",
    "        except ValueError:\n",
    "            # element not in list anymore\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T12:04:02.059617Z",
     "start_time": "2020-03-23T12:04:02.055025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct categories (starting from the largest): 177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Shopping', 993),\n",
       " ('Soul Food', 105),\n",
       " ('Noodles', 104),\n",
       " ('Hawaiian', 99),\n",
       " ('German', 94),\n",
       " ('Public Transportation', 87),\n",
       " ('Personal Injury Law', 70),\n",
       " ('Campgrounds', 70),\n",
       " ('Brazilian', 69),\n",
       " ('Irish', 66),\n",
       " ('Osteopathic Physicians', 59),\n",
       " ('Zoos', 58),\n",
       " ('Afghan', 54),\n",
       " ('Home & Rental Insurance', 53),\n",
       " ('Argentine', 53),\n",
       " ('Ethiopian', 50),\n",
       " ('New Mexican Cuisine', 49),\n",
       " ('Pet Boarding', 47),\n",
       " ('Brewpubs', 47),\n",
       " ('Language Schools', 46)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\n",
    "    f\"Number of distinct categories (starting from the largest): {len(set_cats_distinct)}\"\n",
    ")\n",
    "cnt_dist_cats = Counter({\n",
    "    cat: cnt\n",
    "    for cat, cnt in cnt_cat_cats.most_common(None) if cat in set_cats_distinct\n",
    "})\n",
    "cnt_dist_cats.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maybe start with any element / or with random sorting (shuffled) N times and select the minimum length result or with the best mean/avg value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### search for distinct groups but just with no element intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T12:06:39.011899Z",
     "start_time": "2020-03-23T12:06:38.992076Z"
    }
   },
   "outputs": [],
   "source": [
    "lst_cat_dist_candidates = [cat for cat, _ in cnt_cat_cats.most_common(None)]\n",
    "# TODO: maybe random shuffle of candidates?\n",
    "set_cats_distinct = set()\n",
    "\n",
    "# [lst_cat_dist_candidates.pop(0) for _ in range(1)]\n",
    "\n",
    "while lst_cat_dist_candidates:\n",
    "    cat = lst_cat_dist_candidates.pop(0)\n",
    "    set_cats_distinct.add(cat)\n",
    "\n",
    "    # remove directly related categories, i.e. categories that appear together\n",
    "    # TODO: maybe compute frequency of co-occurrence of categories and make a cut-off???\n",
    "    for rcat in map_cat_cats[cat]:\n",
    "        try:\n",
    "            lst_cat_dist_candidates.remove(rcat)\n",
    "        except ValueError:\n",
    "            # element not in list anymore\n",
    "            pass\n",
    "\n",
    "    # remove categories that contain a shared related category\n",
    "    for rcat in map_cat_cats[cat]:\n",
    "        # for all current related categories\n",
    "        # check any of the remaining categories\n",
    "        for ocat in list(lst_cat_dist_candidates):\n",
    "            # that they contain not any related category as our current one\n",
    "            if rcat in set(map_cat_cats[ocat]):\n",
    "                lst_cat_dist_candidates.remove(ocat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T12:20:22.801134Z",
     "start_time": "2020-03-23T12:20:22.791620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct categories (starting from the largest): 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Shopping', 993)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\n",
    "    f\"Number of distinct categories (starting from the largest): {len(set_cats_distinct)}\"\n",
    ")\n",
    "cnt_dist_cats = Counter({\n",
    "    cat: cnt\n",
    "    for cat, cnt in cnt_cat_cats.most_common(None) if cat in set_cats_distinct\n",
    "})\n",
    "cnt_dist_cats.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test / Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exceprt\n",
    "df  # .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings info\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many reviews per business (id)\n",
    "df.groupby(\"id\").count()[[\"rating\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fn_goodness(row):\n",
    "#     row[\"good\"] = True if row[\"rating\"].value >= 2.5 else False\n",
    "#     return row\n",
    "\n",
    "# try:\n",
    "#     df = df.progress_apply(fn_goodness, axis=1)\n",
    "# except AttributeError:\n",
    "#     df = df.apply(fn_goodness)\n",
    "\n",
    "# set good if more than 3 in rating\n",
    "df[\"goodness\"] = df[\"rating\"] > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"goodness\", \"id\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only those that have more than N reviews\n",
    "df.groupby([\"id\", \"goodness\"])[[\"id\"]].count() > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only those IDs that contain both pos + negative reviews\n",
    "df.groupby([\"id\", \"goodness\"], as_index=False).count().groupby(\"id\")[[\"id\"]].count() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df), len(df[df[\"rating\"] == 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_bid_cats[list(inv_bid_cats.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T12:23:03.390943Z",
     "start_time": "2020-03-23T12:23:00.726693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1002159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>542394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>739280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>1468985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>2933082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id\n",
       "rating         \n",
       "1.0     1002159\n",
       "2.0      542394\n",
       "3.0      739280\n",
       "4.0     1468985\n",
       "5.0     2933082"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"rating\"], ).count()[[\"id\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter reviews with at least N ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-23T12:07:02.762909Z",
     "start_time": "2020-03-23T11:32:53.447Z"
    }
   },
   "outputs": [],
   "source": [
    "# min ratings per class (good|bad)\n",
    "min_ratings = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter with at least N ratings per goodness\n",
    "df_filter = df.groupby([\"id\", \"goodness\"])[[\"id\"]].count() < min_ratings\n",
    "df_filter = df_filter.rename(columns={\"id\": \"filter\"})\n",
    "df_filter = df_filter[df_filter[\"filter\"] == True]\n",
    "\n",
    "# build a filter id list\n",
    "df_filter_list = df_filter.reset_index()[\"id\"].to_list()\n",
    "\n",
    "# filter with list\n",
    "df_filtered = df[~df.id.isin(df_filter_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter only reviews with both good and bad rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build filter for ids that contain both positive and negative samples\n",
    "df_filter = df.groupby([\"id\", \"goodness\"], as_index=False).count().groupby(\"id\")[[\"id\"]].count() == 2\n",
    "df_filter = df_filter.rename(columns={\"id\": \"filter\"})\n",
    "df_filter = df_filter[df_filter[\"filter\"] == True]\n",
    "\n",
    "# create list of IDs for which this is true\n",
    "df_filter_list = df_filter.reset_index()[\"id\"].to_list()\n",
    "\n",
    "# filter with list\n",
    "df_filtered = df[df.id.isin(df_filter_list)]\n",
    "\n",
    "# df_filtered\n",
    "# df_filtered.groupby([\"id\", \"goodness\"]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "\n",
    "- check similar length -- what strategy to use to combine similar length pairs best?\n",
    "- flip sides (arg1 <-> arg2)\n",
    "- make more pairs (?) -- but keep same ratio of same-sentiment vs. not-same-sentiment (different-sentiment) ??\n",
    "- group by categories (topics) -- more explicit training (uniqueness of single arguments / topic structures)\n",
    "- evaluate on same-argument (S3C) - or amazon reviews\n",
    "- compute mean/avg sequence length\n",
    "- fixed layers in fine-tuning\n",
    "- visualization of trained layers and activation for good / bad inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Positive + negative same-sentiment pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pairs_per_class = 2\n",
    "# N positive + N negative\n",
    "# --> 2N pos+neg (not same-sentiment)\n",
    "\n",
    "# TODO: randomness for positive ss / negative ss - sum is equal / or sum equal compared to not ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     19,
     35
    ]
   },
   "outputs": [],
   "source": [
    "pairs_good = list()\n",
    "pairs_bad = list()\n",
    "\n",
    "for id_, group in tqdm(df.groupby(\"id\")):\n",
    "    grouper = group.groupby(\"goodness\")\n",
    "    reviews_good = grouper.get_group(True)\n",
    "    reviews_bad = grouper.get_group(False)\n",
    "\n",
    "    # TESTING\n",
    "    # print(\"id:\", id_)\n",
    "    # print(\"#good:\", len(reviews_good))\n",
    "    # print(\"#bad:\", len(reviews_bad))\n",
    "    # print(group)\n",
    "    # break\n",
    "\n",
    "    # make pairings -- good ss\n",
    "    rg_idx = reviews_good.index.values\n",
    "    # print(\"pos_idx:\", rg_idx)\n",
    "    rg_idx_sel = np.random.choice(rg_idx, 2 * num_pairs_per_class, replace=False)\n",
    "    for id1, id2 in zip(rg_idx_sel[::2], rg_idx_sel[1::2]):\n",
    "        # print(\"pair:\", id1, id2)\n",
    "        r1, r2 = df.loc[id1], df.loc[id2]\n",
    "        pair = {\n",
    "            \"argument1\": r1[\"text\"], \"argument2\": r2[\"text\"],\n",
    "            \"argument1_id\": f\"\"\"{r1[\"id\"]}|{r1[\"rid\"]}\"\"\", \"argument2_id\": f\"\"\"{r2[\"id\"]}|{r2[\"rid\"]}\"\"\",\n",
    "            \"is_same_side\": True, \"is_good_side\": True,\n",
    "            \"topic\": inv_bid_cats.get(r1[\"id\"], None)\n",
    "        }\n",
    "        # print(pair)\n",
    "        pairs_good.append(pair)\n",
    "\n",
    "    # make pairings -- bad ss\n",
    "    rb_idx = reviews_bad.index.values\n",
    "    # print(\"neg_idx:\", rb_idx)\n",
    "    rb_idx_sel = np.random.choice(rb_idx, 2 * num_pairs_per_class, replace=False)\n",
    "    for id1, id2 in zip(rb_idx_sel[::2], rb_idx_sel[1::2]):\n",
    "        r1, r2 = df.loc[id1], df.loc[id2]\n",
    "        pair = {\n",
    "            \"argument1\": r1[\"text\"], \"argument2\": r2[\"text\"],\n",
    "            \"argument1_id\": f\"\"\"{r1[\"id\"]}|{r1[\"rid\"]}\"\"\", \"argument2_id\": f\"\"\"{r2[\"id\"]}|{r2[\"rid\"]}\"\"\",\n",
    "            \"is_same_side\": True, \"is_good_side\": False,\n",
    "            \"topic\": inv_bid_cats.get(r1[\"id\"], None)\n",
    "        }\n",
    "        pairs_bad.append(pair)\n",
    "        \n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#ss (pos)\", len(pairs_good))\n",
    "print(\"#ss (neg)\", len(pairs_bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### not same-sentiment pairs (combinations positive + negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#: number of negative same-sentiment samples same as positive same-sentiment samples\n",
    "num_pairs_negative = 2 * num_pairs_per_class\n",
    "\n",
    "#: whether for a single side (good or bad) there can be multiple occurrences of the same review\n",
    "#: may need to check afterwared that not by chance same pairing happens ...\n",
    "repeatable_on_side = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     16,
     28
    ]
   },
   "outputs": [],
   "source": [
    "pairs_not_ss = list()\n",
    "\n",
    "for id_, group in tqdm(df.groupby(\"id\")):\n",
    "    grouper = group.groupby(\"goodness\")\n",
    "    reviews_good = grouper.get_group(True)\n",
    "    reviews_bad = grouper.get_group(False)\n",
    "\n",
    "    # find indices for reviews per business\n",
    "    rg_idx = reviews_good.index.values\n",
    "    rb_idx = reviews_bad.index.values\n",
    "\n",
    "    # randomly select from each side\n",
    "    rg_idx_sel = np.random.choice(rg_idx, num_pairs_negative, replace=repeatable_on_side)\n",
    "    rb_idx_sel = np.random.choice(rb_idx, num_pairs_negative, replace=repeatable_on_side)\n",
    "    \n",
    "    # pair them together -- good-bad pairs\n",
    "    for idg, idb in zip(rg_idx_sel[::2], rb_idx_sel[::2]):\n",
    "        rg, rb = df.loc[idg], df.loc[idb]\n",
    "        pair = {\n",
    "            \"argument1\": rg[\"text\"], \"argument2\": rb[\"text\"],\n",
    "            \"argument1_id\": f\"\"\"{rg[\"id\"]}|{rg[\"rid\"]}\"\"\", \"argument2_id\": f\"\"\"{rb[\"id\"]}|{rb[\"rid\"]}\"\"\",\n",
    "            \"is_same_side\": False, \"is_good_side\": None,\n",
    "            \"topic\": inv_bid_cats.get(rg[\"id\"], None)\n",
    "        }\n",
    "        # print(pair)\n",
    "        pairs_not_ss.append(pair)\n",
    "    \n",
    "    # bad-good pairs\n",
    "    for idb, idg in zip(rb_idx_sel[1::2], rg_idx_sel[1::2]):\n",
    "        rb, rg = df.loc[idb], df.loc[idg]\n",
    "        pair = {\n",
    "            \"argument1\": rb[\"text\"], \"argument2\": rg[\"text\"],\n",
    "            \"argument1_id\": f\"\"\"{rb[\"id\"]}|{rb[\"rid\"]}\"\"\", \"argument2_id\": f\"\"\"{rg[\"id\"]}|{rg[\"rid\"]}\"\"\",\n",
    "            \"is_same_side\": False, \"is_good_side\": None,\n",
    "            \"topic\": inv_bid_cats.get(rb[\"id\"], None)\n",
    "        }\n",
    "        # print(pair)\n",
    "        pairs_not_ss.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"#nss\", len(pairs_not_ss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dataframe for training etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_all = pairs_good + pairs_bad + pairs_not_ss\n",
    "print(\"#~ss\", len(pairs_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pairs_all[0][\"argument1_id\"])\n",
    "np.random.shuffle(pairs_all)\n",
    "# print(pairs_all[0][\"argument1_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_traindev = pd.DataFrame.from_dict(pairs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_yelp_df = data_yelp_path / \"df_traindev.p\"\n",
    "\n",
    "with open(fn_yelp_df, \"wb\") as fp:\n",
    "    pickle.dump(df_traindev, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reloading for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yelp_path = Path(\"/home/ekoerner/same-side-classification/data/sentiment/yelp/\")\n",
    "fn_yelp_df = data_yelp_path / \"df_traindev.p\"\n",
    "\n",
    "with open(fn_yelp_df, \"rb\") as fp:\n",
    "    all_df = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_dummy_tag(row):\n",
    "    row[\"tag\"] = \"NA\"\n",
    "    return row\n",
    "\n",
    "# all_df = all_df.progress_apply(_add_dummy_tag, axis=1)\n",
    "all_df[\"tag\"] = \"NA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = len(all_df)\n",
    "ratio = 0.3\n",
    "idx_ratio = int(num * (1.0 - ratio))\n",
    "print(f\"traindev: [0:{idx_ratio}], test: [{idx_ratio}:{num}], ratio: {ratio}\")\n",
    "\n",
    "traindev_df = all_df.iloc[:idx_ratio]\n",
    "test_df = all_df.iloc[idx_ratio:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(traindev_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Default S3C training\n",
    "\n",
    "https://gluon-nlp.mxnet.io/install.html\n",
    "\n",
    "```bash\n",
    "pip install --upgrade 'mxnet>=1.3.0'  \n",
    "pip install gluonnlp  \n",
    "wget https://gluon-nlp.mxnet.io/_downloads/sentence_embedding.zip  \n",
    "unzip sentence_embedding.zip  \n",
    "ln -s sentence_embedding/bert bert  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxboard import SummaryWriter\n",
    "\n",
    "from utils_data import get_train_test_sets\n",
    "\n",
    "from utils_gluon import setup_bert_epi128bce, setup_bert_epi512bce\n",
    "from utils_gluon import setup_bert_pro128bce, setup_bert_pro512bce\n",
    "\n",
    "from utils_gluon import transform_dataset\n",
    "from utils_gluon import predict_out_to_ys\n",
    "\n",
    "from utils_gluon import predict\n",
    "from utils_gluon import train\n",
    "\n",
    "# from utils_gluon import print_infos\n",
    "from utils_gluon import plot_train_stats\n",
    "\n",
    "from utils_gluon import report_training_results\n",
    "from utils_gluon import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_data import Timer\n",
    "\n",
    "from utils_data import configure_logging\n",
    "configure_logging()\n",
    "\n",
    "from utils_data import init_random\n",
    "init_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_dev, y_train, y_dev = get_train_test_sets(traindev_df, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_pro128bce(gpu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"prepare training / validation data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"yelp_1_pro128BCE\"\n",
    "fn_run_path = Path(f\"data/{run_name}\")\n",
    "\n",
    "! mkdir data/yelp_1_pro128BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training with TRAIN set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_id in range(num_epochs):\n",
    "    with Timer(f\"train model - {epoch_id}\"), SummaryWriter(logdir=str(fn_run_path), flush_secs=600) as sw:\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=batch_size, lr=5e-6, num_epochs=epoch_id + 1, sw=sw, checkpoint_dir=str(fn_run_path))\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(f\"evaluate - {epoch_id}\"), SummaryWriter(logdir=str(fn_run_path), flush_secs=600) as sw:\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=batch_size, sw=sw)\n",
    "        print(f\"Accuracy in epoch {epoch_id}:\", metric.get()[1])\n",
    "\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=f\"BERTClassifier - {run_name}\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(str(fn_run_path / \"bert.model.params\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results with validation set (DEV set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=batch_size)\n",
    "y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "with open(fn_run_path / \"eval_untrained.p\", \"wb\") as fp:\n",
    "    pickle.dump(y_true, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(y_pred, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "report_training_results(y_true, y_pred, name=f\"BERTClassifier - {run_name}\", heatmap=False)\n",
    "plot_confusion_matrix(y_true, y_pred, labels=[0, 1], title=f\"{run_name} Untrained\", values_format=\"d\", cmap=\"Blues\", include_colorbar=False)\n",
    "plt.show()\n",
    "\n",
    "print(\"Evaluate trained model for each epoch ...\")\n",
    "for epoch_id in range(num_epochs):\n",
    "    fn_model_state = fn_run_path / f\"bert.model.checkpoint{epoch_id}.params\"\n",
    "    print(f\"Load {fn_model_state} ...\")\n",
    "    model.load_parameters(str(fn_model_state), ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=batch_size)\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    with open(fn_run_path / f\"eval_epoch{epoch_id}.p\", \"wb\") as fp:\n",
    "        pickle.dump(y_true, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickle.dump(y_pred, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    report_training_results(y_true, y_pred, name=f\"BERTClassifier - {run_name}\", heatmap=False)\n",
    "    plot_confusion_matrix(y_true, y_pred, labels=[0, 1], title=f\"Yelp 1 Epoch {epoch_id}\", values_format=\"d\", cmap=\"Blues\", include_colorbar=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validate with TEST set (usually hidden y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_data import names_columns_X, names_columns_y\n",
    "\n",
    "X_test = test_df[names_columns_X]\n",
    "y_test = test_df[names_columns_y]\n",
    "data_test_raw, data_test = transform_dataset(X_test, y_test, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epoch_id = num_epochs - 1\n",
    "model.load_parameters(str(fn_run_path / f\"bert.model.checkpoint{epoch_id}.params\"), ctx=ctx)\n",
    "\n",
    "all_predictions, cum_loss = predict(model, data_test, ctx, metric, loss_function, batch_size=batch_size, sw=sw)\n",
    "print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "report_training_results(y_true, y_pred, name=f\"BERTClassifier - {run_name}\", heatmap=True)\n",
    "plot_confusion_matrix(y_true, y_pred, labels=[0, 1], title=f\"Yelp {num_epochs} Epochs\", values_format=\"d\", cmap=\"Blues\", include_colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "report_training_results(y_true, y_pred, name=f\"BERTClassifier - {run_name}\", heatmap=True)\n",
    "plot_confusion_matrix(y_true, y_pred, labels=[0, 1], title=f\"Yelp {num_epochs} Epochs\", values_format=\"d\", cmap=\"Blues\", include_colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results\n",
    "\n",
    "###### Exp. 1\n",
    "\n",
    "Yelp\n",
    "\n",
    "- 5 reviews per pos/neg, at least pos+neg per business\n",
    "- rating range [1, 5] &rarr;  >3 for positive, below negative\n",
    "- 2 pairs each for pos-pos, neg-neg, pos-neg, neg-pos; random selection of reviews ordering\n",
    "- data split:\n",
    "\n",
    "  - 492664 pairs (‭61583‬ unique businesses?, twice the number for each XY pairing, 4 times of that for total)\n",
    "  - raw &rarr; 30:70 test/traindev,\n",
    "  - traindev &rarr; 90:10 train/validation\n",
    "  - batches: 1078 validation, 9704 training, test ?\n",
    "\n",
    "\n",
    "BERT\n",
    "\n",
    "- GluonNLP BERT-base EN, seq_len: 128, batch_size: 32, BCE loss\n",
    "- 3 Epochs finetuning\n",
    "\n",
    "```\n",
    "Accuracy: 0.8129431664411366\n",
    "Confusion Matrix:\n",
    "[[60399 13871]\n",
    " [13776 59754]]\n",
    "\n",
    "CM: [60399 13871 13776 59754] \n",
    "[tn, fp, fn, tp]\n",
    "  accuracy: 0.813\n",
    " precision: 0.813\n",
    "    recall: 0.813\n",
    "  f1-score: 0.813\n",
    "\n",
    "\n",
    "Accuracy:  0.813 \n",
    "\n",
    "Report for [BERTClassifier - yelp_1_pro128BCE]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.81      0.81      0.81     74270\n",
    "           1       0.81      0.81      0.81     73530\n",
    "\n",
    "    accuracy                           0.81    147800\n",
    "   macro avg       0.81      0.81      0.81    147800\n",
    "weighted avg       0.81      0.81      0.81    147800\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross evaluate sentiment model against argument dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxboard import SummaryWriter\n",
    "\n",
    "from utils_data import load_and_prepare_official_data\n",
    "from utils_data import get_train_test_sets\n",
    "from utils_data import names_columns_X, names_columns_y\n",
    "\n",
    "from utils_gluon import setup_bert_epi128bce, setup_bert_epi512bce\n",
    "from utils_gluon import setup_bert_pro128bce, setup_bert_pro512bce\n",
    "\n",
    "from utils_gluon import transform_dataset\n",
    "from utils_gluon import predict_out_to_ys\n",
    "\n",
    "from utils_gluon import predict\n",
    "from utils_gluon import train\n",
    "\n",
    "# from utils_gluon import print_infos\n",
    "# from utils_gluon import plot_train_stats\n",
    "\n",
    "from utils_gluon import report_training_results\n",
    "from utils_gluon import plot_confusion_matrix\n",
    "\n",
    "from utils_data import Timer\n",
    "\n",
    "from utils_data import configure_logging\n",
    "configure_logging()\n",
    "\n",
    "from utils_data import init_random\n",
    "init_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"yelp_1_pro128BCE\"\n",
    "fn_run_path = Path(f\"data/{run_name}\")\n",
    "num_epochs = 3\n",
    "\n",
    "data_cross_path = \"data/same-side-classification/cross-topic/{}.csv\"\n",
    "data_within_path = \"data/same-side-classification/within-topic/{}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_pro128bce(gpu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_id = num_epochs - 1\n",
    "model.load_parameters(str(fn_run_path / f\"bert.model.checkpoint{epoch_id}.params\"), ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with Timer(\"load training / validation data\"):\n",
    "    cross_traindev_df, cross_test_df = load_and_prepare_official_data(\"cross\")\n",
    "    within_traindev_df, within_test_df = load_and_prepare_official_data(\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_traindev_df = within_traindev_df\n",
    "val_traindev_df = cross_traindev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"prepare training / validation data\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(val_traindev_df, ratio=0.1)\n",
    "\n",
    "    # data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"prepare training / validation data\"):\n",
    "    # whole dataset\n",
    "    X_dev = val_traindev_df[names_columns_X]\n",
    "    y_dev = val_traindev_df[names_columns_y]\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=32, sw=None)\n",
    "print(f\"Accuracy in epoch {epoch_id}: {metric.get()[1]}\")\n",
    "\n",
    "y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "report_training_results(y_true, y_pred, name=f\"BERTClassifier - {run_name}\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "- `within` validation set, 10% of traindev\n",
    "- 3 epochs fine-tuned BERT model of same-sentiment yelp (Exp. 1)\n",
    "\n",
    "<details>\n",
    "    <summary>Stats</summary>\n",
    "    \n",
    "```\n",
    "Time for [prediction]: 0:01:02.192490\n",
    "Accuracy in epoch 2: 0.5449851353465811\n",
    "Confusion Matrix:\n",
    "[[ 269 2690]\n",
    " [ 218 3214]]\n",
    "\n",
    "CM: [ 269 2690  218 3214] \n",
    "[tn, fp, fn, tp]\n",
    "  accuracy: 0.545\n",
    " precision: 0.548\n",
    "    recall: 0.514\n",
    "  f1-score: 0.422\n",
    "\n",
    "Accuracy:  0.545 \n",
    "\n",
    "Report for [BERTClassifier - yelp_1_pro128BCE]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.55      0.09      0.16      2959\n",
    "           1       0.54      0.94      0.69      3432\n",
    "\n",
    "    accuracy                           0.54      6391\n",
    "   macro avg       0.55      0.51      0.42      6391\n",
    "weighted avg       0.55      0.54      0.44      6391\n",
    "```\n",
    "</details>\n",
    "\n",
    "```python\n",
    "{'macro': 0.422, 'micro': 0.545}\n",
    "```\n",
    "\n",
    "- same as before but with **1 epoch Exp. 1 model**\n",
    "\n",
    "<details>\n",
    "    <summary>Stats</summary>\n",
    "\n",
    "```\n",
    "Time for [prediction]: 0:01:02.188106\n",
    "Accuracy in epoch 0: 0.5373181035831638\n",
    "Confusion Matrix:\n",
    "[[ 156 2803]\n",
    " [ 154 3278]]\n",
    "\n",
    "CM: [ 156 2803  154 3278] \n",
    "[tn, fp, fn, tp]\n",
    "  accuracy: 0.537\n",
    " precision: 0.521\n",
    "    recall: 0.504\n",
    "  f1-score: 0.392\n",
    "\n",
    "Accuracy:  0.537 \n",
    "\n",
    "Report for [BERTClassifier - yelp_1_pro128BCE]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.50      0.05      0.10      2959\n",
    "           1       0.54      0.96      0.69      3432\n",
    "\n",
    "    accuracy                           0.54      6391\n",
    "   macro avg       0.52      0.50      0.39      6391\n",
    "weighted avg       0.52      0.54      0.41      6391\n",
    "```\n",
    "</details>\n",
    "\n",
    "```python\n",
    "{'macro': 0.392, 'micro': 0.537}\n",
    "```\n",
    "\n",
    "- whole `within` dataset but with 3 epochs model\n",
    "\n",
    "<details>\n",
    "    <summary>Stats</summary>\n",
    "\n",
    "```\n",
    "Accuracy in epoch 2: 0.5375178004162559\n",
    "Confusion Matrix:\n",
    "[[ 2615 27177]\n",
    " [ 2377 31734]]\n",
    "\n",
    "CM: [ 2615 27177  2377 31734] \n",
    "[tn, fp, fn, tp]\n",
    "  accuracy: 0.538\n",
    " precision: 0.531\n",
    "    recall: 0.509\n",
    "  f1-score: 0.416\n",
    "\n",
    "Accuracy:  0.538 \n",
    "\n",
    "Report for [BERTClassifier - yelp_1_pro128BCE]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.52      0.09      0.15     29792\n",
    "           1       0.54      0.93      0.68     34111\n",
    "\n",
    "    accuracy                           0.54     63903\n",
    "   macro avg       0.53      0.51      0.42     63903\n",
    "weighted avg       0.53      0.54      0.43     63903\n",
    "```\n",
    "</details>\n",
    "\n",
    "```python\n",
    "{'macro': 0.416, 'micro': 0.538}\n",
    "```\n",
    "\n",
    "- `cross` validation set (10% traindev) with 3 epochs model\n",
    "\n",
    "<details>\n",
    "    <summary>Stats</summary>\n",
    "\n",
    "```\n",
    "Accuracy in epoch 2: 0.513022113022113\n",
    "Confusion Matrix:\n",
    "[[ 286 2738]\n",
    " [ 235 2846]]\n",
    "\n",
    "CM: [ 286 2738  235 2846] \n",
    "[tn, fp, fn, tp]\n",
    "  accuracy: 0.513\n",
    " precision: 0.529\n",
    "    recall: 0.509\n",
    "  f1-score: 0.409\n",
    "\n",
    "Accuracy:  0.513 \n",
    "\n",
    "Report for [BERTClassifier - yelp_1_pro128BCE]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.55      0.09      0.16      3024\n",
    "           1       0.51      0.92      0.66      3081\n",
    "\n",
    "    accuracy                           0.51      6105\n",
    "   macro avg       0.53      0.51      0.41      6105\n",
    "weighted avg       0.53      0.51      0.41      6105\n",
    "```\n",
    "</details>\n",
    "\n",
    "```python\n",
    "{'macro': 0.409, 'micro': 0.513}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make layer until 6 static ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwparams = model.collect_params()\n",
    "keys = kwparams.keys()\n",
    "lt6 = [\n",
    "    k for k in keys\n",
    "    if k.startswith(\"bertencoder0_transformer\") and int(k[24]) in (\n",
    "        0, 1, 2, 3, 4, 5) and k[25] == \"_\"\n",
    "]\n",
    "lt6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal params\n",
    "params = [\n",
    "    p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "]\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_nr(name):\n",
    "    # name = param.name\n",
    "    \n",
    "    if not name.startswith(\"bertencoder\") or \"transformer\" not in name:\n",
    "        return None\n",
    "    \n",
    "    name_layer = name.split(\"_\")[1]\n",
    "    num = int(name_layer[11:])\n",
    "    return num\n",
    "\n",
    "\n",
    "def is_first_n_layer(name, n):\n",
    "    return get_layer_nr(name) in set(range(n))\n",
    "\n",
    "\n",
    "def is_last_n_layer(name, n, params_all):\n",
    "    num_layers = max(n for n in (get_layer_nr(p) for p in params_all.keys()) if n is not None) + 1\n",
    "    return get_layer_nr(name) in set(range(num_layers - n, num_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params[10].name\n",
    "get_layer_nr(params[10])\n",
    "\n",
    "params = [\n",
    "    p for p in model.collect_params().values()\n",
    "    if p.grad_req != 'null' and not is_first_n_layer(p.name, 6)\n",
    "]\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: in trainer remove layer params that we do not want trained\n",
    "# https://mxnet.incubator.apache.org/api/python/docs/_modules/mxnet/gluon/parameter.html#ParameterDict.zero_grad\n",
    "# https://beta.mxnet.io/_modules/mxnet/gluon/block.html#Block.collect_params\n",
    "# \n",
    "# ret = ParameterDict(self._params.prefix)\n",
    "# ret.update({name:value for name, value in self.params.items() if pattern.match(name)})\n",
    "# \n",
    "from mxnet.gluon.parameter import ParameterDict\n",
    "params_all = model.collect_params()\n",
    "params_sub = ParameterDict(params_all.prefix)\n",
    "params_sub.update({name: value for name, value in params_all.items() if not is_first_n_layer(name, 6)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon\n",
    "\n",
    "- https://nijianmo.github.io/amazon/index.html#subsets\n",
    "- https://colab.research.google.com/drive/1Zv6MARGQcrBbLHyjPVVMZVnRWsRnVMpV#scrollTo=LgWrDtZ94w89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_amazon_path = Path(\"/home/ekoerner/same-side-classification/data/sentiment/amazon/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example from colab notebook\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_amazon_auto_reviews = data_amazon_path / \"Automotive_5.json\"\n",
    "\n",
    "data = list()\n",
    "with jsonlines.open(fn_amazon_auto_reviews, \"r\") as fp:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
