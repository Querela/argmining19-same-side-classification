{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gluon-nlp.mxnet.io/install.html\n",
    "\n",
    "```\n",
    "pip install --upgrade 'mxnet>=1.3.0'\n",
    "pip install gluonnlp\n",
    "wget https://gluon-nlp.mxnet.io/_downloads/sentence_embedding.zip\n",
    "unzip sentence_embedding.zip\n",
    "ln -s sentence_embedding/bert bert\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:45:45.911755Z",
     "start_time": "2019-11-26T11:45:44.630469Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import csv\n",
    "import gluonnlp as nlp\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from bert import *\n",
    "from mxboard import SummaryWriter\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.data import Dataset, SimpleDataset\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:45:45.917236Z",
     "start_time": "2019-11-26T11:45:45.913913Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:45:45.928577Z",
     "start_time": "2019-11-26T11:45:45.918691Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:45:45.940716Z",
     "start_time": "2019-11-26T11:45:45.930463Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# set repeatable random state\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:45:45.951736Z",
     "start_time": "2019-11-26T11:45:45.942633Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply progress bars for pandas .apply() -> .progress_apply()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:45:45.994413Z",
     "start_time": "2019-11-26T11:45:45.955538Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make tqdm jupyter friendly\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# for .progress_apply() we have to hack it like this?\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:45:46.001437Z",
     "start_time": "2019-11-26T11:45:45.996623Z"
    },
    "code_folding": [
     0,
     4
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        time_end = time.time()\n",
    "        time_delta = datetime.timedelta(seconds=(time_end - self.time_start))\n",
    "        if self.name:\n",
    "            print((\"Time for [{}]: {}\".format(self.name, time_delta)))\n",
    "        else:\n",
    "            print((\"Time: {}\".format(time_delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Same Side Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:45:46.013351Z",
     "start_time": "2019-11-26T11:45:46.003803Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cross_path = 'data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = 'data/same-side-classification/within-topic/{}.csv'\n",
    "new_within_test = 'data/same-side-classification/within-topic/within_test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load within-topics and cross-topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:45:48.315001Z",
     "start_time": "2019-11-26T11:45:46.016210Z"
    },
    "code_folding": [
     0,
     10
    ],
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [read cross]: 0:00:00.943807\n",
      "Time for [read within]: 0:00:00.943532\n",
      "Time for [read new within]: 0:00:00.399884\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"read cross\"):\n",
    "    cross_traindev_df = pd.read_csv(data_cross_path.format('training'),\n",
    "                                    quotechar='\"',\n",
    "                                    quoting=csv.QUOTE_ALL,\n",
    "                                    encoding='utf-8',\n",
    "                                    escapechar='\\\\',\n",
    "                                    doublequote=False,\n",
    "                                    index_col='id')\n",
    "    cross_test_df = pd.read_csv(data_cross_path.format('test'), index_col='id')\n",
    "\n",
    "with Timer(\"read within\"):\n",
    "    within_traindev_df = pd.read_csv(data_within_path.format('training'),\n",
    "                                     quotechar='\"',\n",
    "                                     quoting=csv.QUOTE_ALL,\n",
    "                                     encoding='utf-8',\n",
    "                                     escapechar='\\\\',\n",
    "                                     doublequote=False,\n",
    "                                     index_col='id')\n",
    "    # within_test_df = pd.read_csv(data_within_path.format('test'),\n",
    "    #                              quotechar='\"',\n",
    "    #                              quoting=csv.QUOTE_ALL,\n",
    "    #                              encoding='utf-8',\n",
    "    #                              escapechar='\\\\',\n",
    "    #                              doublequote=True,  # <-- change, \"\" as quote escape in text?\n",
    "    #                              index_col='id')\n",
    "    within_test_df = pd.read_csv(data_within_path.format('test'), index_col='id')\n",
    "\n",
    "with Timer(\"read new within\"):\n",
    "    new_within_test_df = pd.read_csv(new_within_test, index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 5 data/same-side-classification/within-topic/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 5 data/same-side-classification/within-topic/within_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.342633Z",
     "start_time": "2019-11-26T11:45:48.316619Z"
    },
    "code_folding": [
     1
    ],
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=61048), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag cross traindev]: 0:00:36.046703\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6163), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag cross test]: 0:00:03.575852\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=63903), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag within traindev]: 0:00:37.735816\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3552), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag within test]: 0:00:02.131041\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=31475), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag new within test]: 0:00:18.531344\n"
     ]
    }
   ],
   "source": [
    "# Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "def add_tag(row):\n",
    "    title = row['topic'].lower().strip()\n",
    "    if \"abortion\" in title:\n",
    "        row['tag'] = 'abortion'\n",
    "    elif \"gay marriage\"  in title:\n",
    "        row['tag'] = 'gay marriage'\n",
    "    else:\n",
    "        row['tag'] = 'NA'\n",
    "    return row\n",
    "\n",
    "\n",
    "with Timer(\"tag cross traindev\"):\n",
    "    cross_traindev_df = cross_traindev_df.progress_apply(add_tag, axis=1)\n",
    "with Timer(\"tag cross test\"):\n",
    "    cross_test_df = cross_test_df.progress_apply(add_tag, axis=1)\n",
    "\n",
    "with Timer(\"tag within traindev\"):\n",
    "    within_traindev_df = within_traindev_df.progress_apply(add_tag, axis=1)\n",
    "with Timer(\"tag within test\"):\n",
    "    within_test_df = within_test_df.progress_apply(add_tag, axis=1)\n",
    "with Timer(\"tag new within test\"):\n",
    "    new_within_test_df = new_within_test_df.progress_apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview about each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.353686Z",
     "start_time": "2019-11-26T11:47:26.344154Z"
    },
    "code_folding": [
     4
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# requires nltk  wordtokenize\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# model uses BERT Tokenizer ...\n",
    "\n",
    "def get_overview(df, task='same-side', class_name='is_same_side'):\n",
    "    # Total instance numbers\n",
    "    total = len(df)\n",
    "    print(\"Task: \", task)\n",
    "    print('=' * 40, '\\n')\n",
    "\n",
    "    print('Total instances: ', total)\n",
    "    print('\\n')\n",
    "\n",
    "    print('For each topic:')\n",
    "    for tag, tag_df in df.groupby(['tag']):\n",
    "        print(tag, ': ', len(tag_df), ' instances')\n",
    "        if class_name in df.columns:\n",
    "            for is_same_side, side_df in tag_df.groupby([class_name]):\n",
    "                print('\\t\\t', is_same_side, ': ', len(side_df), ' instances')\n",
    "    print('\\n')\n",
    "\n",
    "    if class_name in df.columns:\n",
    "        print('For each class value:')\n",
    "        for class_value, class_df in df.groupby([class_name]):\n",
    "            print(class_value, ': ', len(class_df), ' instances')\n",
    "        print('\\n')\n",
    "\n",
    "    print('Unique argument1:', len(df['argument1'].unique()))\n",
    "    print('Unique argument2:', len(df['argument2'].unique()))\n",
    "    arguments = df['argument1'].values\n",
    "    arguments = np.concatenate([arguments, df['argument2'].values])\n",
    "\n",
    "    print('Unique total arguments:', len(set(list(arguments))), '\\n')\n",
    "    \n",
    "    return\n",
    "\n",
    "    print('-' * 40, '\\n')\n",
    "\n",
    "    arguments_length_lst = [\n",
    "        len(word_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_length_lst.extend(\n",
    "        [len(word_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Words:')\n",
    "    print('\\tshortest argument:', min(arguments_length_lst), ' words')\n",
    "    print('\\tlongest argument:', max(arguments_length_lst), ' words')\n",
    "    print('\\targument average length:', np.mean(arguments_length_lst),\n",
    "          ' words')\n",
    "\n",
    "    arguments_sent_length_lst = [\n",
    "        len(sent_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_sent_length_lst.extend(\n",
    "        [len(sent_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Sentences:')\n",
    "    print('\\tshortest argument:', min(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\tlongest argument:', max(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\targument average length:', np.mean(arguments_sent_length_lst),\n",
    "          ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with Timer(\"overview cross\"):\n",
    "    get_overview(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"overview within\"):\n",
    "    get_overview(within_traindev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count raw length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compute_arg_len(row):\n",
    "    row['argument1_len'] = len(row['argument1'])\n",
    "    row['argument2_len'] = len(row['argument2'])\n",
    "    row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "    row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff']\n",
    "    return row\n",
    "\n",
    "\n",
    "cross_traindev_df = cross_traindev_df.progress_apply(compute_arg_len, axis=1)\n",
    "within_traindev_df = within_traindev_df.progress_apply(compute_arg_len, axis=1)\n",
    "cross_test_df = cross_test_df.progress_apply(compute_arg_len, axis=1)\n",
    "within_test_df = within_test_df.progress_apply(compute_arg_len, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize and count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()\n",
    "_, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                    dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                    pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                    use_decoder=False, use_classifier=False)\n",
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "tokenizer = bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download('punct')\n",
    "\n",
    "\n",
    "# tokenizer from BERT\n",
    "def tokenize_arguments(row):\n",
    "    # tokenize\n",
    "    row['argument1_tokens'] = tokenizer(row['argument1'])\n",
    "    row['argument2_tokens'] = tokenizer(row['argument2'])\n",
    "\n",
    "    # count tokens\n",
    "    row['argument1_len'] = len(row['argument1_tokens'])\n",
    "    row['argument2_len'] = len(row['argument2_tokens'])\n",
    "    # token number diff\n",
    "    row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "    row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff'])\n",
    "    return row\n",
    "\n",
    "\n",
    "cross_traindev_df = cross_traindev_df.progress_apply(tokenize_arguments, axis=1)\n",
    "within_traindev_df = within_traindev_df.progress_apply(tokenize_arguments, axis=1)\n",
    "cross_test_df = cross_test_df.progress_apply(tokenize_arguments, axis=1)\n",
    "within_test_df = within_test_df.progress_apply(tokenize_arguments, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_lengths(df, slicen=None, abs_diff=True, title=None):\n",
    "    if df is None:\n",
    "        print(\"no lengths to plot\")\n",
    "        return\n",
    "    \n",
    "    arg1_lens = df['argument1_len']\n",
    "    arg2_lens = df['argument2_len']\n",
    "    arg_diff_len = df['argument12_len_diff']\n",
    "    \n",
    "    if abs_diff:\n",
    "        arg_diff_len = np.abs(arg_diff_len)\n",
    "    \n",
    "    if slicen is not None:\n",
    "        arg1_lens = arg1_lens[slicen]\n",
    "        arg2_lens = arg2_lens[slicen]\n",
    "        arg_diff_len = arg_diff_len[slicen]\n",
    "\n",
    "    x = np.arange(len(arg1_lens))  # arange/linspace\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, arg1_lens, label='argument1')  # Linie: '-', 'o-', '.-'\n",
    "    plt.plot(x, arg2_lens, label='argument2')  # Linie: '-', 'o-', '.-'\n",
    "    plt.legend()\n",
    "    plt.title('Lengths of arguments' if not title else title)\n",
    "    plt.ylabel('Lengths of arguments 1 and 2')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, arg_diff_len)\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Differences')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_lengths(within_traindev_df, slice(None, None, 500), title='Length of arguments within train/dev, every 500')\n",
    "plot_lengths(cross_traindev_df, slice(None, None, 500), title='Length of arguments cross train/dev, every 500')\n",
    "plot_lengths(within_test_df, slice(None, None, 1), title='Length of arguments within test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dev set - 70% 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.374294Z",
     "start_time": "2019-11-26T11:47:26.355064Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']]\n",
    "    y = df[['is_same_side']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "- https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.391156Z",
     "start_time": "2019-11-26T11:47:26.375592Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyBERTDataset(SimpleDataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        super(MyBERTDataset, self).__init__(self._convert())\n",
    "\n",
    "    def _convert(self):\n",
    "        allsamples = list()\n",
    "\n",
    "        if self._y is not None:\n",
    "            df = self._X.merge(self._y, left_index=True, right_index=True)\n",
    "            for _, row in df.iterrows():\n",
    "                # allsamples.append([\n",
    "                #     row['argument1'], row['argument2'],\n",
    "                #     \"1\" if str(row['is_same_side']) == \"True\" else \"0\"\n",
    "                # ])\n",
    "                allsamples.append([\n",
    "                    row['argument1'], row['argument2'],\n",
    "                    1 if str(row['is_same_side']) == \"True\" else 0\n",
    "                ])\n",
    "\n",
    "        else:\n",
    "            for _, row in self._X.iterrows():\n",
    "                allsamples.append([row['argument1'], row['argument2'], None])\n",
    "\n",
    "        return allsamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### my own `BERTDatasetTransform` for extracting chunks from arguments or last part etc.\n",
    "\n",
    "```python\n",
    "transform = dataset.BERTDatasetTransform(bert_tokenizer, 512,\n",
    "                                         labels=['0', '1'],\n",
    "                                         label_dtype='int32',\n",
    "                                         pad=True,\n",
    "                                         pair=True)\n",
    "```\n",
    "\n",
    "http://localhost:9001/edit/bert/dataset.py @454\n",
    "```python\n",
    "# substitute with my own (e. g. last part, many parts etc.)\n",
    "def __init__(...):\n",
    "    self._bert_xform = BERTSentenceTransform(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "```\n",
    "https://gluon-nlp.mxnet.io/master/_modules/gluonnlp/data/transforms.html#BERTSentenceTransform\n",
    "```python\n",
    "# substitute with my own (e. g. only last part (trim from start))\n",
    "self._truncate_seq_pair(tokens_a, tokens_b, self._max_seq_length - 3)\n",
    "```\n",
    "\n",
    "https://mxnet.incubator.apache.org/_modules/mxnet/gluon/data/dataset.html#Dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.415939Z",
     "start_time": "2019-11-26T11:47:26.392814Z"
    },
    "code_folding": [
     3,
     8,
     72,
     87
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from gluonnlp.data import BERTSentenceTransform\n",
    "\n",
    "\n",
    "class FirstAndLastPartBERTSentenceTransform(BERTSentenceTransform):\n",
    "    def __init__(self, tokenizer, max_seq_length, pad=True, pair=True):\n",
    "        super(FirstAndLastPartBERTSentenceTransform,\n",
    "              self).__init__(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "    def __call__(self, line):\n",
    "        # convert to unicode\n",
    "        text_a = line[0]\n",
    "        if self._pair:\n",
    "            assert len(line) == 2\n",
    "            text_b = line[1]\n",
    "\n",
    "        tokens_a = self._tokenizer(text_a)\n",
    "        tokens_a_epi = tokens_a.copy()\n",
    "        tokens_b = None\n",
    "        tokens_b_epi = None\n",
    "\n",
    "        if self._pair:\n",
    "            tokens_b = self._tokenizer(text_b)\n",
    "            tokens_b_epi = tokens_b.copy()\n",
    "\n",
    "        if tokens_b:\n",
    "            self._truncate_seq_pair_prolog(tokens_a, tokens_b,\n",
    "                                           self._max_seq_length - 3)\n",
    "            self._truncate_seq_pair_epilog(tokens_a_epi, tokens_b_epi,\n",
    "                                           self._max_seq_length - 3)\n",
    "        else:\n",
    "            if len(tokens_a) > self._max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
    "            if len(tokens_a_epi) > self._max_seq_length - 2:\n",
    "                tokens_a_epi = tokens_a_epi[0:(self._max_seq_length - 2)]\n",
    "\n",
    "        vocab = self._tokenizer.vocab\n",
    "        tokens, tokens_epi = [], []\n",
    "        tokens.append(vocab.cls_token)\n",
    "        tokens_epi.append(vocab.cls_token)\n",
    "        tokens.extend(tokens_a)\n",
    "        tokens_epi.extend(tokens_a_epi)\n",
    "        tokens.append(vocab.sep_token)\n",
    "        tokens_epi.append(vocab.sep_token)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        segment_ids_epi = [0] * len(tokens_epi)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens.extend(tokens_b)\n",
    "            tokens_epi.extend(tokens_b_epi)\n",
    "            tokens.append(vocab.sep_token)\n",
    "            tokens_epi.append(vocab.sep_token)\n",
    "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
    "            segment_ids_epi.extend([1] * (len(tokens) - len(segment_ids_epi)))\n",
    "\n",
    "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids_epi = self._tokenizer.convert_tokens_to_ids(tokens_epi)\n",
    "        valid_length = len(input_ids)\n",
    "        valid_length_epi = len(input_ids_epi)\n",
    "\n",
    "        if self._pad:\n",
    "            padding_length = self._max_seq_length - valid_length\n",
    "            padding_length_epi = self._max_seq_length - valid_length_epi\n",
    "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
    "            input_ids_epi.extend([vocab[vocab.padding_token]] *\n",
    "                                 padding_length_epi)\n",
    "            segment_ids.extend([0] * padding_length)\n",
    "            segment_ids_epi.extend([0] * padding_length_epi)\n",
    "\n",
    "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
    "            np.array(segment_ids, dtype='int32'), np.array(input_ids_epi, dtype='int32'),\\\n",
    "            np.array(valid_length_epi, dtype='int32'), np.array(segment_ids_epi, dtype='int32')\n",
    "\n",
    "    def _truncate_seq_pair_prolog(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "\n",
    "    def _truncate_seq_pair_epilog(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\n",
    "        Removes from end of token list.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop(0)\n",
    "            else:\n",
    "                tokens_b.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.428025Z",
     "start_time": "2019-11-26T11:47:26.417603Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class FirstAndLastPartBERTDatasetTransform(dataset.BERTDatasetTransform):\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 max_seq_length,\n",
    "                 labels=None,\n",
    "                 pad=True,\n",
    "                 pair=True,\n",
    "                 label_dtype='float32'):\n",
    "        super(FirstAndLastPartBERTDatasetTransform,\n",
    "              self).__init__(tokenizer,\n",
    "                             max_seq_length,\n",
    "                             labels=labels,\n",
    "                             pad=pad,\n",
    "                             pair=pair,\n",
    "                             label_dtype=label_dtype)\n",
    "        self._bert_xform = FirstAndLastPartBERTSentenceTransform(\n",
    "            tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "    def __call__(self, line):\n",
    "        input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi = self._bert_xform(\n",
    "            line[:-1])\n",
    "\n",
    "        label = line[-1]\n",
    "\n",
    "        # if label is None than we are predicting unknown data\n",
    "        if label is None:\n",
    "            # early abort\n",
    "            return input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi\n",
    "            \n",
    "        if self.labels:  # for classification task\n",
    "            label = self._label_map[label]\n",
    "        label = np.array([label], dtype=self.label_dtype)\n",
    "\n",
    "        return input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.445962Z",
     "start_time": "2019-11-26T11:47:26.429329Z"
    },
    "code_folding": [
     4
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import Block\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "\n",
    "class BERTProEpiClassifier(Block):\n",
    "    \"\"\"Model for sentence (pair) classification task with BERT.\n",
    "\n",
    "    The model feeds token ids and token type ids into BERT to get the\n",
    "    pooled BERT sequence representation, then apply a Dense layer for\n",
    "    classification. Does this also for an adversarial classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bert: BERTModel\n",
    "        Bidirectional encoder with transformer.\n",
    "    num_classes : int, default is 2\n",
    "        The number of target classes.\n",
    "    dropout : float or None, default 0.0.\n",
    "        Dropout probability for the bert output.\n",
    "    prefix : str or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    params : ParameterDict or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 dropout=0.0,\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(BERTProEpiClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.classifier.add(nn.Dense(units=num_classes))\n",
    "\n",
    "    def forward(self,\n",
    "                inputs,\n",
    "                token_types,\n",
    "                valid_length=None,\n",
    "                inputs_epi=None,\n",
    "                token_types_epi=None,\n",
    "                valid_length_epi=None):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Generate the unnormalized scores for the given the input sequences.\n",
    "        From both classifiers (classifier + adversarial_classifier).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : NDArray, shape (batch_size, seq_length)\n",
    "            Input words for the sequences.\n",
    "        token_types : NDArray, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one.\n",
    "        valid_length : NDArray or None, shape (batch_size)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "        inputs_epi : NDArray or None, shape (batch_size, seq_length)\n",
    "            Input words for the sequences. If None then same as inputs.\n",
    "        token_types_epi : NDArray or None, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one. If None then same as token_types.\n",
    "        valid_length_epi : NDArray or None, shape (batch_size)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : NDArray\n",
    "            Shape (batch_size, num_classes), outputs of classifier.\n",
    "        \"\"\"\n",
    "        # if inputs_epi is None and token_types_epi is None:\n",
    "        #     inputs_epi = inputs\n",
    "        #     token_types_epi = token_types\n",
    "        #     valid_length_epi = valid_length\n",
    "\n",
    "        _, pooler_out = self.bert(inputs, token_types, valid_length)\n",
    "        _, pooler_out_epi = self.bert(inputs_epi, token_types_epi, valid_length_epi)\n",
    "        pooler_concat = mx.nd.concat(pooler_out, pooler_out_epi, dim=1)\n",
    "        return self.classifier(pooler_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.462896Z",
     "start_time": "2019-11-26T11:47:26.447799Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_bert():\n",
    "    # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "    ctx = mx.gpu(0)\n",
    "    # ctx = [mx.gpu(i) for i in range(2)]\n",
    "    # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "    # ctx = mx.cpu()\n",
    "\n",
    "    bert_base, vocabulary = nlp.model.get_model(\n",
    "        'bert_12_768_12',\n",
    "        dataset_name='book_corpus_wiki_en_uncased',\n",
    "        pretrained=True,\n",
    "        ctx=ctx,\n",
    "        use_pooler=True,\n",
    "        use_decoder=False,\n",
    "        use_classifier=False)\n",
    "    print(bert_base)\n",
    "\n",
    "    #model = BERTProEpiClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "    model = BERTProEpiClassifier(bert_base, num_classes=1, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    #loss_function = gluon.loss.SoftmaxCELoss()\n",
    "    loss_function = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "\n",
    "    # use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    # maximum sequence length\n",
    "    # max_len = 128  # + batch_size: 32\n",
    "    # 384 - 12\n",
    "    max_len = 512  # + batch_size: 6 ?\n",
    "    # the labels for the two classes\n",
    "    #all_labels = [\"0\", \"1\"]\n",
    "    all_labels = [0, 1]\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    transform = FirstAndLastPartBERTDatasetTransform(bert_tokenizer,\n",
    "                                                     max_len,\n",
    "                                                     labels=all_labels,\n",
    "                                                     label_dtype='int32',\n",
    "                                                     pad=True,\n",
    "                                                     pair=True)\n",
    "\n",
    "    return model, vocabulary, ctx, bert_tokenizer, transform, loss_function, metric, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.480110Z",
     "start_time": "2019-11-26T11:47:26.467876Z"
    },
    "code_folding": [
     0,
     6
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def transform_dataset(X, y, transform):\n",
    "    data_train_raw = MyBERTDataset(X, y)\n",
    "    data_train = data_train_raw.transform(transform)\n",
    "    return data_train_raw, data_train\n",
    "\n",
    "\n",
    "def predict_out_to_ys(all_predictions, all_labels):\n",
    "    y_true, y_pred = list(), list()\n",
    "\n",
    "    for _, y_true_many, y_pred_many in all_predictions:\n",
    "        y_true_many = y_true_many.T[0].asnumpy()\n",
    "        # https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss\n",
    "        # pred: the prediction tensor, where the batch_axis dimension ranges over batch size and axis dimension ranges over the number of classes.\n",
    "        #y_pred_many = np.argmax(y_pred_many, axis=1).asnumpy()\n",
    "        y_pred_many = y_pred_many.asnumpy()\n",
    "\n",
    "        y_true.extend(list(y_true_many))\n",
    "        y_pred.extend(list(y_pred_many))\n",
    "        # TODO: convert label_id to label?\n",
    "        # y_pred.extend(all_labels[c] for c in list(y_pred_many))\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-GPU?\n",
    "- https://gluon.mxnet.io/chapter07_distributed-learning/multiple-gpus-gluon.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.505784Z",
     "start_time": "2019-11-26T11:47:26.482927Z"
    },
    "code_folding": [
     0,
     11,
     12,
     16,
     19,
     27,
     36,
     89
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          data_train,\n",
    "          ctx,\n",
    "          metric,\n",
    "          loss_function,\n",
    "          batch_size=32,\n",
    "          lr=5e-6,\n",
    "          num_epochs=3,\n",
    "          sw=None,\n",
    "          checkpoint_dir=\"data\",\n",
    "          use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(), 'adam', {\n",
    "            'learning_rate': lr,\n",
    "            'epsilon': 1e-9\n",
    "        })\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [\n",
    "            p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "        ]\n",
    "\n",
    "    log_interval = 500\n",
    "    global_step = 0\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                    epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(\n",
    "                        checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                global_step = epoch_id * len(bert_dataloader)\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               token_ids_epi, valid_length_epi,\n",
    "                               segment_ids_epi,\n",
    "                               label) in enumerate(tqdm(bert_dataloader)):\n",
    "                    global_step += 1\n",
    "                    with mx.autograd.record():\n",
    "                        # load data to GPU\n",
    "                        token_ids = token_ids.as_in_context(ctx)\n",
    "                        valid_length = valid_length.as_in_context(ctx)\n",
    "                        segment_ids = segment_ids.as_in_context(ctx)\n",
    "                        token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "                        valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "                        segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "                        label = label.as_in_context(ctx)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = model(token_ids, segment_ids,\n",
    "                                    valid_length.astype('float32'),\n",
    "                                    token_ids_epi, segment_ids_epi,\n",
    "                                    valid_length_epi.astype('float32'))\n",
    "                        label = label.astype('float32')\n",
    "                        ls = loss_function(out, label).mean()\n",
    "\n",
    "                    # backward computation\n",
    "                    ls.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    step_loss += ls.asscalar()\n",
    "                    out = out.sigmoid().round().astype('int32')\n",
    "                    label = label.astype('int32')\n",
    "                    metric.update([label], [out])\n",
    "                    stats.append((metric.get()[1], ls.asscalar()))\n",
    "\n",
    "                    if sw:\n",
    "                        sw.add_scalar(tag='T-ls', value=ls.asscalar(), global_step=global_step)\n",
    "                        sw.add_scalar(tag='T-acc', value=metric.get()[1], global_step=global_step)\n",
    "\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(\n",
    "                                epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                step_loss / log_interval,\n",
    "                                trainer.learning_rate,\n",
    "                                metric.get()[1],\n",
    "                                datetime.timedelta(seconds=(time.time() -\n",
    "                                                            t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T20:27:24.898587Z",
     "start_time": "2019-11-17T20:27:24.868975Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train_multi(model,\n",
    "                data_train,\n",
    "                ctx,\n",
    "                metric,\n",
    "                loss_function,\n",
    "                batch_size=32,\n",
    "                lr=5e-6,\n",
    "                num_epochs=3,\n",
    "                checkpoint_dir=\"data\",\n",
    "                use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(),\n",
    "                                'adam', {\n",
    "                                    'learning_rate': lr,\n",
    "                                    'epsilon': 1e-9\n",
    "                                },\n",
    "                                update_on_kvstore=False)\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [\n",
    "            p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "        ]\n",
    "\n",
    "    log_interval = 500\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                    epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(\n",
    "                        checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               token_ids_epi, valid_length_epi,\n",
    "                               segment_ids_epi,\n",
    "                               label) in enumerate(bert_dataloader):\n",
    "                    with mx.autograd.record():\n",
    "                        # load data to GPU\n",
    "                        token_ids = gluon.utils.split_and_load(\n",
    "                            token_ids, ctx, even_split=False)\n",
    "                        valid_length = gluon.utils.split_and_load(\n",
    "                            valid_length, ctx, even_split=False)\n",
    "                        segment_ids = gluon.utils.split_and_load(\n",
    "                            segment_ids, ctx, even_split=False)\n",
    "                        token_ids_epi = gluon.utils.split_and_load(\n",
    "                            token_ids_epi, ctx, even_split=False)\n",
    "                        valid_length_epi = gluon.utils.split_and_load(\n",
    "                            valid_length_epi, ctx, even_split=False)\n",
    "                        segment_ids_epi = gluon.utils.split_and_load(\n",
    "                            segment_ids_epi, ctx, even_split=False)\n",
    "                        label = gluon.utils.split_and_load(label,\n",
    "                                                           ctx,\n",
    "                                                           even_split=False)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = [\n",
    "                            model(t1, s1, v1.astype('float32'), t2, s2,\n",
    "                                  v2.astype('float32'))\n",
    "                            for t1, s1, v1, t2, s2, v2 in zip(\n",
    "                                token_ids, segment_ids, valid_length,\n",
    "                                token_ids_epi, segment_ids_epi,\n",
    "                                valid_length_epi)\n",
    "                        ]\n",
    "                        ls = [\n",
    "                            loss_function(o, l.astype('float32')).mean()\n",
    "                            for o, l in zip(out, label)\n",
    "                        ]\n",
    "\n",
    "                    # backward computation\n",
    "                    for l in ls:\n",
    "                        l.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    for l in ls:\n",
    "                        step_loss += l.asscalar()\n",
    "                    for o, l in zip(out, label):\n",
    "                        metric.update([l.astype('int32')],\n",
    "                                      [o.sigmoid().round().astype('int32')])\n",
    "                    stats.append((metric.get()[1], [l.asscalar() for l in ls]))\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(\n",
    "                                epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                step_loss / log_interval,\n",
    "                                trainer.learning_rate,\n",
    "                                metric.get()[1],\n",
    "                                datetime.timedelta(seconds=(time.time() -\n",
    "                                                            t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.517896Z",
     "start_time": "2019-11-26T11:47:26.507230Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def predict(model, data_predict, ctx, metric, loss_function, batch_size=32, sw=None):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict,\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "    all_predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        metric.reset()\n",
    "        cum_loss = 0\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, token_ids_epi,\n",
    "                       valid_length_epi, segment_ids_epi,\n",
    "                       label) in enumerate(tqdm(bert_dataloader)):\n",
    "            global_step = batch_id\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "            valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "            segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'),\n",
    "                        token_ids_epi, segment_ids_epi,\n",
    "                        valid_length_epi.astype('float32'))\n",
    "            label = label.astype('float32')\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "            out = out.sigmoid().round().astype('int32')\n",
    "            label = label.astype('int32')\n",
    "            metric.update([label], [out])\n",
    "            cum_loss += ls.asscalar()  # .sum() ?\n",
    "\n",
    "            if sw:\n",
    "                sw.add_scalar(tag='P-ls', value=ls.asscalar(), global_step=global_step)\n",
    "                sw.add_scalar(tag='P-acc', value=metric.get()[1], global_step=global_step)\n",
    "\n",
    "            all_predictions.append((batch_id, label, out))\n",
    "\n",
    "    return all_predictions, cum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.535251Z",
     "start_time": "2019-11-26T11:47:26.519066Z"
    },
    "code_folding": [
     0,
     6
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def predict_unknown(model, data_predict, ctx, label_map=None, batch_size=32):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict,\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "    predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, token_ids_epi,\n",
    "                       valid_length_epi,\n",
    "                       segment_ids_epi) in enumerate(tqdm(bert_dataloader)):\n",
    "            global_step = batch_id\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "            valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "            segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'),\n",
    "                        token_ids_epi, segment_ids_epi,\n",
    "                        valid_length_epi.astype('float32'))\n",
    "\n",
    "            # to binary: 0/1\n",
    "            out = out.sigmoid().round().astype('int32')\n",
    "            # to numpy (not mxnet)\n",
    "            out = out.asnumpy()\n",
    "            # get mapping type\n",
    "            if label_map:\n",
    "                out = [label_map[c] for c in list(out)]\n",
    "\n",
    "            predictions.extend(out)\n",
    "\n",
    "    # list to numpy array\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.552388Z",
     "start_time": "2019-11-26T11:47:26.536747Z"
    },
    "code_folding": [
     0,
     24
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def print_infos(vocabulary, data_train_raw, data_train):\n",
    "    sample_id = 0\n",
    "\n",
    "    # sentence a\n",
    "    print(data_train_raw[sample_id][0])\n",
    "    # sentence b\n",
    "    print(data_train_raw[sample_id][1])\n",
    "    # 1 means equivalent, 0 means not equivalent\n",
    "    print(data_train_raw[sample_id][2])\n",
    "\n",
    "    print('vocabulary used for tokenization = \\n%s' % vocabulary)\n",
    "    print('[PAD] token id = %s' % (vocabulary['[PAD]']))\n",
    "    print('[CLS] token id = %s' % (vocabulary['[CLS]']))\n",
    "    print('[SEP] token id = %s' % (vocabulary['[SEP]']))\n",
    "\n",
    "    print('token ids = \\n%s' % data_train[sample_id][0])\n",
    "    print('valid length = \\n%s' % data_train[sample_id][1])\n",
    "    print('segment ids = \\n%s' % data_train[sample_id][2])\n",
    "    print('epi token ids = \\n%s' % data_train[sample_id][3])\n",
    "    print('epi valid length = \\n%s' % data_train[sample_id][4])\n",
    "    print('epi segment ids = \\n%s' % data_train[sample_id][5])\n",
    "    print('label = \\n%s' % data_train[sample_id][6])\n",
    "\n",
    "\n",
    "def plot_train_stats(stats):\n",
    "    if not stats:\n",
    "        print(\"no stats to plot\")\n",
    "        return\n",
    "\n",
    "    x = np.arange(len(stats))  # arange/linspace\n",
    "\n",
    "    acc_dots, loss_dots = zip(*stats)\n",
    "    # if isinstance(loss_dots, tuple):\n",
    "    #     loss_dots, loss_dots2 = zip(*loss_dots)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, acc_dots)  # Linie: '-', 'o-', '.-'\n",
    "    plt.title('Training BERTClassifier')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, loss_dots)\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T11:47:26.569850Z",
     "start_time": "2019-11-26T11:47:26.553816Z"
    },
    "code_folding": [
     0,
     12
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def heatconmat(y_test, y_pred):\n",
    "    sns.set_context('talk')\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred),\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                cbar=False,\n",
    "                cmap='gist_earth_r',\n",
    "                yticklabels=sorted(np.unique(y_test)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def report_training_results(y_test, y_pred, name=None, heatmap=True):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    if heatmap:\n",
    "        heatconmat(y_test, y_pred)\n",
    "    print()\n",
    "    print('Accuracy: ', round(accuracy_score(y_test, y_pred), 2), '\\n')  #\n",
    "\n",
    "    print('Report{}:'.format(\"\" if not name else \" for [{}]\".format(name)))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    f1_dic = {}\n",
    "    f1_dic['macro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='macro'), 2)\n",
    "    f1_dic['micro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='micro'), 2)\n",
    "    return f1_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T20:27:32.456352Z",
     "start_time": "2019-11-17T20:27:32.445768Z"
    },
    "code_folding": [
     3
    ]
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def load_distinct_data(name=\"within\"):\n",
    "    fn = \"data/distinct_sets/{name}/{name}_{mode}_arg_pickle.pkl\"\n",
    "    fn_train = fn.format(mode=\"train\", name=name)\n",
    "    fn_dev = fn.format(mode=\"dev\", name=name)\n",
    "\n",
    "    with open(fn_train, \"rb\") as fp:\n",
    "        train_df = pickle.load(fp)\n",
    "    with open(fn_dev, \"rb\") as fp:\n",
    "        dev_df = pickle.load(fp)\n",
    "\n",
    "    X_train = train_df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']]\n",
    "    y_train = train_df[['is_same_side']]\n",
    "    X_dev = dev_df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']]\n",
    "    y_dev = dev_df[['is_same_side']]\n",
    "    \n",
    "    return X_train, X_dev, y_train, y_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    X_train, X_dev, y_train, y_dev = load_distinct_data(\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T20:27:42.286221Z",
     "start_time": "2019-11-17T20:27:39.978704Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTModel(\n",
      "  (encoder): BERTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (1): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (2): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (3): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (4): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (5): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (6): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (7): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (8): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (9): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (10): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (11): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): HybridSequential(\n",
      "    (0): Embedding(30522 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (token_type_embed): HybridSequential(\n",
      "    (0): Embedding(2 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n",
      "Time for [2 - setup BERT model]: 0:00:02.302376\n"
     ]
    }
   ],
   "source": [
    "# 2. setup\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-17T20:27:47.592033Z",
     "start_time": "2019-11-17T20:27:46.720376Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a woman has the right to her own body. not nearly as many people have emotional ties to the baby as people do to the mother. this should make her the priority. the mother could be very hurt mentally and physically by the pregnancy. it could also keep her from taking a job she wants greatly and that would change and improve her life, it could keep her from going to school and bettering her life, it could mess up her relationships with people, and many other bad effects are likely to happen. i look forward to my opponent responses and contentions.\n",
      "abortion should be illegal in the united states.\n",
      "0\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[PAD]', '[CLS]', '[SEP]', '[MASK]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2  1037  2450  2038  1996  2157  2000  2014  2219  2303  1012  2025\n",
      "  3053  2004  2116  2111  2031  6832  7208  2000  1996  3336  2004  2111\n",
      "  2079  2000  1996  2388  1012  2023  2323  2191  2014  1996  9470  1012\n",
      "  1996  2388  2071  2022  2200  3480 10597  1998  8186  2011  1996 10032\n",
      "  1012  2009  2071  2036  2562  2014  2013  2635  1037  3105  2016  4122\n",
      "  6551  1998  2008  2052  2689  1998  5335  2014  2166  1010  2009  2071\n",
      "  2562  2014  2013  2183  2000  2082  1998  2488  2075  2014  2166  1010\n",
      "  2009  2071  6752  2039  2014  6550  2007  2111  1010  1998  2116  2060\n",
      "  2919  3896  2024  3497  2000  4148  1012  1045  2298  2830  2000  2026\n",
      "  7116 10960  1998 18974  2015  1012     3 11324  2323  2022  6206  1999\n",
      "  1996  2142  2163  1012     3     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "125\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "epi token ids = \n",
      "[    2  1037  2450  2038  1996  2157  2000  2014  2219  2303  1012  2025\n",
      "  3053  2004  2116  2111  2031  6832  7208  2000  1996  3336  2004  2111\n",
      "  2079  2000  1996  2388  1012  2023  2323  2191  2014  1996  9470  1012\n",
      "  1996  2388  2071  2022  2200  3480 10597  1998  8186  2011  1996 10032\n",
      "  1012  2009  2071  2036  2562  2014  2013  2635  1037  3105  2016  4122\n",
      "  6551  1998  2008  2052  2689  1998  5335  2014  2166  1010  2009  2071\n",
      "  2562  2014  2013  2183  2000  2082  1998  2488  2075  2014  2166  1010\n",
      "  2009  2071  6752  2039  2014  6550  2007  2111  1010  1998  2116  2060\n",
      "  2919  3896  2024  3497  2000  4148  1012  1045  2298  2830  2000  2026\n",
      "  7116 10960  1998 18974  2015  1012     3 11324  2323  2022  6206  1999\n",
      "  1996  2142  2163  1012     3     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "epi valid length = \n",
      "125\n",
      "epi segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[0]\n",
      "Time for [3 - prepare training data]: 0:00:00.866458\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    print_infos(vocabulary, data_train_raw, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir data/within_traindev_proepi512_BCE_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:32:04.683492Z",
     "start_time": "2019-11-17T20:29:54.427986Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "582d44dfb60143f3a17f7b18eec90742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16423), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [setup training]: 0:02:28.490805\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85587d72ef754898b442b023b2d59c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=8214), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-17 21:32:23,880 : INFO : successfully opened events file: data/within_traindev_proepi512_BCE_distinct/events.out.tfevents.1574022743.cuda\n",
      "2019-11-17 21:32:23,894 : INFO : wrote 1 event to disk\n",
      "2019-11-17 21:32:23,896 : INFO : wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 500/8214] loss=0.6826, lr=0.0000050, acc=0.582 - time 0:05:05.318789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-17 21:42:24,074 : INFO : wrote 1966 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 1000/8214] loss=0.6801, lr=0.0000050, acc=0.595 - time 0:05:05.601772\n",
      "[Epoch 0 Batch 1500/8214] loss=0.6421, lr=0.0000050, acc=0.618 - time 0:05:06.908283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-17 21:52:24,669 : INFO : wrote 1960 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 2000/8214] loss=0.6472, lr=0.0000050, acc=0.631 - time 0:05:05.850101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 1500/8214] loss=0.7358, lr=0.0000050, acc=0.786 - time 0:05:06.845316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-17 23:12:28,039 : INFO : wrote 1956 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 Batch 2000/8214] loss=0.7750, lr=0.0000050, acc=0.783 - time 0:05:06.885274\n",
      "[Epoch 1 Batch 2500/8214] loss=0.7602, lr=0.0000050, acc=0.781 - time 0:05:05.764616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-17 23:22:28,309 : INFO : wrote 1960 events to disk\n",
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 Batch 2000/8214] loss=0.5098, lr=0.0000050, acc=0.865 - time 0:05:06.611643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 00:42:31,217 : INFO : wrote 1960 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 Batch 2500/8214] loss=0.5090, lr=0.0000050, acc=0.865 - time 0:05:06.320312\n",
      "[Epoch 2 Batch 3000/8214] loss=0.5609, lr=0.0000050, acc=0.864 - time 0:05:06.864928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 Batch 2500/8214] loss=0.3445, lr=0.0000050, acc=0.916 - time 0:05:07.088965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 02:12:34,194 : INFO : wrote 1956 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3 Batch 3000/8214] loss=0.3966, lr=0.0000050, acc=0.914 - time 0:05:06.776996\n",
      "[Epoch 3 Batch 3500/8214] loss=0.3492, lr=0.0000050, acc=0.914 - time 0:05:06.341592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 2500/8214] loss=0.2355, lr=0.0000050, acc=0.953 - time 0:05:06.704941\n",
      "[Epoch 4 Batch 3000/8214] loss=0.2336, lr=0.0000050, acc=0.951 - time 0:05:06.322553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 03:42:37,560 : INFO : wrote 1960 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 3500/8214] loss=0.2977, lr=0.0000050, acc=0.948 - time 0:05:06.478293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_name = \"within_traindev_proepi512_BCE_distinct\"\n",
    "with Timer(\"4 - train model\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=600) as sw:\n",
    "    stats = train(model, data_train, ctx, metric, loss_function, batch_size=2, lr=5e-6, num_epochs=5, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")\n",
    "\n",
    "    plot_train_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T03:32:05.251985Z",
     "start_time": "2019-11-18T03:32:05.071148Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "civil unions are \"separate, but not equal\"\n",
      "marriage is defined as between a man and woman . president barack obama has said on multiple occassions during his political career, including the 2008 presidential election campaign: \"i believe marriage is between a man and a woman.\"[2] indeed, marriage, throughout its thousands of years of existence, has only been used to describe the union of a man and woman, toward the general end of starting a family and raising children. to change the definition to include gays would go against thousands of years of history, from which definitions are formed and should be maintained.\n",
      "1\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[PAD]', '[CLS]', '[SEP]', '[MASK]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2  2942  9209  2024  1000  3584  1010  2021  2025  5020  1000     3\n",
      "  3510  2003  4225  2004  2090  1037  2158  1998  2450  1012  2343 13857\n",
      "  8112  2038  2056  2006  3674  1051 16665 28231  2015  2076  2010  2576\n",
      "  2476  1010  2164  1996  2263  4883  2602  3049  1024  1000  1045  2903\n",
      "  3510  2003  2090  1037  2158  1998  1037  2450  1012  1000  1031  1016\n",
      "  1033  5262  1010  3510  1010  2802  2049  5190  1997  2086  1997  4598\n",
      "  1010  2038  2069  2042  2109  2000  6235  1996  2586  1997  1037  2158\n",
      "  1998  2450  1010  2646  1996  2236  2203  1997  3225  1037  2155  1998\n",
      "  6274  2336  1012  2000  2689  1996  6210  2000  2421  5637  2015  2052\n",
      "  2175  2114  5190  1997  2086  1997  2381  1010  2013  2029 15182  2024\n",
      "  2719  1998  2323  2022  5224  1012     3     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "127\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "epi token ids = \n",
      "[    2  2942  9209  2024  1000  3584  1010  2021  2025  5020  1000     3\n",
      "  3510  2003  4225  2004  2090  1037  2158  1998  2450  1012  2343 13857\n",
      "  8112  2038  2056  2006  3674  1051 16665 28231  2015  2076  2010  2576\n",
      "  2476  1010  2164  1996  2263  4883  2602  3049  1024  1000  1045  2903\n",
      "  3510  2003  2090  1037  2158  1998  1037  2450  1012  1000  1031  1016\n",
      "  1033  5262  1010  3510  1010  2802  2049  5190  1997  2086  1997  4598\n",
      "  1010  2038  2069  2042  2109  2000  6235  1996  2586  1997  1037  2158\n",
      "  1998  2450  1010  2646  1996  2236  2203  1997  3225  1037  2155  1998\n",
      "  6274  2336  1012  2000  2689  1996  6210  2000  2421  5637  2015  2052\n",
      "  2175  2114  5190  1997  2086  1997  2381  1010  2013  2029 15182  2024\n",
      "  2719  1998  2323  2022  5224  1012     3     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "epi valid length = \n",
      "127\n",
      "epi segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[1]\n",
      "Time for [5 - prepare eval data]: 0:00:00.178082\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T08:07:20.729454Z",
     "start_time": "2019-11-18T08:03:28.658543Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dddd90f7e3a4f9b9bd8549fa8520f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1598), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 09:03:28,874 : INFO : successfully opened events file: data/within_traindev_proepi512_BCE_distinct/events.out.tfevents.1574064208.cuda\n",
      "2019-11-18 09:03:28,876 : INFO : wrote 1 event to disk\n",
      "2019-11-18 09:03:28,877 : INFO : wrote 1 event to disk\n",
      "2019-11-18 09:04:28,911 : INFO : wrote 844 events to disk\n",
      "2019-11-18 09:05:28,918 : INFO : wrote 828 events to disk\n",
      "2019-11-18 09:06:29,034 : INFO : wrote 818 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [prediction]: 0:03:51.702502\n",
      "Accuracy: 0.5809076682316119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 09:07:20,726 : INFO : wrote 705 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[926 537]\n",
      " [802 930]]\n",
      "\n",
      "Accuracy:  0.58 \n",
      "\n",
      "Report for [BERTClassifier - BCE prolog+epilog distinct]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.63      0.58      1463\n",
      "           1       0.63      0.54      0.58      1732\n",
      "\n",
      "    accuracy                           0.58      3195\n",
      "   macro avg       0.58      0.58      0.58      3195\n",
      "weighted avg       0.59      0.58      0.58      3195\n",
      "\n",
      "Time for [6 - evaluate]: 0:03:52.062580\n"
     ]
    }
   ],
   "source": [
    "run_name = \"within_traindev_proepi512_BCE_distinct\"\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    # bert.model.checkpoint4.params\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - BCE prolog+epilog distinct\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)):\n",
    "        # stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=epoch_id + 1)\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=2, lr=5e-6, num_epochs=epoch_id + 1)  # seq_len: 512\n",
    "        # stats = train_multi(model, data_train, ctx, metric, loss_function, batch_size=4, lr=5e-6, num_epochs=epoch_id + 1)  # seq_len: 512\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)):\n",
    "        # all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function)\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)  # seq_len: 512\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - last part\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/bert.model.params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T08:09:27.982984Z",
     "start_time": "2019-11-18T08:09:27.720944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [1 - test/train split]: 0:00:00.256572\n"
     ]
    }
   ],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    X_train, X_dev, y_train, y_dev = load_distinct_data(\"cross\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T08:09:31.259989Z",
     "start_time": "2019-11-18T08:09:30.496507Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTModel(\n",
      "  (encoder): BERTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (1): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (2): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (3): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (4): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (5): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (6): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (7): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (8): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (9): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (10): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (11): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): HybridSequential(\n",
      "    (0): Embedding(30522 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (token_type_embed): HybridSequential(\n",
      "    (0): Embedding(2 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n",
      "Time for [2 - setup BERT model]: 0:00:00.760692\n"
     ]
    }
   ],
   "source": [
    "# 2. setup\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T08:09:41.825626Z",
     "start_time": "2019-11-18T08:09:38.797220Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are two reasons why this debate should go to the con side. reason 1: my opponent agrees with me in his last round, my opponent states that the \"number one reason does have to [do with] financial reasons.\" he cites that 42% of women who had abortions could not afford a child and were unmarried, which means the total proportion of women who had abortions that could not afford a child is likely to be at least 50% (if you factor in women who could not afford a child and were married). 38% of women who had abortions said that children would interfere with their education, and 38% of said that children would interfere with their employment. again, even assuming 50% overlap (an unrealistically high overlap) between the two groups, the proportion of women aborting due to career considerations is still over half. my opponent says that abortion should be illegal with exceptions. is the status of the majority the exception or the rule? my opponent claims that aborting for job prospects is unethical. without good career prospects, poor women doom their children to lives of economic pressure. many of these women would rather have their children unborn than have them live in poverty. as patrick henry famously said, \"is life so dear ... as to be purchased at the price of chains and slavery.\" through abortion, women are saving their children from the chains of debt and economic slavery. in a country that emphasizes rags-to-riches, the rights of women to their own future are essential and unalienable. reason 2: my opponent's sources contradict his arguments my opponent is pro-life. his sources are pro-choice. the women's center ends their article with a plea to fight for abortion rights. infoplease states that 54% of women having abortions had used contraceptives prior to getting pregnant. my opponent says that if women were scared to have a child they would not have sex, but he forgets that they could have safe sex and not be scared. it's not the woman's fault if the contraceptives don't work. the statistic from infoplease also invalidates the last paragraph that my opponent posted this round. people who use contraceptives usually realize from the beginning that they want children. conclusion: as we can see here, the question comes down to is it ethical to restrict the futures of women and their born children by forcing them to have children they cannot afford. most of the women who had abortions were sure from the beginning that they didn't want a(nother) child. they sacrifice the child they carry for the economic well-being of their born children and their future children. they are not the stereotyped teenage girls, but rational adult women. they are a significant portion of the population. therefore i urge a vote for abortion being legal with exceptions. thank you. sources i used in this debate: http://www.nbcnews.com...\n",
      "i will give my opponent a chance to respond.\n",
      "1\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[PAD]', '[CLS]', '[SEP]', '[MASK]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2  2045  2024  2048  4436  2339  2023  5981  2323  2175  2000  1996\n",
      "  9530  2217  1012  3114  1015  1024  2026  7116 10217  2007  2033  1999\n",
      "  2010  2197  2461  1010  2026  7116  2163  2008  1996  1000  2193  2028\n",
      "  3114  2515  2031  2000  1031  2079  2007  1033  3361  4436  1012  1000\n",
      "  2002 17248  2008  4413  1003  1997  2308  2040  2018 11324  2015  2071\n",
      "  2025  8984  1037  2775  1998  2020 17204  1010  2029  2965  1996  2561\n",
      " 10817  1997  2308  2040  2018 11324  2015  2008  2071  2025  8984  1037\n",
      "  2775  2003  3497  2000  2022  2012  2560  2753  1003  1006  2065  2017\n",
      "  5387  1999  2308  2040  2071  2025  8984  1037  2775  1998  2020  2496\n",
      "  1007  1012  4229  1003  1997  2308  2040  2018 11324  2015  2056  2008\n",
      "  2336  2052 15115  2007  2037  2495  1010  1998  4229  1003  1997  2056\n",
      "  2008  2336  2052 15115  2007  2037  6107  1012  2153  1010  2130 10262\n",
      "  2753  1003 17702  1006  2019  4895 22852  6553  3973  2152 17702  1007\n",
      "  2090  1996  2048  2967  1010  1996 10817  1997  2308 11113 11589  2075\n",
      "  2349  2000  2476 16852  2003  2145  2058  2431  1012  2026  7116  2758\n",
      "  2008 11324  2323  2022  6206  2007 11790  1012  2003  1996  3570  1997\n",
      "  1996  3484  1996  6453  2030  1996  3627  1029  2026  7116  4447  2008\n",
      " 11113 11589  2075  2005  3105 16746  2003 16655 23048  2389  1012  2302\n",
      "  2204  2476 16746  1010  3532  2308 12677  2037  2336  2000  3268  1997\n",
      "  3171  3778  1012  2116  1997  2122  2308  2052  2738  2031  2037  2336\n",
      "  4895 10280  2084  2031  2068  2444  1999  5635  1012  2004  4754  2888\n",
      " 18172  2056  1010  1000  2003  2166  2061  6203  1012  1012  1012  2004\n",
      "  2000  2022  4156  2012  1996  3976  1997  8859  1998  8864  1012  1000\n",
      "  2083 11324  1010  2308  2024  7494  2037  2336  2013  1996  8859  1997\n",
      "  7016  1998  3171  8864  1012  1999  1037  2406  2008 20618 26346  1011\n",
      "  2000  1011 26768  1010  1996  2916  1997  2308  2000  2037  2219  2925\n",
      "  2024  6827  1998 14477  8751 22966  1012  3114  1016  1024  2026  7116\n",
      "  1005  1055  4216 24528 29201  2010  9918  2026  7116  2003  4013  1011\n",
      "  2166  1012  2010  4216  2024  4013  1011  3601  1012  1996  2308  1005\n",
      "  1055  2415  4515  2037  3720  2007  1037 14865  2000  2954  2005 11324\n",
      "  2916  1012 18558 10814 11022  2163  2008  5139  1003  1997  2308  2383\n",
      " 11324  2015  2018  2109 24528 28687  2015  3188  2000  2893  6875  1012\n",
      "  2026  7116  2758  2008  2065  2308  2020  6015  2000  2031  1037  2775\n",
      "  2027  2052  2025  2031  3348  1010  2021  2002  5293  2015  2008  2027\n",
      "  2071  2031  3647  3348  1998  2025  2022  6015  1012  2009  1005  1055\n",
      "  2025  1996  2450  1005  1055  6346  2065  1996 24528 28687  2015  2123\n",
      "  1005  1056  2147  1012  1996 28093  6553  2013 18558 10814 11022  2036\n",
      " 19528  8520  1996  2197 20423  2008  2026  7116  6866  2023  2461  1012\n",
      "  2111  2040  2224 24528 28687  2015  2788  5382  2013  1996  2927  2008\n",
      "  2027  2215  2336  1012  7091  1024  2004  2057  2064  2156  2182  1010\n",
      "  1996  3160  3310  2091  2000  2003  2009 12962  2000 21573  1996 17795\n",
      "  1997  2308  1998  2037  2141  2336  2011  6932     3  1045  2097  2507\n",
      "  2026  7116  1037  3382  2000  6869  1012     3]\n",
      "valid length = \n",
      "512\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      "epi token ids = \n",
      "[    2 11324  2015  2056  2008  2336  2052 15115  2007  2037  2495  1010\n",
      "  1998  4229  1003  1997  2056  2008  2336  2052 15115  2007  2037  6107\n",
      "  1012  2153  1010  2130 10262  2753  1003 17702  1006  2019  4895 22852\n",
      "  6553  3973  2152 17702  1007  2090  1996  2048  2967  1010  1996 10817\n",
      "  1997  2308 11113 11589  2075  2349  2000  2476 16852  2003  2145  2058\n",
      "  2431  1012  2026  7116  2758  2008 11324  2323  2022  6206  2007 11790\n",
      "  1012  2003  1996  3570  1997  1996  3484  1996  6453  2030  1996  3627\n",
      "  1029  2026  7116  4447  2008 11113 11589  2075  2005  3105 16746  2003\n",
      " 16655 23048  2389  1012  2302  2204  2476 16746  1010  3532  2308 12677\n",
      "  2037  2336  2000  3268  1997  3171  3778  1012  2116  1997  2122  2308\n",
      "  2052  2738  2031  2037  2336  4895 10280  2084  2031  2068  2444  1999\n",
      "  5635  1012  2004  4754  2888 18172  2056  1010  1000  2003  2166  2061\n",
      "  6203  1012  1012  1012  2004  2000  2022  4156  2012  1996  3976  1997\n",
      "  8859  1998  8864  1012  1000  2083 11324  1010  2308  2024  7494  2037\n",
      "  2336  2013  1996  8859  1997  7016  1998  3171  8864  1012  1999  1037\n",
      "  2406  2008 20618 26346  1011  2000  1011 26768  1010  1996  2916  1997\n",
      "  2308  2000  2037  2219  2925  2024  6827  1998 14477  8751 22966  1012\n",
      "  3114  1016  1024  2026  7116  1005  1055  4216 24528 29201  2010  9918\n",
      "  2026  7116  2003  4013  1011  2166  1012  2010  4216  2024  4013  1011\n",
      "  3601  1012  1996  2308  1005  1055  2415  4515  2037  3720  2007  1037\n",
      " 14865  2000  2954  2005 11324  2916  1012 18558 10814 11022  2163  2008\n",
      "  5139  1003  1997  2308  2383 11324  2015  2018  2109 24528 28687  2015\n",
      "  3188  2000  2893  6875  1012  2026  7116  2758  2008  2065  2308  2020\n",
      "  6015  2000  2031  1037  2775  2027  2052  2025  2031  3348  1010  2021\n",
      "  2002  5293  2015  2008  2027  2071  2031  3647  3348  1998  2025  2022\n",
      "  6015  1012  2009  1005  1055  2025  1996  2450  1005  1055  6346  2065\n",
      "  1996 24528 28687  2015  2123  1005  1056  2147  1012  1996 28093  6553\n",
      "  2013 18558 10814 11022  2036 19528  8520  1996  2197 20423  2008  2026\n",
      "  7116  6866  2023  2461  1012  2111  2040  2224 24528 28687  2015  2788\n",
      "  5382  2013  1996  2927  2008  2027  2215  2336  1012  7091  1024  2004\n",
      "  2057  2064  2156  2182  1010  1996  3160  3310  2091  2000  2003  2009\n",
      " 12962  2000 21573  1996 17795  1997  2308  1998  2037  2141  2336  2011\n",
      "  6932  2068  2000  2031  2336  2027  3685  8984  1012  2087  1997  1996\n",
      "  2308  2040  2018 11324  2015  2020  2469  2013  1996  2927  2008  2027\n",
      "  2134  1005  1056  2215  1037  1006  2025  5886  1007  2775  1012  2027\n",
      "  8688  1996  2775  2027  4287  2005  1996  3171  2092  1011  2108  1997\n",
      "  2037  2141  2336  1998  2037  2925  2336  1012  2027  2024  2025  1996\n",
      " 12991 13874  2094  9454  3057  1010  2021 11581  4639  2308  1012  2027\n",
      "  2024  1037  3278  4664  1997  1996  2313  1012  3568  1045  9075  1037\n",
      "  3789  2005 11324  2108  3423  2007 11790  1012  4067  2017  1012  4216\n",
      "  1045  2109  1999  2023  5981  1024  8299  1024  1013  1013  7479  1012\n",
      "  6788  2638  9333  1012  4012  1012  1012  1012     3  1045  2097  2507\n",
      "  2026  7116  1037  3382  2000  6869  1012     3]\n",
      "epi valid length = \n",
      "512\n",
      "epi segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1]\n",
      "label = \n",
      "[1]\n",
      "Time for [3 - prepare training data]: 0:00:03.022960\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    print_infos(vocabulary, data_train_raw, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-18T08:10:34.496280Z",
     "start_time": "2019-11-18T08:10:34.282109Z"
    }
   },
   "outputs": [],
   "source": [
    "! mkdir data/cross_traindev_proepi512_BCE_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-18T08:11:46.678Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c647718ba55b4674af7c01a095baf79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57825), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [setup training]: 0:04:03.337669\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569fc32f6bcd4bb3b7707c7a502a393e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28917), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 09:15:47,631 : INFO : successfully opened events file: data/cross_traindev_proepi512_BCE_distinct/events.out.tfevents.1574064947.cuda\n",
      "2019-11-18 09:15:47,632 : INFO : wrote 1 event to disk\n",
      "2019-11-18 09:15:47,633 : INFO : wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 500/28917] loss=0.6921, lr=0.0000050, acc=0.554 - time 0:05:01.647055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 09:25:48,133 : INFO : wrote 1990 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 1000/28917] loss=0.6576, lr=0.0000050, acc=0.575 - time 0:05:02.018792\n",
      "[Epoch 0 Batch 1500/28917] loss=0.6490, lr=0.0000050, acc=0.583 - time 0:05:01.984879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 09:35:48,195 : INFO : wrote 1986 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 2000/28917] loss=0.6282, lr=0.0000050, acc=0.594 - time 0:05:02.313565\n",
      "[Epoch 0 Batch 2500/28917] loss=0.6138, lr=0.0000050, acc=0.610 - time 0:05:01.667695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 09:45:48,493 : INFO : wrote 1988 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 3000/28917] loss=0.5573, lr=0.0000050, acc=0.625 - time 0:05:02.157616\n",
      "[Epoch 0 Batch 3500/28917] loss=0.5889, lr=0.0000050, acc=0.634 - time 0:05:01.801193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 09:55:48,873 : INFO : wrote 1988 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 4000/28917] loss=0.5268, lr=0.0000050, acc=0.646 - time 0:05:02.223681\n",
      "[Epoch 0 Batch 4500/28917] loss=0.4083, lr=0.0000050, acc=0.665 - time 0:05:01.643777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 10:05:49,146 : INFO : wrote 1884 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 5000/28917] loss=0.4636, lr=0.0000050, acc=0.679 - time 0:05:42.928002\n",
      "[Epoch 0 Batch 5500/28917] loss=0.4345, lr=0.0000050, acc=0.691 - time 0:05:59.247858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 10:15:49,305 : INFO : wrote 1672 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 6000/28917] loss=0.3812, lr=0.0000050, acc=0.702 - time 0:05:58.198266\n",
      "[Epoch 0 Batch 6500/28917] loss=0.4257, lr=0.0000050, acc=0.711 - time 0:05:57.313529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 10:25:49,937 : INFO : wrote 1680 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 7000/28917] loss=0.4365, lr=0.0000050, acc=0.721 - time 0:05:49.328552\n",
      "[Epoch 0 Batch 7500/28917] loss=0.4485, lr=0.0000050, acc=0.728 - time 0:05:04.167083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 10:35:50,107 : INFO : wrote 1858 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 8000/28917] loss=0.3939, lr=0.0000050, acc=0.737 - time 0:05:28.540205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 10:45:50,777 : INFO : wrote 1750 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 8500/28917] loss=0.4308, lr=0.0000050, acc=0.743 - time 0:05:59.322339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 10:55:50,799 : INFO : wrote 1672 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 9500/28917] loss=0.3783, lr=0.0000050, acc=0.754 - time 0:05:58.545815\n",
      "[Epoch 0 Batch 10000/28917] loss=0.4349, lr=0.0000050, acc=0.759 - time 0:05:57.418344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 11:05:51,269 : INFO : wrote 1686 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 10500/28917] loss=0.4083, lr=0.0000050, acc=0.763 - time 0:05:41.395134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 11:15:51,656 : INFO : wrote 1710 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 11000/28917] loss=0.4304, lr=0.0000050, acc=0.767 - time 0:06:00.731662\n",
      "[Epoch 0 Batch 11500/28917] loss=0.3853, lr=0.0000050, acc=0.772 - time 0:05:49.938684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 11:25:52,282 : INFO : wrote 1698 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 12000/28917] loss=0.4510, lr=0.0000050, acc=0.774 - time 0:05:54.123925\n",
      "[Epoch 0 Batch 12500/28917] loss=0.4197, lr=0.0000050, acc=0.777 - time 0:05:54.742216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 11:35:52,574 : INFO : wrote 1700 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 13000/28917] loss=0.3395, lr=0.0000050, acc=0.781 - time 0:05:56.874768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 11:45:52,971 : INFO : wrote 1682 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 13500/28917] loss=0.4099, lr=0.0000050, acc=0.784 - time 0:05:56.543339\n",
      "[Epoch 0 Batch 14000/28917] loss=0.3886, lr=0.0000050, acc=0.786 - time 0:05:56.715541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 11:55:53,306 : INFO : wrote 1682 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 14500/28917] loss=0.4130, lr=0.0000050, acc=0.789 - time 0:05:56.676666\n",
      "[Epoch 0 Batch 15000/28917] loss=0.4312, lr=0.0000050, acc=0.792 - time 0:05:55.736456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 12:05:53,982 : INFO : wrote 1688 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 15500/28917] loss=0.4330, lr=0.0000050, acc=0.793 - time 0:05:57.560315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 12:15:54,653 : INFO : wrote 1684 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 16000/28917] loss=0.3960, lr=0.0000050, acc=0.795 - time 0:05:55.843479\n",
      "[Epoch 0 Batch 16500/28917] loss=0.4177, lr=0.0000050, acc=0.797 - time 0:05:55.743137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 12:25:55,178 : INFO : wrote 1688 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 17000/28917] loss=0.3789, lr=0.0000050, acc=0.799 - time 0:05:55.644738\n",
      "[Epoch 0 Batch 17500/28917] loss=0.3713, lr=0.0000050, acc=0.801 - time 0:05:56.076135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 12:35:55,228 : INFO : wrote 1686 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 18000/28917] loss=0.4280, lr=0.0000050, acc=0.803 - time 0:05:55.708282\n",
      "[Epoch 0 Batch 18500/28917] loss=0.4082, lr=0.0000050, acc=0.804 - time 0:05:57.560422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-18 12:45:55,797 : INFO : wrote 1684 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 19000/28917] loss=0.3727, lr=0.0000050, acc=0.806 - time 0:05:55.231795\n"
     ]
    }
   ],
   "source": [
    "run_name = \"cross_traindev_proepi512_BCE_distinct\"\n",
    "with Timer(\"4 - train model\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=600) as sw:\n",
    "    stats = train(model, data_train, ctx, metric, loss_function, batch_size=2, lr=5e-6, num_epochs=3, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")\n",
    "\n",
    "    plot_train_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T14:52:16.423814Z",
     "start_time": "2019-11-19T14:52:16.180808Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abortion is not morally wrong until the fetus can survive outside the mother's body i agree to bossyburrito's rules, terms, and first definition. *internet high fives bossyburrito* just one quick note, the second definition, on the left side (i'm assuming) should read as \"viability,\" which is defined by the definition he gave. now, he gave the definition as \"the point at which it is not virtually guaranteed (i.e. with 99% probability) that the fetus will die outside the mother's body.\" however, the point of viability being at about 22 to 24 weeks, the child simply has a \"better than average\" chance of surviving outside the mother's body, with chances increasing as the pregnancy continues. so it would probably help if bossyburrito was more specific about how viable the unborn must be before it should be considered wrong to kill him/her.\n",
      "abortion is not morally wrong until the fetus can survive outside the mother's body resolution: abortion is not morally wrong until the fetus can survive outside the mother's body. rules: (1) debater must have typing experience and internet access. (2) place your arguments and sources inside the debate (3) structure the debate in a readable, coherent fashion. (4) no semantics, trolling, or lawyering. rounds: (1) acceptance + internet high five (2) main argument (3) rebuttal to opponent's main argument (4) response to rebuttal + closing arguments + voting issues (one paragraph) definitions abortion: terminating a pregnancy through the death and removal of the fetus. ability to survive outside the mother's body: being able to live after being removed from the mother; the point at which it is not virtually guaranteed (i.e with ~99 % probability) that the fetus will die outside the mother's body. {opening round format taken with permission from wallstreetatheist}\n",
      "0\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[PAD]', '[CLS]', '[SEP]', '[MASK]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2 11324  2003  2025 28980  3308  2127  1996 10768  5809  2064  5788\n",
      "  2648  1996  2388  1005  1055  2303  1045  5993  2000  5795  2100  8569\n",
      " 18752  3406  1005  1055  3513  1010  3408  1010  1998  2034  6210  1012\n",
      "  1008  4274  2152  2274  2015  5795  2100  8569 18752  3406  1008  2074\n",
      "  2028  4248  3602  1010  1996  2117  6210  1010  2006  1996  2187  2217\n",
      "  1006  1045  1005  1049 10262  1007  2323  3191  2004  1000  3081  8553\n",
      "  1010  1000  2029  2003  4225  2011  1996  6210  2002  2435  1012  2085\n",
      "  1010  2002  2435  1996  6210  2004  1000  1996  2391  2012  2029  2009\n",
      "  2003  2025  8990 12361  1006  1045  1012  1041  1012  2007  5585  1003\n",
      "  9723  1007  2008  1996 10768  5809  2097  3280  2648  1996  2388  1005\n",
      "  1055  2303  1012  1000  2174  1010  1996  2391  1997  3081  8553  2108\n",
      "  2012  2055  2570  2000  2484  3134  1010  1996  2775  3432  2038  1037\n",
      "  1000  2488  2084  2779  1000  3382  1997  6405  2648  1996  2388  1005\n",
      "  1055  2303  1010  2007  9592  4852  2004  1996 10032  4247  1012  2061\n",
      "  2009  2052  2763  2393  2065  5795  2100  8569 18752  3406  2001  2062\n",
      "  3563  2055  2129 14874  1996  4895 10280  2442  2022  2077  2009  2323\n",
      "  2022  2641  3308  2000  3102  2032  1013  2014  1012     3 11324  2003\n",
      "  2025 28980  3308  2127  1996 10768  5809  2064  5788  2648  1996  2388\n",
      "  1005  1055  2303  5813  1024 11324  2003  2025 28980  3308  2127  1996\n",
      " 10768  5809  2064  5788  2648  1996  2388  1005  1055  2303  1012  3513\n",
      "  1024  1006  1015  1007  5981  2099  2442  2031 22868  3325  1998  4274\n",
      "  3229  1012  1006  1016  1007  2173  2115  9918  1998  4216  2503  1996\n",
      "  5981  1006  1017  1007  3252  1996  5981  1999  1037  3191  3085  1010\n",
      " 18920  4827  1012  1006  1018  1007  2053 28081  1010 18792  2075  1010\n",
      "  2030  5160  2075  1012  6241  1024  1006  1015  1007  9920  1009  4274\n",
      "  2152  2274  1006  1016  1007  2364  6685  1006  1017  1007  2128  8569\n",
      " 28200  2000  7116  1005  1055  2364  6685  1006  1018  1007  3433  2000\n",
      "  2128  8569 28200  1009  5494  9918  1009  6830  3314  1006  2028 20423\n",
      "  1007 15182 11324  1024 23552  1037 10032  2083  1996  2331  1998  8208\n",
      "  1997  1996 10768  5809  1012  3754  2000  5788  2648  1996  2388  1005\n",
      "  1055  2303  1024  2108  2583  2000  2444  2044  2108  3718  2013  1996\n",
      "  2388  1025  1996  2391  2012  2029  2009  2003  2025  8990 12361  1006\n",
      "  1045  1012  1041  2007  1066  5585  1003  9723  1007  2008  1996 10768\n",
      "  5809  2097  3280  2648  1996  2388  1005  1055  2303  1012  1063  3098\n",
      "  2461  4289  2579  2007  6656  2013  3681 13334 29336 26036  3367  1065\n",
      "     3     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "421\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "epi token ids = \n",
      "[    2 11324  2003  2025 28980  3308  2127  1996 10768  5809  2064  5788\n",
      "  2648  1996  2388  1005  1055  2303  1045  5993  2000  5795  2100  8569\n",
      " 18752  3406  1005  1055  3513  1010  3408  1010  1998  2034  6210  1012\n",
      "  1008  4274  2152  2274  2015  5795  2100  8569 18752  3406  1008  2074\n",
      "  2028  4248  3602  1010  1996  2117  6210  1010  2006  1996  2187  2217\n",
      "  1006  1045  1005  1049 10262  1007  2323  3191  2004  1000  3081  8553\n",
      "  1010  1000  2029  2003  4225  2011  1996  6210  2002  2435  1012  2085\n",
      "  1010  2002  2435  1996  6210  2004  1000  1996  2391  2012  2029  2009\n",
      "  2003  2025  8990 12361  1006  1045  1012  1041  1012  2007  5585  1003\n",
      "  9723  1007  2008  1996 10768  5809  2097  3280  2648  1996  2388  1005\n",
      "  1055  2303  1012  1000  2174  1010  1996  2391  1997  3081  8553  2108\n",
      "  2012  2055  2570  2000  2484  3134  1010  1996  2775  3432  2038  1037\n",
      "  1000  2488  2084  2779  1000  3382  1997  6405  2648  1996  2388  1005\n",
      "  1055  2303  1010  2007  9592  4852  2004  1996 10032  4247  1012  2061\n",
      "  2009  2052  2763  2393  2065  5795  2100  8569 18752  3406  2001  2062\n",
      "  3563  2055  2129 14874  1996  4895 10280  2442  2022  2077  2009  2323\n",
      "  2022  2641  3308  2000  3102  2032  1013  2014  1012     3 11324  2003\n",
      "  2025 28980  3308  2127  1996 10768  5809  2064  5788  2648  1996  2388\n",
      "  1005  1055  2303  5813  1024 11324  2003  2025 28980  3308  2127  1996\n",
      " 10768  5809  2064  5788  2648  1996  2388  1005  1055  2303  1012  3513\n",
      "  1024  1006  1015  1007  5981  2099  2442  2031 22868  3325  1998  4274\n",
      "  3229  1012  1006  1016  1007  2173  2115  9918  1998  4216  2503  1996\n",
      "  5981  1006  1017  1007  3252  1996  5981  1999  1037  3191  3085  1010\n",
      " 18920  4827  1012  1006  1018  1007  2053 28081  1010 18792  2075  1010\n",
      "  2030  5160  2075  1012  6241  1024  1006  1015  1007  9920  1009  4274\n",
      "  2152  2274  1006  1016  1007  2364  6685  1006  1017  1007  2128  8569\n",
      " 28200  2000  7116  1005  1055  2364  6685  1006  1018  1007  3433  2000\n",
      "  2128  8569 28200  1009  5494  9918  1009  6830  3314  1006  2028 20423\n",
      "  1007 15182 11324  1024 23552  1037 10032  2083  1996  2331  1998  8208\n",
      "  1997  1996 10768  5809  1012  3754  2000  5788  2648  1996  2388  1005\n",
      "  1055  2303  1024  2108  2583  2000  2444  2044  2108  3718  2013  1996\n",
      "  2388  1025  1996  2391  2012  2029  2009  2003  2025  8990 12361  1006\n",
      "  1045  1012  1041  2007  1066  5585  1003  9723  1007  2008  1996 10768\n",
      "  5809  2097  3280  2648  1996  2388  1005  1055  2303  1012  1063  3098\n",
      "  2461  4289  2579  2007  6656  2013  3681 13334 29336 26036  3367  1065\n",
      "     3     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "epi valid length = \n",
      "421\n",
      "epi segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[0]\n",
      "Time for [5 - prepare eval data]: 0:00:00.237505\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T14:57:14.685068Z",
     "start_time": "2019-11-19T14:52:44.927968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439a6dd1275b464eadfba30fea207acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1598), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-19 15:52:45,113 : INFO : successfully opened events file: data/cross_traindev_proepi512_BCE_distinct/events.out.tfevents.1574175165.cuda\n",
      "2019-11-19 15:52:45,163 : INFO : wrote 1 event to disk\n",
      "2019-11-19 15:52:45,164 : INFO : wrote 1 event to disk\n",
      "2019-11-19 15:53:45,228 : INFO : wrote 738 events to disk\n",
      "2019-11-19 15:54:45,326 : INFO : wrote 690 events to disk\n",
      "2019-11-19 15:55:45,359 : INFO : wrote 730 events to disk\n",
      "2019-11-19 15:56:45,394 : INFO : wrote 720 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [prediction]: 0:04:29.418066\n",
      "Accuracy: 0.7305164319248826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-19 15:57:14,681 : INFO : wrote 317 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[1384  451]\n",
      " [ 410  950]]\n",
      "\n",
      "Accuracy:  0.73 \n",
      "\n",
      "Report for [BERTClassifier - (cross) BCE prolog+epilog distinct]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76      1835\n",
      "           1       0.68      0.70      0.69      1360\n",
      "\n",
      "    accuracy                           0.73      3195\n",
      "   macro avg       0.72      0.73      0.73      3195\n",
      "weighted avg       0.73      0.73      0.73      3195\n",
      "\n",
      "Time for [6 - evaluate]: 0:04:29.747870\n"
     ]
    }
   ],
   "source": [
    "run_name = \"cross_traindev_proepi512_BCE_distinct\"\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    # bert.model.checkpoint4.params\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - (cross) BCE prolog+epilog distinct\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with Timer(\"4 - train model\"):\n",
    "    # train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=3)\n",
    "    stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=2)\n",
    "    # model.save_parameters(\"data/same-side-classification/cross-topic/bert.model.params\")\n",
    "    model.save_parameters(\"data/bert.model.params\")\n",
    "\n",
    "    plot_train_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with Timer(\"6 - evaluate\"):\n",
    "    # model.load_parameters(\"data/same-side-classification/cross-topic/bert.model.params\", ctx=ctx)\n",
    "    #model.load_parameters(\"data/bert.model.params\", ctx=ctx)\n",
    "    # load model from \"within\" to evaluate with \"cross\" test-data\n",
    "    #model.load_parameters('data/within_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)\n",
    "    # model.load_parameters('data/cross_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier within->cross 0.1 split\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     14,
     22
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)):\n",
    "        stats = train(model,\n",
    "                      data_train,\n",
    "                      ctx,\n",
    "                      metric,\n",
    "                      loss_function,\n",
    "                      batch_size=2,\n",
    "                      lr=5e-6,\n",
    "                      num_epochs=epoch_id + 1,\n",
    "                      checkpoint_dir='data/cross_traindev_proepi512_BCE')\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)):\n",
    "        all_predictions, cum_loss = predict(model,\n",
    "                                            data_dev,\n",
    "                                            ctx,\n",
    "                                            metric,\n",
    "                                            loss_function,\n",
    "                                            batch_size=2)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true,\n",
    "                                y_pred,\n",
    "                                name=\"BERTClassifier\",\n",
    "                                heatmap=False)\n",
    "\n",
    "    model.save_parameters(\n",
    "        \"data/cross_traindev_proepi512_BCE/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"11 - test/train split\"):\n",
    "    # evaluate on \"within\" test-data\n",
    "    _, X_dev, _, y_dev = get_train_test_sets(within_traindev_df)\n",
    "\n",
    "with Timer(\"12 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)\n",
    "\n",
    "with Timer(\"13 - evaluate\"):\n",
    "    # model from \"cross\"\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier cross with within\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Cross-Model with Within-Test\n",
    "\n",
    "5 epochs of cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_parameters('data/cross_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(within_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "report_training_results(y_true, y_pred, name=\"BERTClassifier cross with within\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:24:48.940295\n",
    "Accuracy: 0.8536330916488446\n",
    "Confusion Matrix:\n",
    "[[7659 1174]\n",
    " [1632 8706]]\n",
    "\n",
    "Accuracy:  0.85 \n",
    "\n",
    "Report for [BERTClassifier cross with within]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.82      0.87      0.85      8833\n",
    "           1       0.88      0.84      0.86     10338\n",
    "\n",
    "    accuracy                           0.85     19171\n",
    "   macro avg       0.85      0.85      0.85     19171\n",
    "weighted avg       0.85      0.85      0.85     19171\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Within-Model with Cross-Test\n",
    "\n",
    "5 epochs of within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_parameters('data/within_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(cross_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"evaluate within with cross\"):\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier within with cross\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:22:17.542674\n",
    "Accuracy: 0.9379197379197379\n",
    "Confusion Matrix:\n",
    "[[8397  539]\n",
    " [ 598 8781]]\n",
    "\n",
    "Accuracy:  0.94 \n",
    "\n",
    "Report for [BERTClassifier]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.93      0.94      0.94      8936\n",
    "           1       0.94      0.94      0.94      9379\n",
    "\n",
    "    accuracy                           0.94     18315\n",
    "   macro avg       0.94      0.94      0.94     18315\n",
    "weighted avg       0.94      0.94      0.94     18315\n",
    "\n",
    "Time for [6 - evaluate]: 0:22:19.841677\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Within-Model with Within-Test\n",
    "\n",
    "5 epochs of within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_parameters('data/within_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(within_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"evaluate within with within\"):\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier within with within\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:19:51.733113\n",
    "Accuracy: 0.9069427781545042\n",
    "Confusion Matrix:\n",
    "[[7972  861]\n",
    " [ 923 9415]]\n",
    "\n",
    "Accuracy:  0.91 \n",
    "\n",
    "Report for [BERTClassifier within with within]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      0.90      0.90      8833\n",
    "           1       0.92      0.91      0.91     10338\n",
    "\n",
    "    accuracy                           0.91     19171\n",
    "   macro avg       0.91      0.91      0.91     19171\n",
    "weighted avg       0.91      0.91      0.91     19171\n",
    "\n",
    "Time for [evaluate within with cross]: 0:19:52.352049\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Cross-Model with Cross-Test\n",
    "\n",
    "5 epochs of cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_parameters('data/cross_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(cross_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "report_training_results(y_true, y_pred, name=\"BERTClassifier cross\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:23:28.845010\n",
    "Accuracy: 0.9197925197925197\n",
    "Confusion Matrix:\n",
    "[[8329  607]\n",
    " [ 862 8517]]\n",
    "\n",
    "Accuracy:  0.92 \n",
    "\n",
    "Report for [BERTClassifier cross]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.91      0.93      0.92      8936\n",
    "           1       0.93      0.91      0.92      9379\n",
    "\n",
    "    accuracy                           0.92     18315\n",
    "   macro avg       0.92      0.92      0.92     18315\n",
    "weighted avg       0.92      0.92      0.92     18315\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details to wrong classified arguments\n",
    "\n",
    "within_traindev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()\n",
    "\n",
    "model.load_parameters('data/within_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(within_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "# print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "report_training_results(y_true, y_pred, name=\"BERTClassifier within-within\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to dataframe\n",
    "dev_pred_df = pd.DataFrame(data=y_pred, columns=[\"prediction\"], dtype=\"bool\")\n",
    "\n",
    "# merge all dataframes\n",
    "dev_df = X_dev.join(y_dev)\n",
    "dev_df = dev_df.reset_index()\n",
    "dev_df = pd.merge(dev_df, dev_pred_df, left_index=True, right_index=True, how='inner')\n",
    "dev_df.set_index('id', inplace=True)\n",
    "\n",
    "# re-apply tag value\n",
    "dev_df = dev_df.progress_apply(add_tag, axis=1)\n",
    "# info\n",
    "dev_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "dev_df_ser_file = \"data/within_traindev_proepi512_BCE/eval_dev_df.pickle\"\n",
    "\n",
    "\n",
    "with open(dev_df_ser_file, \"wb\") as f:\n",
    "    pickle.dump(dev_df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(dev_df_ser_file, \"rb\") as f:\n",
    "    dev_df = pickle.load(f)\n",
    "\n",
    "\n",
    "dev_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPFN_df = dev_df[(dev_df['is_same_side'] != dev_df['prediction'])]  #  and (dev_df['tag'] != 'abortion')\n",
    "FPFN_df.info()\n",
    "FPFN_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import re\n",
    "#import tabulate\n",
    "#display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "\n",
    "\n",
    "def print_args(df, idx, add_linebreaks=True):\n",
    "    row = df.iloc[idx]\n",
    "    print('IDX: {}, tag: {}, topics: {}'.format(idx, row['tag'], row['topic']))\n",
    "    print('Is-Same-Side: {}'.format(row['is_same_side']))\n",
    "\n",
    "    arg1 = row['argument1']\n",
    "    arg2 = row['argument2']\n",
    "    if add_linebreaks:\n",
    "        pat = re.compile(r'(?P<c>(\\.|\\?|\\!|\\:)+\\\"?)')\n",
    "        arg1 = pat.sub(r'\\1<br/>', arg1)\n",
    "        arg2 = pat.sub(r'\\1<br/>', arg2)\n",
    "\n",
    "    display(HTML('''<table>\n",
    "        <tr>\n",
    "            <td style=\"border-right:1px dashed black;\">{arg1}</td>\n",
    "            <td>{arg2}</td>\n",
    "        </tr>\n",
    "    </table>'''.format(arg1=arg1, arg2=arg2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = {print_args(FPFN_df, i) for i in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# tokenizer from BERT\n",
    "def tokenize_arguments(row):\n",
    "    # tokenize\n",
    "    row['argument1_tokens'] = tokenizer(row['argument1'])\n",
    "    row['argument2_tokens'] = tokenizer(row['argument2'])\n",
    "\n",
    "    # count tokens\n",
    "    row['argument1_len'] = len(row['argument1_tokens'])\n",
    "    row['argument2_len'] = len(row['argument2_tokens'])\n",
    "    # token number diff\n",
    "    row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "    row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff'])\n",
    "    return row\n",
    "\n",
    "\n",
    "FPFN_df = FPFN_df.progress_apply(tokenize_arguments, axis=1)\n",
    "FPFN_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPFN_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make final results/predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_parameters('data/within_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)\n",
    "# model.load_parameters('data/cross_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)\n",
    "model.load_parameters('data/within_traindev_proepi512_BCE_0.1/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_pred = within_test_df[['argument1', 'argument2', 'topic']]\n",
    "#X_pred = cross_test_df[['argument1', 'argument2', 'topic']]\n",
    "X_pred = new_within_test_df[['argument1', 'argument2', 'topic']]\n",
    "y_pred = None\n",
    "\n",
    "data_pred_raw, data_pred = transform_dataset(X_pred, y_pred, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# data_pred_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# label_map=all_labels\n",
    "predictions = predict_unknown(model, data_pred, ctx, label_map=None, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data_pred) == len(predictions) == len(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to dataframe\n",
    "# bool works because we mapped 0 to False, 1 to True, is default conversion\n",
    "test_pred_df = pd.DataFrame(data=predictions, columns=[\"prediction\"], dtype=\"bool\")\n",
    "\n",
    "# merge all dataframes\n",
    "# test_df = X_pred.join(y_pred)\n",
    "test_df = X_pred.reset_index()\n",
    "test_df = pd.merge(test_df, test_pred_df, left_index=True, right_index=True, how='inner')\n",
    "test_df.set_index('id', inplace=True)\n",
    "\n",
    "# re-apply tag value\n",
    "test_df = test_df.progress_apply(add_tag, axis=1)\n",
    "# info\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# ser_fn = \"data/within_traindev_proepi512_BCE/within_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/cross_traindev_proepi512_BCE/cross_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/cross_traindev_proepi512_BCE/within_with_cross_model_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/within_traindev_proepi512_BCE/cross_with_within_model_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/within_traindev_proepi512_BCE_0.1/within_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/within_traindev_proepi512_BCE_0.1/cross_with_within_model_test_pred_df.pickle\"\n",
    "ser_fn = \"data/within_traindev_proepi512_BCE_0.1/new_within_test_pred_df.pickle\"\n",
    "\n",
    "with open(ser_fn, \"wb\") as f:\n",
    "    pickle.dump(test_df, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(test_df.itertuples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_fn = \"data/within_traindev_proepi512_BCE/within_results.csv\"\n",
    "# res_fn = \"data/cross_traindev_proepi512_BCE/cross_results.csv\"\n",
    "# res_fn = \"data/cross_traindev_proepi512_BCE/within_with_cross_model_results.csv\"\n",
    "# res_fn = \"data/within_traindev_proepi512_BCE/cross_with_within_model_results.csv\"\n",
    "# res_fn = \"data/within_traindev_proepi512_BCE_0.1/within_results.csv\"\n",
    "# res_fn = \"data/within_traindev_proepi512_BCE_0.1/cross_with_within_model_results.csv\"\n",
    "res_fn = \"data/within_traindev_proepi512_BCE_0.1/new_within_results.csv\"\n",
    "\n",
    "with open(res_fn, \"w\") as of:\n",
    "    of.write('\"id\",\"label\"\\n')\n",
    "    for row_id, row in test_df.iterrows():\n",
    "        of.write('{},\"{}\"\\n'.format(row_id, str(row['prediction'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd data/within_traindev_proepi512_BCE_0.1/\n",
    "cp cross_with_within_model_results.csv cross.csv\n",
    "cp within_results.csv within.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd data/within_traindev_proepi512_BCE_0.1/\n",
    "gzip cross.csv\n",
    "gzip within.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd data/within_traindev_proepi512_BCE_0.1/\n",
    "gzip new_within_results.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: do this for within and cross !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test read\n",
    "# temp_test_df = pd.read_csv(\"data/within_traindev_proepi512_BCE_0.1/cross_with_within_model_results.csv\", index_col='id')\n",
    "temp_test_df = pd.read_csv(\"data/within_traindev_proepi512_BCE_0.1/new_within_results.csv\", index_col='id')\n",
    "temp_test_df.info()\n",
    "temp_test_df.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
