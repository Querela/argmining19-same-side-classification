{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:52:42.021856Z",
     "start_time": "2020-05-25T15:52:42.018182Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:52:44.273913Z",
     "start_time": "2020-05-25T15:52:43.295731Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils_data import configure_logging\n",
    "configure_logging()\n",
    "\n",
    "from utils_data import init_random\n",
    "init_random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:52:56.852350Z",
     "start_time": "2020-05-25T15:52:56.810110Z"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "from bert import *\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertModel, BertConfig\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:52:57.622931Z",
     "start_time": "2020-05-25T15:52:57.616748Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils_gluon import setup_bert_epi128bce, setup_bert_epi512bce\n",
    "from utils_gluon import setup_bert_pro128bce, setup_bert_pro512bce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MXNet/GluonNLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:53:00.483447Z",
     "start_time": "2020-05-25T15:53:00.478939Z"
    }
   },
   "outputs": [],
   "source": [
    "run_name = \"yelp_1_pro128BCE\"\n",
    "fn_run_path = Path(f\"data/{run_name}\")\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T15:53:02.466787Z",
     "start_time": "2020-05-25T15:53:01.654550Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build BERT same-side classifier model ...\n"
     ]
    }
   ],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert_pro128bce(gpu=None)\n",
    "print(f\"Build BERT same-side classifier model ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # ...\n",
    "    bert_base, vocabulary = nlp.model.get_model(\n",
    "        'bert_12_768_12',\n",
    "        dataset_name='book_corpus_wiki_en_uncased',\n",
    "        pretrained=True,\n",
    "        ctx=ctx,\n",
    "        use_pooler=True,\n",
    "        use_decoder=False,\n",
    "        use_classifier=False)\n",
    "    # ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T17:59:19.032144Z",
     "start_time": "2020-05-25T17:59:18.934939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data/yelp_1_pro128BCE/bert.model.checkpoint2.params ...\n"
     ]
    }
   ],
   "source": [
    "fn_model_state = fn_run_path / f\"bert.model.checkpoint{num_epochs - 1}.params\"\n",
    "print(f\"Load {fn_model_state} ...\")\n",
    "model.load_parameters(str(fn_model_state), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load generic PyTorch BERT model (transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T17:59:27.697063Z",
     "start_time": "2020-05-25T17:59:24.301842Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0525 19:59:24.888704 140389655271232 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ekoerner/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0525 19:59:24.890974 140389655271232 configuration_utils.py:168] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0525 19:59:25.350588 140389655271232 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ekoerner/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0525 19:59:27.694270 140389655271232 modeling_utils.py:405] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0525 19:59:27.694942 140389655271232 modeling_utils.py:408] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "# dev = torch.device(\"cpu\")\n",
    "pytorch_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T18:00:35.595029Z",
     "start_time": "2020-05-25T18:00:30.718030Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0525 20:00:30.726449 140389655271232 configuration_utils.py:71] Configuration saved in temp/config.json\n",
      "I0525 20:00:33.205898 140389655271232 modeling_utils.py:205] Model weights saved in temp/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "dir_name = Path(\"./temp\")\n",
    "if not dir_name.exists():\n",
    "    dir_name.mkdir()\n",
    "    \n",
    "fn_model_params = dir_name / \"pytorch_model.bin\"\n",
    "fn_model_bert_params = dir_name / \"pytorch_model_bert.bin\"\n",
    "fn_model_classifier_params = dir_name / \"pytorch_model_classifier.bin\"\n",
    "\n",
    "# just the BERT part\n",
    "#pytorch_model = pytorch_model.bert\n",
    "pytorch_model.save_pretrained(str(dir_name))\n",
    "pytorch_params = torch.load(str(fn_model_params))\n",
    "\n",
    "# save only BERT part\n",
    "torch.save(pytorch_model.bert.state_dict(), str(fn_model_bert_params))\n",
    "pytorch_params = torch.load(str(fn_model_bert_params))\n",
    "\n",
    "# save classifier part\n",
    "torch.save(pytorch_model.classifier.state_dict(), str(fn_model_classifier_params))\n",
    "pytorch_classifier_params = torch.load(str(fn_model_classifier_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy params from GluonNLP to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T17:59:43.490204Z",
     "start_time": "2020-05-25T17:59:43.487198Z"
    },
    "code_folding": [
     3,
     24
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# mapping from GluonNLP to PyTorch\n",
    "# NOTE: only BERT part\n",
    "\n",
    "mapping = {\n",
    "    'encoder.layer_norm.beta': 'embeddings.LayerNorm.bias',\n",
    "    'encoder.layer_norm.gamma': 'embeddings.LayerNorm.weight',\n",
    "    'encoder.position_weight': 'embeddings.position_embeddings.weight',\n",
    "    'word_embed.0.weight': 'embeddings.word_embeddings.weight',\n",
    "    'token_type_embed.0.weight': 'embeddings.token_type_embeddings.weight',\n",
    "\n",
    "    'pooler': 'pooler.dense',\n",
    "    'encoder.transformer_cells': 'encoder.layer',  # 'transformer.layer'\n",
    "\n",
    "    'attention_cell': 'attention',\n",
    "    '.proj.': '.attention.output.dense.',  # '.attention.out_lin.'\n",
    "    'proj_key': 'self.key',  # 'k_lin'\n",
    "    'proj_query': 'self.query',  # 'q_lin'\n",
    "    'proj_value': 'self.value',  # 'v_lin'\n",
    "    'ffn.ffn_1': 'intermediate.dense',  # 'lin1'\n",
    "    'ffn.ffn_2': 'output.dense',  # 'lin2'\n",
    "    'ffn.layer_norm.beta': 'output.LayerNorm.bias',  # output_layer_norm.bias'\n",
    "    'ffn.layer_norm.gamma': 'output.LayerNorm.weight',  # 'output_layer_norm.weight'\n",
    "}\n",
    "\n",
    "secondary_map = {\n",
    "    # because of overlap (if sorted reversed) of '.ffn.layer_norm.*'\n",
    "    'layer_norm.beta': 'attention.output.LayerNorm.bias',  # 'sa_layer_norm.bias'\n",
    "    'layer_norm.gamma': 'attention.output.LayerNorm.weight',  # 'sa_layer_norm.weight'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T17:59:48.673261Z",
     "start_time": "2020-05-25T17:59:48.659039Z"
    }
   },
   "outputs": [],
   "source": [
    "# params = model.bert.collect_params()\n",
    "# pytorch_model.bert.named_parameters()\n",
    "\n",
    "# NOTE: need to feed dummy input to build model to get initialized params ... or something like it\n",
    "model.bert.initialize(init=mx.init.Normal(0.02))\n",
    "ones = mx.nd.ones((2, 8))\n",
    "out = model.bert(ones, ones, mx.nd.array([5, 6]), mx.nd.array([[1], [2]]))\n",
    "params = model.bert._collect_params_with_prefix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T18:01:17.327163Z",
     "start_time": "2020-05-25T18:01:17.160034Z"
    },
    "code_folding": [
     4
    ],
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.position_weight [GluonNlp] -> embeddings.position_embeddings.weight [PyTorch]\n",
      "encoder.layer_norm.gamma [GluonNlp] -> embeddings.LayerNorm.weight [PyTorch]\n",
      "encoder.layer_norm.beta [GluonNlp] -> embeddings.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.0.attention_cell.proj_query.weight [GluonNlp] -> encoder.layer.0.attention.self.query.weight [PyTorch]\n",
      "encoder.transformer_cells.0.attention_cell.proj_query.bias [GluonNlp] -> encoder.layer.0.attention.self.query.bias [PyTorch]\n",
      "encoder.transformer_cells.0.attention_cell.proj_key.weight [GluonNlp] -> encoder.layer.0.attention.self.key.weight [PyTorch]\n",
      "encoder.transformer_cells.0.attention_cell.proj_key.bias [GluonNlp] -> encoder.layer.0.attention.self.key.bias [PyTorch]\n",
      "encoder.transformer_cells.0.attention_cell.proj_value.weight [GluonNlp] -> encoder.layer.0.attention.self.value.weight [PyTorch]\n",
      "encoder.transformer_cells.0.attention_cell.proj_value.bias [GluonNlp] -> encoder.layer.0.attention.self.value.bias [PyTorch]\n",
      "encoder.transformer_cells.0.proj.weight [GluonNlp] -> encoder.layer.0.attention.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.0.proj.bias [GluonNlp] -> encoder.layer.0.attention.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.0.ffn.ffn_1.weight [GluonNlp] -> encoder.layer.0.intermediate.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.0.ffn.ffn_1.bias [GluonNlp] -> encoder.layer.0.intermediate.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.0.ffn.ffn_2.weight [GluonNlp] -> encoder.layer.0.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.0.ffn.ffn_2.bias [GluonNlp] -> encoder.layer.0.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.0.ffn.layer_norm.gamma [GluonNlp] -> encoder.layer.0.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.0.ffn.layer_norm.beta [GluonNlp] -> encoder.layer.0.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.0.layer_norm.gamma [GluonNlp] -> encoder.layer.0.attention.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.0.layer_norm.beta [GluonNlp] -> encoder.layer.0.attention.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.1.attention_cell.proj_query.weight [GluonNlp] -> encoder.layer.1.attention.self.query.weight [PyTorch]\n",
      "encoder.transformer_cells.1.attention_cell.proj_query.bias [GluonNlp] -> encoder.layer.1.attention.self.query.bias [PyTorch]\n",
      "encoder.transformer_cells.1.attention_cell.proj_key.weight [GluonNlp] -> encoder.layer.1.attention.self.key.weight [PyTorch]\n",
      "encoder.transformer_cells.1.attention_cell.proj_key.bias [GluonNlp] -> encoder.layer.1.attention.self.key.bias [PyTorch]\n",
      "encoder.transformer_cells.1.attention_cell.proj_value.weight [GluonNlp] -> encoder.layer.1.attention.self.value.weight [PyTorch]\n",
      "encoder.transformer_cells.1.attention_cell.proj_value.bias [GluonNlp] -> encoder.layer.1.attention.self.value.bias [PyTorch]\n",
      "encoder.transformer_cells.1.proj.weight [GluonNlp] -> encoder.layer.1.attention.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.1.proj.bias [GluonNlp] -> encoder.layer.1.attention.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.1.ffn.ffn_1.weight [GluonNlp] -> encoder.layer.1.intermediate.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.1.ffn.ffn_1.bias [GluonNlp] -> encoder.layer.1.intermediate.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.1.ffn.ffn_2.weight [GluonNlp] -> encoder.layer.1.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.1.ffn.ffn_2.bias [GluonNlp] -> encoder.layer.1.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.1.ffn.layer_norm.gamma [GluonNlp] -> encoder.layer.1.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.1.ffn.layer_norm.beta [GluonNlp] -> encoder.layer.1.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.1.layer_norm.gamma [GluonNlp] -> encoder.layer.1.attention.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.1.layer_norm.beta [GluonNlp] -> encoder.layer.1.attention.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.2.attention_cell.proj_query.weight [GluonNlp] -> encoder.layer.2.attention.self.query.weight [PyTorch]\n",
      "encoder.transformer_cells.2.attention_cell.proj_query.bias [GluonNlp] -> encoder.layer.2.attention.self.query.bias [PyTorch]\n",
      "encoder.transformer_cells.2.attention_cell.proj_key.weight [GluonNlp] -> encoder.layer.2.attention.self.key.weight [PyTorch]\n",
      "encoder.transformer_cells.2.attention_cell.proj_key.bias [GluonNlp] -> encoder.layer.2.attention.self.key.bias [PyTorch]\n",
      "encoder.transformer_cells.2.attention_cell.proj_value.weight [GluonNlp] -> encoder.layer.2.attention.self.value.weight [PyTorch]\n",
      "encoder.transformer_cells.2.attention_cell.proj_value.bias [GluonNlp] -> encoder.layer.2.attention.self.value.bias [PyTorch]\n",
      "encoder.transformer_cells.2.proj.weight [GluonNlp] -> encoder.layer.2.attention.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.2.proj.bias [GluonNlp] -> encoder.layer.2.attention.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.2.ffn.ffn_1.weight [GluonNlp] -> encoder.layer.2.intermediate.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.2.ffn.ffn_1.bias [GluonNlp] -> encoder.layer.2.intermediate.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.2.ffn.ffn_2.weight [GluonNlp] -> encoder.layer.2.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.2.ffn.ffn_2.bias [GluonNlp] -> encoder.layer.2.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.2.ffn.layer_norm.gamma [GluonNlp] -> encoder.layer.2.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.2.ffn.layer_norm.beta [GluonNlp] -> encoder.layer.2.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.2.layer_norm.gamma [GluonNlp] -> encoder.layer.2.attention.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.2.layer_norm.beta [GluonNlp] -> encoder.layer.2.attention.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.3.attention_cell.proj_query.weight [GluonNlp] -> encoder.layer.3.attention.self.query.weight [PyTorch]\n",
      "encoder.transformer_cells.3.attention_cell.proj_query.bias [GluonNlp] -> encoder.layer.3.attention.self.query.bias [PyTorch]\n",
      "encoder.transformer_cells.3.attention_cell.proj_key.weight [GluonNlp] -> encoder.layer.3.attention.self.key.weight [PyTorch]\n",
      "encoder.transformer_cells.3.attention_cell.proj_key.bias [GluonNlp] -> encoder.layer.3.attention.self.key.bias [PyTorch]\n",
      "encoder.transformer_cells.3.attention_cell.proj_value.weight [GluonNlp] -> encoder.layer.3.attention.self.value.weight [PyTorch]\n",
      "encoder.transformer_cells.3.attention_cell.proj_value.bias [GluonNlp] -> encoder.layer.3.attention.self.value.bias [PyTorch]\n",
      "encoder.transformer_cells.3.proj.weight [GluonNlp] -> encoder.layer.3.attention.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.3.proj.bias [GluonNlp] -> encoder.layer.3.attention.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.3.ffn.ffn_1.weight [GluonNlp] -> encoder.layer.3.intermediate.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.3.ffn.ffn_1.bias [GluonNlp] -> encoder.layer.3.intermediate.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.3.ffn.ffn_2.weight [GluonNlp] -> encoder.layer.3.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.3.ffn.ffn_2.bias [GluonNlp] -> encoder.layer.3.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.3.ffn.layer_norm.gamma [GluonNlp] -> encoder.layer.3.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.3.ffn.layer_norm.beta [GluonNlp] -> encoder.layer.3.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.3.layer_norm.gamma [GluonNlp] -> encoder.layer.3.attention.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.3.layer_norm.beta [GluonNlp] -> encoder.layer.3.attention.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.4.attention_cell.proj_query.weight [GluonNlp] -> encoder.layer.4.attention.self.query.weight [PyTorch]\n",
      "encoder.transformer_cells.4.attention_cell.proj_query.bias [GluonNlp] -> encoder.layer.4.attention.self.query.bias [PyTorch]\n",
      "encoder.transformer_cells.4.attention_cell.proj_key.weight [GluonNlp] -> encoder.layer.4.attention.self.key.weight [PyTorch]\n",
      "encoder.transformer_cells.4.attention_cell.proj_key.bias [GluonNlp] -> encoder.layer.4.attention.self.key.bias [PyTorch]\n",
      "encoder.transformer_cells.4.attention_cell.proj_value.weight [GluonNlp] -> encoder.layer.4.attention.self.value.weight [PyTorch]\n",
      "encoder.transformer_cells.4.attention_cell.proj_value.bias [GluonNlp] -> encoder.layer.4.attention.self.value.bias [PyTorch]\n",
      "encoder.transformer_cells.4.proj.weight [GluonNlp] -> encoder.layer.4.attention.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.4.proj.bias [GluonNlp] -> encoder.layer.4.attention.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.4.ffn.ffn_1.weight [GluonNlp] -> encoder.layer.4.intermediate.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.4.ffn.ffn_1.bias [GluonNlp] -> encoder.layer.4.intermediate.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.4.ffn.ffn_2.weight [GluonNlp] -> encoder.layer.4.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.4.ffn.ffn_2.bias [GluonNlp] -> encoder.layer.4.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.4.ffn.layer_norm.gamma [GluonNlp] -> encoder.layer.4.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.4.ffn.layer_norm.beta [GluonNlp] -> encoder.layer.4.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.4.layer_norm.gamma [GluonNlp] -> encoder.layer.4.attention.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.4.layer_norm.beta [GluonNlp] -> encoder.layer.4.attention.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.5.attention_cell.proj_query.weight [GluonNlp] -> encoder.layer.5.attention.self.query.weight [PyTorch]\n",
      "encoder.transformer_cells.5.attention_cell.proj_query.bias [GluonNlp] -> encoder.layer.5.attention.self.query.bias [PyTorch]\n",
      "encoder.transformer_cells.5.attention_cell.proj_key.weight [GluonNlp] -> encoder.layer.5.attention.self.key.weight [PyTorch]\n",
      "encoder.transformer_cells.5.attention_cell.proj_key.bias [GluonNlp] -> encoder.layer.5.attention.self.key.bias [PyTorch]\n",
      "encoder.transformer_cells.5.attention_cell.proj_value.weight [GluonNlp] -> encoder.layer.5.attention.self.value.weight [PyTorch]\n",
      "encoder.transformer_cells.5.attention_cell.proj_value.bias [GluonNlp] -> encoder.layer.5.attention.self.value.bias [PyTorch]\n",
      "encoder.transformer_cells.5.proj.weight [GluonNlp] -> encoder.layer.5.attention.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.5.proj.bias [GluonNlp] -> encoder.layer.5.attention.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.5.ffn.ffn_1.weight [GluonNlp] -> encoder.layer.5.intermediate.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.5.ffn.ffn_1.bias [GluonNlp] -> encoder.layer.5.intermediate.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.5.ffn.ffn_2.weight [GluonNlp] -> encoder.layer.5.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.5.ffn.ffn_2.bias [GluonNlp] -> encoder.layer.5.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.5.ffn.layer_norm.gamma [GluonNlp] -> encoder.layer.5.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.5.ffn.layer_norm.beta [GluonNlp] -> encoder.layer.5.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.5.layer_norm.gamma [GluonNlp] -> encoder.layer.5.attention.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.5.layer_norm.beta [GluonNlp] -> encoder.layer.5.attention.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.6.attention_cell.proj_query.weight [GluonNlp] -> encoder.layer.6.attention.self.query.weight [PyTorch]\n",
      "encoder.transformer_cells.6.attention_cell.proj_query.bias [GluonNlp] -> encoder.layer.6.attention.self.query.bias [PyTorch]\n",
      "encoder.transformer_cells.6.attention_cell.proj_key.weight [GluonNlp] -> encoder.layer.6.attention.self.key.weight [PyTorch]\n",
      "encoder.transformer_cells.6.attention_cell.proj_key.bias [GluonNlp] -> encoder.layer.6.attention.self.key.bias [PyTorch]\n",
      "encoder.transformer_cells.6.attention_cell.proj_value.weight [GluonNlp] -> encoder.layer.6.attention.self.value.weight [PyTorch]\n",
      "encoder.transformer_cells.6.attention_cell.proj_value.bias [GluonNlp] -> encoder.layer.6.attention.self.value.bias [PyTorch]\n",
      "encoder.transformer_cells.6.proj.weight [GluonNlp] -> encoder.layer.6.attention.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.6.proj.bias [GluonNlp] -> encoder.layer.6.attention.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.6.ffn.ffn_1.weight [GluonNlp] -> encoder.layer.6.intermediate.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.6.ffn.ffn_1.bias [GluonNlp] -> encoder.layer.6.intermediate.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.6.ffn.ffn_2.weight [GluonNlp] -> encoder.layer.6.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.6.ffn.ffn_2.bias [GluonNlp] -> encoder.layer.6.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.6.ffn.layer_norm.gamma [GluonNlp] -> encoder.layer.6.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.6.ffn.layer_norm.beta [GluonNlp] -> encoder.layer.6.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.6.layer_norm.gamma [GluonNlp] -> encoder.layer.6.attention.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.6.layer_norm.beta [GluonNlp] -> encoder.layer.6.attention.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.7.attention_cell.proj_query.weight [GluonNlp] -> encoder.layer.7.attention.self.query.weight [PyTorch]\n",
      "encoder.transformer_cells.7.attention_cell.proj_query.bias [GluonNlp] -> encoder.layer.7.attention.self.query.bias [PyTorch]\n",
      "encoder.transformer_cells.7.attention_cell.proj_key.weight [GluonNlp] -> encoder.layer.7.attention.self.key.weight [PyTorch]\n",
      "encoder.transformer_cells.7.attention_cell.proj_key.bias [GluonNlp] -> encoder.layer.7.attention.self.key.bias [PyTorch]\n",
      "encoder.transformer_cells.7.attention_cell.proj_value.weight [GluonNlp] -> encoder.layer.7.attention.self.value.weight [PyTorch]\n",
      "encoder.transformer_cells.7.attention_cell.proj_value.bias [GluonNlp] -> encoder.layer.7.attention.self.value.bias [PyTorch]\n",
      "encoder.transformer_cells.7.proj.weight [GluonNlp] -> encoder.layer.7.attention.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.7.proj.bias [GluonNlp] -> encoder.layer.7.attention.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.7.ffn.ffn_1.weight [GluonNlp] -> encoder.layer.7.intermediate.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.7.ffn.ffn_1.bias [GluonNlp] -> encoder.layer.7.intermediate.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.7.ffn.ffn_2.weight [GluonNlp] -> encoder.layer.7.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.7.ffn.ffn_2.bias [GluonNlp] -> encoder.layer.7.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.7.ffn.layer_norm.gamma [GluonNlp] -> encoder.layer.7.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.7.ffn.layer_norm.beta [GluonNlp] -> encoder.layer.7.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.7.layer_norm.gamma [GluonNlp] -> encoder.layer.7.attention.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.7.layer_norm.beta [GluonNlp] -> encoder.layer.7.attention.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.8.attention_cell.proj_query.weight [GluonNlp] -> encoder.layer.8.attention.self.query.weight [PyTorch]\n",
      "encoder.transformer_cells.8.attention_cell.proj_query.bias [GluonNlp] -> encoder.layer.8.attention.self.query.bias [PyTorch]\n",
      "encoder.transformer_cells.8.attention_cell.proj_key.weight [GluonNlp] -> encoder.layer.8.attention.self.key.weight [PyTorch]\n",
      "encoder.transformer_cells.8.attention_cell.proj_key.bias [GluonNlp] -> encoder.layer.8.attention.self.key.bias [PyTorch]\n",
      "encoder.transformer_cells.8.attention_cell.proj_value.weight [GluonNlp] -> encoder.layer.8.attention.self.value.weight [PyTorch]\n",
      "encoder.transformer_cells.8.attention_cell.proj_value.bias [GluonNlp] -> encoder.layer.8.attention.self.value.bias [PyTorch]\n",
      "encoder.transformer_cells.8.proj.weight [GluonNlp] -> encoder.layer.8.attention.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.8.proj.bias [GluonNlp] -> encoder.layer.8.attention.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.8.ffn.ffn_1.weight [GluonNlp] -> encoder.layer.8.intermediate.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.8.ffn.ffn_1.bias [GluonNlp] -> encoder.layer.8.intermediate.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.8.ffn.ffn_2.weight [GluonNlp] -> encoder.layer.8.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.8.ffn.ffn_2.bias [GluonNlp] -> encoder.layer.8.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.8.ffn.layer_norm.gamma [GluonNlp] -> encoder.layer.8.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.8.ffn.layer_norm.beta [GluonNlp] -> encoder.layer.8.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.8.layer_norm.gamma [GluonNlp] -> encoder.layer.8.attention.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.8.layer_norm.beta [GluonNlp] -> encoder.layer.8.attention.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.9.attention_cell.proj_query.weight [GluonNlp] -> encoder.layer.9.attention.self.query.weight [PyTorch]\n",
      "encoder.transformer_cells.9.attention_cell.proj_query.bias [GluonNlp] -> encoder.layer.9.attention.self.query.bias [PyTorch]\n",
      "encoder.transformer_cells.9.attention_cell.proj_key.weight [GluonNlp] -> encoder.layer.9.attention.self.key.weight [PyTorch]\n",
      "encoder.transformer_cells.9.attention_cell.proj_key.bias [GluonNlp] -> encoder.layer.9.attention.self.key.bias [PyTorch]\n",
      "encoder.transformer_cells.9.attention_cell.proj_value.weight [GluonNlp] -> encoder.layer.9.attention.self.value.weight [PyTorch]\n",
      "encoder.transformer_cells.9.attention_cell.proj_value.bias [GluonNlp] -> encoder.layer.9.attention.self.value.bias [PyTorch]\n",
      "encoder.transformer_cells.9.proj.weight [GluonNlp] -> encoder.layer.9.attention.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.9.proj.bias [GluonNlp] -> encoder.layer.9.attention.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.9.ffn.ffn_1.weight [GluonNlp] -> encoder.layer.9.intermediate.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.9.ffn.ffn_1.bias [GluonNlp] -> encoder.layer.9.intermediate.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.9.ffn.ffn_2.weight [GluonNlp] -> encoder.layer.9.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.9.ffn.ffn_2.bias [GluonNlp] -> encoder.layer.9.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.9.ffn.layer_norm.gamma [GluonNlp] -> encoder.layer.9.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.9.ffn.layer_norm.beta [GluonNlp] -> encoder.layer.9.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.9.layer_norm.gamma [GluonNlp] -> encoder.layer.9.attention.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.9.layer_norm.beta [GluonNlp] -> encoder.layer.9.attention.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.10.attention_cell.proj_query.weight [GluonNlp] -> encoder.layer.10.attention.self.query.weight [PyTorch]\n",
      "encoder.transformer_cells.10.attention_cell.proj_query.bias [GluonNlp] -> encoder.layer.10.attention.self.query.bias [PyTorch]\n",
      "encoder.transformer_cells.10.attention_cell.proj_key.weight [GluonNlp] -> encoder.layer.10.attention.self.key.weight [PyTorch]\n",
      "encoder.transformer_cells.10.attention_cell.proj_key.bias [GluonNlp] -> encoder.layer.10.attention.self.key.bias [PyTorch]\n",
      "encoder.transformer_cells.10.attention_cell.proj_value.weight [GluonNlp] -> encoder.layer.10.attention.self.value.weight [PyTorch]\n",
      "encoder.transformer_cells.10.attention_cell.proj_value.bias [GluonNlp] -> encoder.layer.10.attention.self.value.bias [PyTorch]\n",
      "encoder.transformer_cells.10.proj.weight [GluonNlp] -> encoder.layer.10.attention.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.10.proj.bias [GluonNlp] -> encoder.layer.10.attention.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.10.ffn.ffn_1.weight [GluonNlp] -> encoder.layer.10.intermediate.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.10.ffn.ffn_1.bias [GluonNlp] -> encoder.layer.10.intermediate.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.10.ffn.ffn_2.weight [GluonNlp] -> encoder.layer.10.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.10.ffn.ffn_2.bias [GluonNlp] -> encoder.layer.10.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.10.ffn.layer_norm.gamma [GluonNlp] -> encoder.layer.10.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.10.ffn.layer_norm.beta [GluonNlp] -> encoder.layer.10.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.10.layer_norm.gamma [GluonNlp] -> encoder.layer.10.attention.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.10.layer_norm.beta [GluonNlp] -> encoder.layer.10.attention.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.11.attention_cell.proj_query.weight [GluonNlp] -> encoder.layer.11.attention.self.query.weight [PyTorch]\n",
      "encoder.transformer_cells.11.attention_cell.proj_query.bias [GluonNlp] -> encoder.layer.11.attention.self.query.bias [PyTorch]\n",
      "encoder.transformer_cells.11.attention_cell.proj_key.weight [GluonNlp] -> encoder.layer.11.attention.self.key.weight [PyTorch]\n",
      "encoder.transformer_cells.11.attention_cell.proj_key.bias [GluonNlp] -> encoder.layer.11.attention.self.key.bias [PyTorch]\n",
      "encoder.transformer_cells.11.attention_cell.proj_value.weight [GluonNlp] -> encoder.layer.11.attention.self.value.weight [PyTorch]\n",
      "encoder.transformer_cells.11.attention_cell.proj_value.bias [GluonNlp] -> encoder.layer.11.attention.self.value.bias [PyTorch]\n",
      "encoder.transformer_cells.11.proj.weight [GluonNlp] -> encoder.layer.11.attention.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.11.proj.bias [GluonNlp] -> encoder.layer.11.attention.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.11.ffn.ffn_1.weight [GluonNlp] -> encoder.layer.11.intermediate.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.11.ffn.ffn_1.bias [GluonNlp] -> encoder.layer.11.intermediate.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.11.ffn.ffn_2.weight [GluonNlp] -> encoder.layer.11.output.dense.weight [PyTorch]\n",
      "encoder.transformer_cells.11.ffn.ffn_2.bias [GluonNlp] -> encoder.layer.11.output.dense.bias [PyTorch]\n",
      "encoder.transformer_cells.11.ffn.layer_norm.gamma [GluonNlp] -> encoder.layer.11.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.11.ffn.layer_norm.beta [GluonNlp] -> encoder.layer.11.output.LayerNorm.bias [PyTorch]\n",
      "encoder.transformer_cells.11.layer_norm.gamma [GluonNlp] -> encoder.layer.11.attention.output.LayerNorm.weight [PyTorch]\n",
      "encoder.transformer_cells.11.layer_norm.beta [GluonNlp] -> encoder.layer.11.attention.output.LayerNorm.bias [PyTorch]\n",
      "word_embed.0.weight [GluonNlp] -> embeddings.word_embeddings.weight [PyTorch]\n",
      "token_type_embed.0.weight [GluonNlp] -> embeddings.token_type_embeddings.weight [PyTorch]\n",
      "pooler.weight [GluonNlp] -> pooler.dense.weight [PyTorch]\n",
      "pooler.bias [GluonNlp] -> pooler.dense.bias [PyTorch]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['encoder.layer.9.attention.output.LayerNorm.bias',\n",
       "  'encoder.layer.9.attention.output.LayerNorm.weight',\n",
       "  'encoder.layer.9.attention.output.dense.bias',\n",
       "  'encoder.layer.9.attention.output.dense.weight',\n",
       "  'encoder.layer.9.attention.self.key.bias',\n",
       "  'encoder.layer.9.attention.self.key.weight',\n",
       "  'encoder.layer.9.attention.self.query.bias',\n",
       "  'encoder.layer.9.attention.self.query.weight',\n",
       "  'encoder.layer.9.attention.self.value.bias',\n",
       "  'encoder.layer.9.attention.self.value.weight',\n",
       "  'encoder.layer.9.intermediate.dense.bias',\n",
       "  'encoder.layer.9.intermediate.dense.weight',\n",
       "  'encoder.layer.9.output.LayerNorm.bias',\n",
       "  'encoder.layer.9.output.LayerNorm.weight',\n",
       "  'encoder.layer.9.output.dense.bias',\n",
       "  'encoder.layer.9.output.dense.weight',\n",
       "  'pooler.dense.bias',\n",
       "  'pooler.dense.weight'],\n",
       " ['encoder.layer.8.output.dense.weight',\n",
       "  'encoder.layer.9.attention.output.LayerNorm.bias',\n",
       "  'encoder.layer.9.attention.output.LayerNorm.weight',\n",
       "  'encoder.layer.9.attention.output.dense.bias',\n",
       "  'encoder.layer.9.attention.output.dense.weight',\n",
       "  'encoder.layer.9.attention.self.key.bias',\n",
       "  'encoder.layer.9.attention.self.key.weight',\n",
       "  'encoder.layer.9.attention.self.query.bias',\n",
       "  'encoder.layer.9.attention.self.query.weight',\n",
       "  'encoder.layer.9.attention.self.value.bias',\n",
       "  'encoder.layer.9.attention.self.value.weight',\n",
       "  'encoder.layer.9.intermediate.dense.bias',\n",
       "  'encoder.layer.9.intermediate.dense.weight',\n",
       "  'encoder.layer.9.output.LayerNorm.bias',\n",
       "  'encoder.layer.9.output.LayerNorm.weight',\n",
       "  'encoder.layer.9.output.dense.bias',\n",
       "  'encoder.layer.9.output.dense.weight',\n",
       "  'pooler.dense.bias',\n",
       "  'pooler.dense.weight'],\n",
       " ['encoder.transformer_cells.9.attention_cell.proj_key.bias',\n",
       "  'encoder.transformer_cells.9.attention_cell.proj_key.weight',\n",
       "  'encoder.transformer_cells.9.attention_cell.proj_query.bias',\n",
       "  'encoder.transformer_cells.9.attention_cell.proj_query.weight',\n",
       "  'encoder.transformer_cells.9.attention_cell.proj_value.bias',\n",
       "  'encoder.transformer_cells.9.attention_cell.proj_value.weight',\n",
       "  'encoder.transformer_cells.9.ffn.ffn_1.bias',\n",
       "  'encoder.transformer_cells.9.ffn.ffn_1.weight',\n",
       "  'encoder.transformer_cells.9.ffn.ffn_2.bias',\n",
       "  'encoder.transformer_cells.9.ffn.ffn_2.weight',\n",
       "  'encoder.transformer_cells.9.ffn.layer_norm.beta',\n",
       "  'encoder.transformer_cells.9.ffn.layer_norm.gamma',\n",
       "  'encoder.transformer_cells.9.layer_norm.beta',\n",
       "  'encoder.transformer_cells.9.layer_norm.gamma',\n",
       "  'encoder.transformer_cells.9.proj.bias',\n",
       "  'encoder.transformer_cells.9.proj.weight',\n",
       "  'pooler.bias',\n",
       "  'pooler.weight',\n",
       "  'token_type_embed.0.weight',\n",
       "  'word_embed.0.weight'])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test/Debug\n",
    "\n",
    "names_gluon1, names_gluon2 = list(), list()\n",
    "\n",
    "for name, tensor in params.items():\n",
    "    pytorch_name = name\n",
    "    print(pytorch_name, \"[GluonNlp]\", end=\" -> \")\n",
    "    for k, v in mapping.items():\n",
    "        pytorch_name = pytorch_name.replace(k, v)\n",
    "    # print(pytorch_name, end=\" -> \")\n",
    "    for k, v in secondary_map.items():\n",
    "        pytorch_name = pytorch_name.replace(k, v)\n",
    "    print(pytorch_name, \"[PyTorch]\")\n",
    "    \n",
    "    names_gluon1.append(name)\n",
    "    names_gluon2.append(pytorch_name)\n",
    "    \n",
    "    assert pytorch_name in pytorch_params, f\"Gluon:{name}, Torch?:{pytorch_name} not in PyTorch model?\"\n",
    "    arr = mx.nd.array(pytorch_params[pytorch_name])\n",
    "    assert arr.shape == params[name].shape, f\"Gluon:{name}, Torch?:{pytorch_name} mismatch?\"\n",
    "    \n",
    "    #tensor\n",
    "    \n",
    "names_gluon1, names_gluon2 = set(names_gluon1), set(names_gluon2)\n",
    "names_torch = set(pytorch_params.keys())\n",
    "\n",
    "sorted(names_torch)[-18:], sorted(names_gluon2)[-19:], sorted(names_gluon1)[-20:]\n",
    "# names_gluon2 - names_torch\n",
    "# names_torch - names_gluon2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T18:01:29.373667Z",
     "start_time": "2020-05-25T18:01:27.097626Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Copy GluonNLP to PyTorch\n",
    "\n",
    "for name, tensor in params.items():\n",
    "    pytorch_name = name\n",
    "    for k, v in mapping.items():\n",
    "        pytorch_name = pytorch_name.replace(k, v)\n",
    "    for k, v in secondary_map.items():\n",
    "        pytorch_name = pytorch_name.replace(k, v)\n",
    "    \n",
    "    assert pytorch_name in pytorch_params, f\"Gluon:{name}, Torch?:{pytorch_name} not in PyTorch model?\"\n",
    "    arr = mx.nd.array(pytorch_params[pytorch_name])\n",
    "    assert arr.shape == params[name].shape, f\"Gluon:{name}, Torch?:{pytorch_name} mismatch?\"\n",
    "    \n",
    "    tensor_pytorch = torch.tensor(tensor.data().asnumpy())\n",
    "    # tensor_pytorch = torch.nn.parameter.Parameter(tensor_pytorch)\n",
    "    pytorch_params[pytorch_name] = tensor_pytorch\n",
    "    \n",
    "torch.save(pytorch_params, str(fn_model_bert_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T18:01:34.570165Z",
     "start_time": "2020-05-25T18:01:34.566474Z"
    }
   },
   "outputs": [],
   "source": [
    "params_classifier = model.classifier._collect_params_with_prefix()\n",
    "pytorch_classifier_params = torch.load(str(fn_model_classifier_params))\n",
    "\n",
    "pytorch_classifier_params[\"weight\"] = torch.tensor(params_classifier[\"1.weight\"].data().asnumpy())\n",
    "pytorch_classifier_params[\"bias\"] = torch.tensor(params_classifier[\"1.bias\"].data().asnumpy())\n",
    "\n",
    "torch.save(pytorch_classifier_params, str(fn_model_classifier_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Rest (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T17:04:15.196459Z",
     "start_time": "2020-05-25T17:04:15.188481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier.1.weight': Parameter bertclassifier0_dense0_weight (shape=(1, 768), dtype=float32),\n",
       " 'classifier.1.bias': Parameter bertclassifier0_dense0_bias (shape=(1,), dtype=float32)}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params2 = model._collect_params_with_prefix()\n",
    "params2 = {k: v for k, v in params2.items() if k.startswith(\"classifier.\")}\n",
    "params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T17:51:04.549608Z",
     "start_time": "2020-05-25T17:51:04.538604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier.weight': tensor([[-0.0442, -0.0256, -0.0154,  ...,  0.0229,  0.0080, -0.0004],\n",
       "         [ 0.0159, -0.0092,  0.0290,  ..., -0.0198, -0.0095,  0.0116]]),\n",
       " 'classifier.bias': tensor([0., 0.])}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#params2_pytorch = dict(pytorch_model.named_parameters())\n",
    "params2_pytorch = dict(pytorch_model.state_dict())\n",
    "params2_pytorch = {k: v for k, v in params2_pytorch.items() if k.startswith(\"classifier.\")}\n",
    "params2_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load converted model ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T18:06:09.709692Z",
     "start_time": "2020-05-25T18:06:09.704019Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dir_name = Path(\"./temp\")\n",
    "if not dir_name.exists():\n",
    "    dir_name.mkdir()\n",
    "    \n",
    "fn_model_params = dir_name / \"pytorch_model.bin\"\n",
    "fn_model_bert_params = dir_name / \"pytorch_model_bert.bin\"\n",
    "fn_model_classifier_params = dir_name / \"pytorch_model_classifier.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T18:36:33.964689Z",
     "start_time": "2020-05-25T18:36:30.634762Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0525 20:36:31.150555 140389655271232 configuration_utils.py:151] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/ekoerner/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0525 20:36:31.152992 140389655271232 configuration_utils.py:168] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0525 20:36:31.606558 140389655271232 modeling_utils.py:337] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /home/ekoerner/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0525 20:36:33.847275 140389655271232 modeling_utils.py:405] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0525 20:36:33.847986 140389655271232 modeling_utils.py:408] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "\n",
    "# TODO: check num_labels and pytorch_classifier_params[\"bias\"].shape == (2,) or (1,)?\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\", num_labels=1)\n",
    "pytorch_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)\n",
    "\n",
    "pytorch_bert_params = torch.load(str(fn_model_bert_params))\n",
    "pytorch_classifier_params = torch.load(str(fn_model_classifier_params))\n",
    "\n",
    "# copy BERT params\n",
    "pytorch_model.bert.load_state_dict(pytorch_bert_params)\n",
    "\n",
    "# copy classifier params\n",
    "pytorch_model_classifier_params = pytorch_model.classifier.load_state_dict(pytorch_classifier_params)\n",
    "# ??pytorch_model.classifier.load_state_dict\n",
    "# ??pytorch_model.classifier._load_from_state_dict\n",
    "#pytorch_model.classifier.state_dict()['weight'].copy_(pytorch_classifier_params[\"weight\"])\n",
    "#pytorch_model.classifier.state_dict()['bias'].copy_(pytorch_classifier_params[\"bias\"])\n",
    "# pytorch_model.classifier._parameters['weight'].copy_(pytorch_classifier_params[\"weight\"])\n",
    "# pytorch_model.classifier._parameters['bias'].copy_(pytorch_classifier_params[\"bias\"])\n",
    "    \n",
    "#pytorch_model.num_labels = 1  # ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://huggingface.co/transformers/usage.html\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T18:37:16.442419Z",
     "start_time": "2020-05-25T18:37:15.922214Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0525 20:37:16.405678 140389655271232 tokenization_utils.py:374] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/ekoerner/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T18:39:58.646365Z",
     "start_time": "2020-05-25T18:39:58.568853Z"
    }
   },
   "outputs": [],
   "source": [
    "sequence_0 = \"The company HuggingFace is based in New York City\"\n",
    "sequence_1 = \"Apples are especially bad for your health\"\n",
    "\n",
    "pytorch_model.num_labels = 1  # ???\n",
    "\n",
    "paraphrase = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=\"pt\")\n",
    "paraphrase_classification_logits = pytorch_model(**paraphrase)[0]\n",
    "if pytorch_model.num_labels >= 2:\n",
    "    paraphrase_results = torch.softmax(paraphrase_classification_logits, dim=1).tolist()[0]\n",
    "    class_probab = [f\"{round(p * 100, 1)}%\" for p in paraphrase_results]\n",
    "else:\n",
    "    paraphrase_results = torch.sigmoid(paraphrase_classification_logits).tolist()[0]\n",
    "    class_probab = [round(p) for p in paraphrase_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T18:40:07.381646Z",
     "start_time": "2020-05-25T18:40:07.378252Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2331]], grad_fn=<AddmmBackward>), [0.44198086857795715], [0])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase_classification_logits, paraphrase_results, class_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
