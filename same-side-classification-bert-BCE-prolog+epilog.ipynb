{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gluon-nlp.mxnet.io/install.html\n",
    "\n",
    "```\n",
    "pip install --upgrade 'mxnet>=1.3.0'\n",
    "pip install gluonnlp\n",
    "wget https://gluon-nlp.mxnet.io/_downloads/sentence_embedding.zip\n",
    "unzip sentence_embedding.zip\n",
    "ln -s sentence_embedding/bert bert\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:14:48.913470Z",
     "start_time": "2019-07-20T10:14:47.804371Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import csv\n",
    "import gluonnlp as nlp\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from bert import *\n",
    "from mxboard import SummaryWriter\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.data import Dataset, SimpleDataset\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:14:48.917854Z",
     "start_time": "2019-07-20T10:14:48.915088Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:14:48.928863Z",
     "start_time": "2019-07-20T10:14:48.919207Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:14:48.940553Z",
     "start_time": "2019-07-20T10:14:48.930231Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# set repeatable random state\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:14:48.951226Z",
     "start_time": "2019-07-20T10:14:48.942247Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply progress bars for pandas .apply() -> .progress_apply()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:14:48.996312Z",
     "start_time": "2019-07-20T10:14:48.953208Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327c140cc8c94dfca2b1d89a9bb275bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make tqdm jupyter friendly\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# for .progress_apply() we have to hack it like this?\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:14:49.004782Z",
     "start_time": "2019-07-20T10:14:48.998418Z"
    },
    "code_folding": [
     0,
     4
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        time_end = time.time()\n",
    "        time_delta = datetime.timedelta(seconds=(time_end - self.time_start))\n",
    "        if self.name:\n",
    "            print((\"Time for [{}]: {}\".format(self.name, time_delta)))\n",
    "        else:\n",
    "            print((\"Time: {}\".format(time_delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Same Side Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:14:49.017247Z",
     "start_time": "2019-07-20T10:14:49.007391Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cross_path = 'data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = 'data/same-side-classification/within-topic/{}.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load within-topics and cross-topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:14:50.733191Z",
     "start_time": "2019-07-20T10:14:49.019837Z"
    },
    "code_folding": [
     0,
     10
    ],
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [read cross]: 0:00:00.857154\n",
      "Time for [read within]: 0:00:00.841385\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"read cross\"):\n",
    "    cross_traindev_df = pd.read_csv(data_cross_path.format('training'),\n",
    "                                    quotechar='\"',\n",
    "                                    quoting=csv.QUOTE_ALL,\n",
    "                                    encoding='utf-8',\n",
    "                                    escapechar='\\\\',\n",
    "                                    doublequote=False,\n",
    "                                    index_col='id')\n",
    "    cross_test_df = pd.read_csv(data_cross_path.format('test'), index_col='id')\n",
    "\n",
    "with Timer(\"read within\"):\n",
    "    within_traindev_df = pd.read_csv(data_within_path.format('training'),\n",
    "                                     quotechar='\"',\n",
    "                                     quoting=csv.QUOTE_ALL,\n",
    "                                     encoding='utf-8',\n",
    "                                     escapechar='\\\\',\n",
    "                                     doublequote=False,\n",
    "                                     index_col='id')\n",
    "    # within_test_df = pd.read_csv(data_within_path.format('test'),\n",
    "    #                              quotechar='\"',\n",
    "    #                              quoting=csv.QUOTE_ALL,\n",
    "    #                              encoding='utf-8',\n",
    "    #                              escapechar='\\\\',\n",
    "    #                              doublequote=True,  # <-- change, \"\" as quote escape in text?\n",
    "    #                              index_col='id')\n",
    "    within_test_df = pd.read_csv(data_within_path.format('test'), index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -n 5 data/same-side-classification/within-topic/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.300231Z",
     "start_time": "2019-07-20T10:14:50.734466Z"
    },
    "code_folding": [
     1
    ],
    "init_cell": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3500743e8125415ea4c204bd3119146a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=61048), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag cross traindev]: 0:00:32.995948\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5683a49836264c3a893412af3eea9ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6163), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag cross test]: 0:00:03.248861\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b370ed207a541548aadfcdbd6ab258b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=63903), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag within traindev]: 0:00:34.442718\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd9846edd3848ffbd655ce5bf48f136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3552), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [tag within test]: 0:00:01.873477\n"
     ]
    }
   ],
   "source": [
    "# Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "def add_tag(row):\n",
    "    title = row['topic'].lower().strip()\n",
    "    if \"abortion\" in title:\n",
    "        row['tag'] = 'abortion'\n",
    "    elif \"gay marriage\"  in title:\n",
    "        row['tag'] = 'gay marriage'\n",
    "    else:\n",
    "        row['tag'] = 'NA'\n",
    "    return row\n",
    "\n",
    "\n",
    "with Timer(\"tag cross traindev\"):\n",
    "    cross_traindev_df = cross_traindev_df.progress_apply(add_tag, axis=1)\n",
    "with Timer(\"tag cross test\"):\n",
    "    cross_test_df = cross_test_df.progress_apply(add_tag, axis=1)\n",
    "\n",
    "with Timer(\"tag within traindev\"):\n",
    "    within_traindev_df = within_traindev_df.progress_apply(add_tag, axis=1)\n",
    "with Timer(\"tag within test\"):\n",
    "    within_test_df = within_test_df.progress_apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview about each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.310066Z",
     "start_time": "2019-07-20T10:16:03.301414Z"
    },
    "code_folding": [
     4
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# requires nltk  wordtokenize\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# model uses BERT Tokenizer ...\n",
    "\n",
    "def get_overview(df, task='same-side', class_name='is_same_side'):\n",
    "    # Total instance numbers\n",
    "    total = len(df)\n",
    "    print(\"Task: \", task)\n",
    "    print('=' * 40, '\\n')\n",
    "\n",
    "    print('Total instances: ', total)\n",
    "    print('\\n')\n",
    "\n",
    "    print('For each topic:')\n",
    "    for tag, tag_df in df.groupby(['tag']):\n",
    "        print(tag, ': ', len(tag_df), ' instances')\n",
    "        if class_name in df.columns:\n",
    "            for is_same_side, side_df in tag_df.groupby([class_name]):\n",
    "                print('\\t\\t', is_same_side, ': ', len(side_df), ' instances')\n",
    "    print('\\n')\n",
    "\n",
    "    if class_name in df.columns:\n",
    "        print('For each class value:')\n",
    "        for class_value, class_df in df.groupby([class_name]):\n",
    "            print(class_value, ': ', len(class_df), ' instances')\n",
    "        print('\\n')\n",
    "\n",
    "    print('Unique argument1:', len(df['argument1'].unique()))\n",
    "    print('Unique argument2:', len(df['argument2'].unique()))\n",
    "    arguments = df['argument1'].values\n",
    "    arguments = np.concatenate([arguments, df['argument2'].values])\n",
    "\n",
    "    print('Unique total arguments:', len(set(list(arguments))), '\\n')\n",
    "    \n",
    "    return\n",
    "\n",
    "    print('-' * 40, '\\n')\n",
    "\n",
    "    arguments_length_lst = [\n",
    "        len(word_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_length_lst.extend(\n",
    "        [len(word_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Words:')\n",
    "    print('\\tshortest argument:', min(arguments_length_lst), ' words')\n",
    "    print('\\tlongest argument:', max(arguments_length_lst), ' words')\n",
    "    print('\\targument average length:', np.mean(arguments_length_lst),\n",
    "          ' words')\n",
    "\n",
    "    arguments_sent_length_lst = [\n",
    "        len(sent_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_sent_length_lst.extend(\n",
    "        [len(sent_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Sentences:')\n",
    "    print('\\tshortest argument:', min(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\tlongest argument:', max(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\targument average length:', np.mean(arguments_sent_length_lst),\n",
    "          ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "with Timer(\"overview cross\"):\n",
    "    get_overview(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"overview within\"):\n",
    "    get_overview(within_traindev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count raw length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compute_arg_len(row):\n",
    "    row['argument1_len'] = len(row['argument1'])\n",
    "    row['argument2_len'] = len(row['argument2'])\n",
    "    row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "    row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff']\n",
    "    return row\n",
    "\n",
    "\n",
    "cross_traindev_df = cross_traindev_df.progress_apply(compute_arg_len, axis=1)\n",
    "within_traindev_df = within_traindev_df.progress_apply(compute_arg_len, axis=1)\n",
    "cross_test_df = cross_test_df.progress_apply(compute_arg_len, axis=1)\n",
    "within_test_df = within_test_df.progress_apply(compute_arg_len, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize and count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()\n",
    "_, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                    dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                    pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                    use_decoder=False, use_classifier=False)\n",
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "tokenizer = bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     5
    ]
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download('punct')\n",
    "\n",
    "\n",
    "# tokenizer from BERT\n",
    "def tokenize_arguments(row):\n",
    "    # tokenize\n",
    "    row['argument1_tokens'] = tokenizer(row['argument1'])\n",
    "    row['argument2_tokens'] = tokenizer(row['argument2'])\n",
    "\n",
    "    # count tokens\n",
    "    row['argument1_len'] = len(row['argument1_tokens'])\n",
    "    row['argument2_len'] = len(row['argument2_tokens'])\n",
    "    # token number diff\n",
    "    row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "    row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff'])\n",
    "    return row\n",
    "\n",
    "\n",
    "cross_traindev_df = cross_traindev_df.progress_apply(tokenize_arguments, axis=1)\n",
    "within_traindev_df = within_traindev_df.progress_apply(tokenize_arguments, axis=1)\n",
    "cross_test_df = cross_test_df.progress_apply(tokenize_arguments, axis=1)\n",
    "within_test_df = within_test_df.progress_apply(tokenize_arguments, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_traindev_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_test_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_lengths(df, slicen=None, abs_diff=True, title=None):\n",
    "    if df is None:\n",
    "        print(\"no lengths to plot\")\n",
    "        return\n",
    "    \n",
    "    arg1_lens = df['argument1_len']\n",
    "    arg2_lens = df['argument2_len']\n",
    "    arg_diff_len = df['argument12_len_diff']\n",
    "    \n",
    "    if abs_diff:\n",
    "        arg_diff_len = np.abs(arg_diff_len)\n",
    "    \n",
    "    if slicen is not None:\n",
    "        arg1_lens = arg1_lens[slicen]\n",
    "        arg2_lens = arg2_lens[slicen]\n",
    "        arg_diff_len = arg_diff_len[slicen]\n",
    "\n",
    "    x = np.arange(len(arg1_lens))  # arange/linspace\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, arg1_lens, label='argument1')  # Linie: '-', 'o-', '.-'\n",
    "    plt.plot(x, arg2_lens, label='argument2')  # Linie: '-', 'o-', '.-'\n",
    "    plt.legend()\n",
    "    plt.title('Lengths of arguments' if not title else title)\n",
    "    plt.ylabel('Lengths of arguments 1 and 2')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, arg_diff_len)\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Differences')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_lengths(within_traindev_df, slice(None, None, 500), title='Length of arguments within train/dev, every 500')\n",
    "plot_lengths(cross_traindev_df, slice(None, None, 500), title='Length of arguments cross train/dev, every 500')\n",
    "plot_lengths(within_test_df, slice(None, None, 1), title='Length of arguments within test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dev set - 70% 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.322148Z",
     "start_time": "2019-07-20T10:16:03.311198Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']]\n",
    "    y = df[['is_same_side']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "- https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.339837Z",
     "start_time": "2019-07-20T10:16:03.323492Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyBERTDataset(SimpleDataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        super(MyBERTDataset, self).__init__(self._convert())\n",
    "\n",
    "    def _convert(self):\n",
    "        allsamples = list()\n",
    "\n",
    "        if self._y is not None:\n",
    "            df = self._X.merge(self._y, left_index=True, right_index=True)\n",
    "            for _, row in df.iterrows():\n",
    "                # allsamples.append([\n",
    "                #     row['argument1'], row['argument2'],\n",
    "                #     \"1\" if str(row['is_same_side']) == \"True\" else \"0\"\n",
    "                # ])\n",
    "                allsamples.append([\n",
    "                    row['argument1'], row['argument2'],\n",
    "                    1 if str(row['is_same_side']) == \"True\" else 0\n",
    "                ])\n",
    "\n",
    "        else:\n",
    "            for _, row in self._X.iterrows():\n",
    "                allsamples.append([row['argument1'], row['argument2'], None])\n",
    "\n",
    "        return allsamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### my own `BERTDatasetTransform` for extracting chunks from arguments or last part etc.\n",
    "\n",
    "```python\n",
    "transform = dataset.BERTDatasetTransform(bert_tokenizer, 512,\n",
    "                                         labels=['0', '1'],\n",
    "                                         label_dtype='int32',\n",
    "                                         pad=True,\n",
    "                                         pair=True)\n",
    "```\n",
    "\n",
    "http://localhost:9001/edit/bert/dataset.py @454\n",
    "```python\n",
    "# substitute with my own (e. g. last part, many parts etc.)\n",
    "def __init__(...):\n",
    "    self._bert_xform = BERTSentenceTransform(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "```\n",
    "https://gluon-nlp.mxnet.io/master/_modules/gluonnlp/data/transforms.html#BERTSentenceTransform\n",
    "```python\n",
    "# substitute with my own (e. g. only last part (trim from start))\n",
    "self._truncate_seq_pair(tokens_a, tokens_b, self._max_seq_length - 3)\n",
    "```\n",
    "\n",
    "https://mxnet.incubator.apache.org/_modules/mxnet/gluon/data/dataset.html#Dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.418373Z",
     "start_time": "2019-07-20T10:16:03.341983Z"
    },
    "code_folding": [
     3,
     8,
     72,
     87
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from gluonnlp.data import BERTSentenceTransform\n",
    "\n",
    "\n",
    "class FirstAndLastPartBERTSentenceTransform(BERTSentenceTransform):\n",
    "    def __init__(self, tokenizer, max_seq_length, pad=True, pair=True):\n",
    "        super(FirstAndLastPartBERTSentenceTransform,\n",
    "              self).__init__(tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "    def __call__(self, line):\n",
    "        # convert to unicode\n",
    "        text_a = line[0]\n",
    "        if self._pair:\n",
    "            assert len(line) == 2\n",
    "            text_b = line[1]\n",
    "\n",
    "        tokens_a = self._tokenizer(text_a)\n",
    "        tokens_a_epi = tokens_a.copy()\n",
    "        tokens_b = None\n",
    "        tokens_b_epi = None\n",
    "\n",
    "        if self._pair:\n",
    "            tokens_b = self._tokenizer(text_b)\n",
    "            tokens_b_epi = tokens_b.copy()\n",
    "\n",
    "        if tokens_b:\n",
    "            self._truncate_seq_pair_prolog(tokens_a, tokens_b,\n",
    "                                           self._max_seq_length - 3)\n",
    "            self._truncate_seq_pair_epilog(tokens_a_epi, tokens_b_epi,\n",
    "                                           self._max_seq_length - 3)\n",
    "        else:\n",
    "            if len(tokens_a) > self._max_seq_length - 2:\n",
    "                tokens_a = tokens_a[0:(self._max_seq_length - 2)]\n",
    "            if len(tokens_a_epi) > self._max_seq_length - 2:\n",
    "                tokens_a_epi = tokens_a_epi[0:(self._max_seq_length - 2)]\n",
    "\n",
    "        vocab = self._tokenizer.vocab\n",
    "        tokens, tokens_epi = [], []\n",
    "        tokens.append(vocab.cls_token)\n",
    "        tokens_epi.append(vocab.cls_token)\n",
    "        tokens.extend(tokens_a)\n",
    "        tokens_epi.extend(tokens_a_epi)\n",
    "        tokens.append(vocab.sep_token)\n",
    "        tokens_epi.append(vocab.sep_token)\n",
    "        segment_ids = [0] * len(tokens)\n",
    "        segment_ids_epi = [0] * len(tokens_epi)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens.extend(tokens_b)\n",
    "            tokens_epi.extend(tokens_b_epi)\n",
    "            tokens.append(vocab.sep_token)\n",
    "            tokens_epi.append(vocab.sep_token)\n",
    "            segment_ids.extend([1] * (len(tokens) - len(segment_ids)))\n",
    "            segment_ids_epi.extend([1] * (len(tokens) - len(segment_ids_epi)))\n",
    "\n",
    "        input_ids = self._tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_ids_epi = self._tokenizer.convert_tokens_to_ids(tokens_epi)\n",
    "        valid_length = len(input_ids)\n",
    "        valid_length_epi = len(input_ids_epi)\n",
    "\n",
    "        if self._pad:\n",
    "            padding_length = self._max_seq_length - valid_length\n",
    "            padding_length_epi = self._max_seq_length - valid_length_epi\n",
    "            input_ids.extend([vocab[vocab.padding_token]] * padding_length)\n",
    "            input_ids_epi.extend([vocab[vocab.padding_token]] *\n",
    "                                 padding_length_epi)\n",
    "            segment_ids.extend([0] * padding_length)\n",
    "            segment_ids_epi.extend([0] * padding_length_epi)\n",
    "\n",
    "        return np.array(input_ids, dtype='int32'), np.array(valid_length, dtype='int32'),\\\n",
    "            np.array(segment_ids, dtype='int32'), np.array(input_ids_epi, dtype='int32'),\\\n",
    "            np.array(valid_length_epi, dtype='int32'), np.array(segment_ids_epi, dtype='int32')\n",
    "\n",
    "    def _truncate_seq_pair_prolog(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "\n",
    "    def _truncate_seq_pair_epilog(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\n",
    "        Removes from end of token list.\"\"\"\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop(0)\n",
    "            else:\n",
    "                tokens_b.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.430772Z",
     "start_time": "2019-07-20T10:16:03.419669Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class FirstAndLastPartBERTDatasetTransform(dataset.BERTDatasetTransform):\n",
    "    def __init__(self,\n",
    "                 tokenizer,\n",
    "                 max_seq_length,\n",
    "                 labels=None,\n",
    "                 pad=True,\n",
    "                 pair=True,\n",
    "                 label_dtype='float32'):\n",
    "        super(FirstAndLastPartBERTDatasetTransform,\n",
    "              self).__init__(tokenizer,\n",
    "                             max_seq_length,\n",
    "                             labels=labels,\n",
    "                             pad=pad,\n",
    "                             pair=pair,\n",
    "                             label_dtype=label_dtype)\n",
    "        self._bert_xform = FirstAndLastPartBERTSentenceTransform(\n",
    "            tokenizer, max_seq_length, pad=pad, pair=pair)\n",
    "\n",
    "    def __call__(self, line):\n",
    "        input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi = self._bert_xform(\n",
    "            line[:-1])\n",
    "\n",
    "        label = line[-1]\n",
    "\n",
    "        # if label is None than we are predicting unknown data\n",
    "        if label is None:\n",
    "            # early abort\n",
    "            return input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi\n",
    "            \n",
    "        if self.labels:  # for classification task\n",
    "            label = self._label_map[label]\n",
    "        label = np.array([label], dtype=self.label_dtype)\n",
    "\n",
    "        return input_ids, valid_length, segment_ids, input_ids_epi, valid_length_epi, segment_ids_epi, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.448349Z",
     "start_time": "2019-07-20T10:16:03.432190Z"
    },
    "code_folding": [
     4
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon import Block\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "\n",
    "class BERTProEpiClassifier(Block):\n",
    "    \"\"\"Model for sentence (pair) classification task with BERT.\n",
    "\n",
    "    The model feeds token ids and token type ids into BERT to get the\n",
    "    pooled BERT sequence representation, then apply a Dense layer for\n",
    "    classification. Does this also for an adversarial classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    bert: BERTModel\n",
    "        Bidirectional encoder with transformer.\n",
    "    num_classes : int, default is 2\n",
    "        The number of target classes.\n",
    "    dropout : float or None, default 0.0.\n",
    "        Dropout probability for the bert output.\n",
    "    prefix : str or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    params : ParameterDict or None\n",
    "        See document of `mx.gluon.Block`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 num_classes=2,\n",
    "                 dropout=0.0,\n",
    "                 prefix=None,\n",
    "                 params=None):\n",
    "        super(BERTProEpiClassifier, self).__init__(prefix=prefix, params=params)\n",
    "        self.bert = bert\n",
    "        with self.name_scope():\n",
    "            self.classifier = nn.HybridSequential(prefix=prefix)\n",
    "            if dropout:\n",
    "                self.classifier.add(nn.Dropout(rate=dropout))\n",
    "            self.classifier.add(nn.Dense(units=num_classes))\n",
    "\n",
    "    def forward(self,\n",
    "                inputs,\n",
    "                token_types,\n",
    "                valid_length=None,\n",
    "                inputs_epi=None,\n",
    "                token_types_epi=None,\n",
    "                valid_length_epi=None):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Generate the unnormalized scores for the given the input sequences.\n",
    "        From both classifiers (classifier + adversarial_classifier).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : NDArray, shape (batch_size, seq_length)\n",
    "            Input words for the sequences.\n",
    "        token_types : NDArray, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one.\n",
    "        valid_length : NDArray or None, shape (batch_size)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "        inputs_epi : NDArray or None, shape (batch_size, seq_length)\n",
    "            Input words for the sequences. If None then same as inputs.\n",
    "        token_types_epi : NDArray or None, shape (batch_size, seq_length)\n",
    "            Token types for the sequences, used to indicate whether the word belongs to the\n",
    "            first sentence or the second one. If None then same as token_types.\n",
    "        valid_length_epi : NDArray or None, shape (batch_size)\n",
    "            Valid length of the sequence. This is used to mask the padded tokens.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : NDArray\n",
    "            Shape (batch_size, num_classes), outputs of classifier.\n",
    "        \"\"\"\n",
    "        # if inputs_epi is None and token_types_epi is None:\n",
    "        #     inputs_epi = inputs\n",
    "        #     token_types_epi = token_types\n",
    "        #     valid_length_epi = valid_length\n",
    "\n",
    "        _, pooler_out = self.bert(inputs, token_types, valid_length)\n",
    "        _, pooler_out_epi = self.bert(inputs_epi, token_types_epi, valid_length_epi)\n",
    "        pooler_concat = mx.nd.concat(pooler_out, pooler_out_epi, dim=1)\n",
    "        return self.classifier(pooler_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.465997Z",
     "start_time": "2019-07-20T10:16:03.450040Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_bert():\n",
    "    # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "    ctx = mx.gpu(0)\n",
    "    # ctx = [mx.gpu(i) for i in range(2)]\n",
    "    # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "    # ctx = mx.cpu()\n",
    "\n",
    "    bert_base, vocabulary = nlp.model.get_model(\n",
    "        'bert_12_768_12',\n",
    "        dataset_name='book_corpus_wiki_en_uncased',\n",
    "        pretrained=True,\n",
    "        ctx=ctx,\n",
    "        use_pooler=True,\n",
    "        use_decoder=False,\n",
    "        use_classifier=False)\n",
    "    print(bert_base)\n",
    "\n",
    "    #model = BERTProEpiClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "    model = BERTProEpiClassifier(bert_base, num_classes=1, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    #loss_function = gluon.loss.SoftmaxCELoss()\n",
    "    loss_function = gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "\n",
    "    # use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    # maximum sequence length\n",
    "    # max_len = 128  # + batch_size: 32\n",
    "    # 384 - 12\n",
    "    max_len = 512  # + batch_size: 6 ?\n",
    "    # the labels for the two classes\n",
    "    #all_labels = [\"0\", \"1\"]\n",
    "    all_labels = [0, 1]\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    transform = FirstAndLastPartBERTDatasetTransform(bert_tokenizer,\n",
    "                                                     max_len,\n",
    "                                                     labels=all_labels,\n",
    "                                                     label_dtype='int32',\n",
    "                                                     pad=True,\n",
    "                                                     pair=True)\n",
    "\n",
    "    return model, vocabulary, ctx, bert_tokenizer, transform, loss_function, metric, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.484049Z",
     "start_time": "2019-07-20T10:16:03.467808Z"
    },
    "code_folding": [
     0,
     6
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def transform_dataset(X, y, transform):\n",
    "    data_train_raw = MyBERTDataset(X, y)\n",
    "    data_train = data_train_raw.transform(transform)\n",
    "    return data_train_raw, data_train\n",
    "\n",
    "\n",
    "def predict_out_to_ys(all_predictions, all_labels):\n",
    "    y_true, y_pred = list(), list()\n",
    "\n",
    "    for _, y_true_many, y_pred_many in all_predictions:\n",
    "        y_true_many = y_true_many.T[0].asnumpy()\n",
    "        # https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss\n",
    "        # pred: the prediction tensor, where the batch_axis dimension ranges over batch size and axis dimension ranges over the number of classes.\n",
    "        #y_pred_many = np.argmax(y_pred_many, axis=1).asnumpy()\n",
    "        y_pred_many = y_pred_many.asnumpy()\n",
    "\n",
    "        y_true.extend(list(y_true_many))\n",
    "        y_pred.extend(list(y_pred_many))\n",
    "        # TODO: convert label_id to label?\n",
    "        # y_pred.extend(all_labels[c] for c in list(y_pred_many))\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-GPU?\n",
    "- https://gluon.mxnet.io/chapter07_distributed-learning/multiple-gpus-gluon.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.514168Z",
     "start_time": "2019-07-20T10:16:03.488471Z"
    },
    "code_folding": [
     0,
     11,
     12,
     16,
     19,
     27,
     36,
     89
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          data_train,\n",
    "          ctx,\n",
    "          metric,\n",
    "          loss_function,\n",
    "          batch_size=32,\n",
    "          lr=5e-6,\n",
    "          num_epochs=3,\n",
    "          sw=None,\n",
    "          checkpoint_dir=\"data\",\n",
    "          use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(), 'adam', {\n",
    "            'learning_rate': lr,\n",
    "            'epsilon': 1e-9\n",
    "        })\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [\n",
    "            p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "        ]\n",
    "\n",
    "    log_interval = 500\n",
    "    global_step = 0\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                    epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(\n",
    "                        checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                global_step = epoch_id * len(bert_dataloader)\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               token_ids_epi, valid_length_epi,\n",
    "                               segment_ids_epi,\n",
    "                               label) in enumerate(tqdm(bert_dataloader)):\n",
    "                    global_step += 1\n",
    "                    with mx.autograd.record():\n",
    "                        # load data to GPU\n",
    "                        token_ids = token_ids.as_in_context(ctx)\n",
    "                        valid_length = valid_length.as_in_context(ctx)\n",
    "                        segment_ids = segment_ids.as_in_context(ctx)\n",
    "                        token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "                        valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "                        segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "                        label = label.as_in_context(ctx)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = model(token_ids, segment_ids,\n",
    "                                    valid_length.astype('float32'),\n",
    "                                    token_ids_epi, segment_ids_epi,\n",
    "                                    valid_length_epi.astype('float32'))\n",
    "                        label = label.astype('float32')\n",
    "                        ls = loss_function(out, label).mean()\n",
    "\n",
    "                    # backward computation\n",
    "                    ls.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    step_loss += ls.asscalar()\n",
    "                    out = out.sigmoid().round().astype('int32')\n",
    "                    label = label.astype('int32')\n",
    "                    metric.update([label], [out])\n",
    "                    stats.append((metric.get()[1], ls.asscalar()))\n",
    "\n",
    "                    if sw:\n",
    "                        sw.add_scalar(tag='T-ls', value=ls.asscalar(), global_step=global_step)\n",
    "                        sw.add_scalar(tag='T-acc', value=metric.get()[1], global_step=global_step)\n",
    "\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(\n",
    "                                epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                step_loss / log_interval,\n",
    "                                trainer.learning_rate,\n",
    "                                metric.get()[1],\n",
    "                                datetime.timedelta(seconds=(time.time() -\n",
    "                                                            t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def train_multi(model,\n",
    "                data_train,\n",
    "                ctx,\n",
    "                metric,\n",
    "                loss_function,\n",
    "                batch_size=32,\n",
    "                lr=5e-6,\n",
    "                num_epochs=3,\n",
    "                checkpoint_dir=\"data\",\n",
    "                use_checkpoints=True):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(),\n",
    "                                'adam', {\n",
    "                                    'learning_rate': lr,\n",
    "                                    'epsilon': 1e-9\n",
    "                                },\n",
    "                                update_on_kvstore=False)\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [\n",
    "            p for p in model.collect_params().values() if p.grad_req != 'null'\n",
    "        ]\n",
    "\n",
    "    log_interval = 500\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            if use_checkpoints:\n",
    "                epoch_checkpoint_savefile = \"bert.model.checkpoint{}.params\".format(\n",
    "                    epoch_id)\n",
    "                if checkpoint_dir is not None:\n",
    "                    epoch_checkpoint_savefile = os.path.join(\n",
    "                        checkpoint_dir, epoch_checkpoint_savefile)\n",
    "                if os.path.exists(epoch_checkpoint_savefile):\n",
    "                    model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                    print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                    continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               token_ids_epi, valid_length_epi,\n",
    "                               segment_ids_epi,\n",
    "                               label) in enumerate(bert_dataloader):\n",
    "                    with mx.autograd.record():\n",
    "                        # load data to GPU\n",
    "                        token_ids = gluon.utils.split_and_load(\n",
    "                            token_ids, ctx, even_split=False)\n",
    "                        valid_length = gluon.utils.split_and_load(\n",
    "                            valid_length, ctx, even_split=False)\n",
    "                        segment_ids = gluon.utils.split_and_load(\n",
    "                            segment_ids, ctx, even_split=False)\n",
    "                        token_ids_epi = gluon.utils.split_and_load(\n",
    "                            token_ids_epi, ctx, even_split=False)\n",
    "                        valid_length_epi = gluon.utils.split_and_load(\n",
    "                            valid_length_epi, ctx, even_split=False)\n",
    "                        segment_ids_epi = gluon.utils.split_and_load(\n",
    "                            segment_ids_epi, ctx, even_split=False)\n",
    "                        label = gluon.utils.split_and_load(label,\n",
    "                                                           ctx,\n",
    "                                                           even_split=False)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = [\n",
    "                            model(t1, s1, v1.astype('float32'), t2, s2,\n",
    "                                  v2.astype('float32'))\n",
    "                            for t1, s1, v1, t2, s2, v2 in zip(\n",
    "                                token_ids, segment_ids, valid_length,\n",
    "                                token_ids_epi, segment_ids_epi,\n",
    "                                valid_length_epi)\n",
    "                        ]\n",
    "                        ls = [\n",
    "                            loss_function(o, l.astype('float32')).mean()\n",
    "                            for o, l in zip(out, label)\n",
    "                        ]\n",
    "\n",
    "                    # backward computation\n",
    "                    for l in ls:\n",
    "                        l.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    for l in ls:\n",
    "                        step_loss += l.asscalar()\n",
    "                    for o, l in zip(out, label):\n",
    "                        metric.update([l.astype('int32')],\n",
    "                                      [o.sigmoid().round().astype('int32')])\n",
    "                    stats.append((metric.get()[1], [l.asscalar() for l in ls]))\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(\n",
    "                                epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                step_loss / log_interval,\n",
    "                                trainer.learning_rate,\n",
    "                                metric.get()[1],\n",
    "                                datetime.timedelta(seconds=(time.time() -\n",
    "                                                            t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            if use_checkpoints:\n",
    "                model.save_parameters(epoch_checkpoint_savefile)\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.526712Z",
     "start_time": "2019-07-20T10:16:03.516019Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def predict(model, data_predict, ctx, metric, loss_function, batch_size=32, sw=None):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict,\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "    all_predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        metric.reset()\n",
    "        cum_loss = 0\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, token_ids_epi,\n",
    "                       valid_length_epi, segment_ids_epi,\n",
    "                       label) in enumerate(tqdm(bert_dataloader)):\n",
    "            global_step = batch_id\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "            valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "            segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'),\n",
    "                        token_ids_epi, segment_ids_epi,\n",
    "                        valid_length_epi.astype('float32'))\n",
    "            label = label.astype('float32')\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "            out = out.sigmoid().round().astype('int32')\n",
    "            label = label.astype('int32')\n",
    "            metric.update([label], [out])\n",
    "            cum_loss += ls.asscalar()  # .sum() ?\n",
    "\n",
    "            if sw:\n",
    "                sw.add_scalar(tag='P-ls', value=ls.asscalar(), global_step=global_step)\n",
    "                sw.add_scalar(tag='P-acc', value=metric.get()[1], global_step=global_step)\n",
    "\n",
    "            all_predictions.append((batch_id, label, out))\n",
    "\n",
    "    return all_predictions, cum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.544386Z",
     "start_time": "2019-07-20T10:16:03.528155Z"
    },
    "code_folding": [
     0,
     6
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def predict_unknown(model, data_predict, ctx, label_map=None, batch_size=32):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict,\n",
    "                                               batch_size=batch_size)\n",
    "\n",
    "    predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        for batch_id, (token_ids, valid_length, segment_ids, token_ids_epi,\n",
    "                       valid_length_epi,\n",
    "                       segment_ids_epi) in enumerate(tqdm(bert_dataloader)):\n",
    "            global_step = batch_id\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            token_ids_epi = token_ids_epi.as_in_context(ctx)\n",
    "            valid_length_epi = valid_length_epi.as_in_context(ctx)\n",
    "            segment_ids_epi = segment_ids_epi.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'),\n",
    "                        token_ids_epi, segment_ids_epi,\n",
    "                        valid_length_epi.astype('float32'))\n",
    "\n",
    "            # to binary: 0/1\n",
    "            out = out.sigmoid().round().astype('int32')\n",
    "            # to numpy (not mxnet)\n",
    "            out = out.asnumpy()\n",
    "            # get mapping type\n",
    "            if label_map:\n",
    "                out = [label_map[c] for c in list(out)]\n",
    "\n",
    "            predictions.extend(out)\n",
    "\n",
    "    # list to numpy array\n",
    "    predictions = np.array(predictions)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.562697Z",
     "start_time": "2019-07-20T10:16:03.546083Z"
    },
    "code_folding": [
     0,
     24
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def print_infos(vocabulary, data_train_raw, data_train):\n",
    "    sample_id = 0\n",
    "\n",
    "    # sentence a\n",
    "    print(data_train_raw[sample_id][0])\n",
    "    # sentence b\n",
    "    print(data_train_raw[sample_id][1])\n",
    "    # 1 means equivalent, 0 means not equivalent\n",
    "    print(data_train_raw[sample_id][2])\n",
    "\n",
    "    print('vocabulary used for tokenization = \\n%s' % vocabulary)\n",
    "    print('[PAD] token id = %s' % (vocabulary['[PAD]']))\n",
    "    print('[CLS] token id = %s' % (vocabulary['[CLS]']))\n",
    "    print('[SEP] token id = %s' % (vocabulary['[SEP]']))\n",
    "\n",
    "    print('token ids = \\n%s' % data_train[sample_id][0])\n",
    "    print('valid length = \\n%s' % data_train[sample_id][1])\n",
    "    print('segment ids = \\n%s' % data_train[sample_id][2])\n",
    "    print('epi token ids = \\n%s' % data_train[sample_id][3])\n",
    "    print('epi valid length = \\n%s' % data_train[sample_id][4])\n",
    "    print('epi segment ids = \\n%s' % data_train[sample_id][5])\n",
    "    print('label = \\n%s' % data_train[sample_id][6])\n",
    "\n",
    "\n",
    "def plot_train_stats(stats):\n",
    "    if not stats:\n",
    "        print(\"no stats to plot\")\n",
    "        return\n",
    "\n",
    "    x = np.arange(len(stats))  # arange/linspace\n",
    "\n",
    "    acc_dots, loss_dots = zip(*stats)\n",
    "    # if isinstance(loss_dots, tuple):\n",
    "    #     loss_dots, loss_dots2 = zip(*loss_dots)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, acc_dots)  # Linie: '-', 'o-', '.-'\n",
    "    plt.title('Training BERTClassifier')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, loss_dots)\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:03.579800Z",
     "start_time": "2019-07-20T10:16:03.564875Z"
    },
    "code_folding": [
     0,
     12
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def heatconmat(y_test, y_pred):\n",
    "    sns.set_context('talk')\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred),\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                cbar=False,\n",
    "                cmap='gist_earth_r',\n",
    "                yticklabels=sorted(np.unique(y_test)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def report_training_results(y_test, y_pred, name=None, heatmap=True):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    if heatmap:\n",
    "        heatconmat(y_test, y_pred)\n",
    "    print()\n",
    "    print('Accuracy: ', round(accuracy_score(y_test, y_pred), 2), '\\n')  #\n",
    "\n",
    "    print('Report{}:'.format(\"\" if not name else \" for [{}]\".format(name)))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    f1_dic = {}\n",
    "    f1_dic['macro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='macro'), 2)\n",
    "    f1_dic['micro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='micro'), 2)\n",
    "    return f1_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Within topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#within_traindev_df = within_traindev_df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:19.432871Z",
     "start_time": "2019-07-20T10:16:19.401418Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [1 - test/train split]: 0:00:00.026057\n"
     ]
    }
   ],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:22.929102Z",
     "start_time": "2019-07-20T10:16:20.693081Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTModel(\n",
      "  (encoder): BERTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (1): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (2): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (3): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (4): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (5): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (6): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (7): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (8): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (9): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (10): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (11): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): HybridSequential(\n",
      "    (0): Embedding(30522 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (token_type_embed): HybridSequential(\n",
      "    (0): Embedding(2 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n",
      "Time for [2 - setup BERT model]: 0:00:02.231379\n"
     ]
    }
   ],
   "source": [
    "# 2. setup\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:34.738597Z",
     "start_time": "2019-07-20T10:16:31.875865Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fewer women would have abortions if they knew what they were doing bernard nathanson, former abortion doctor turned pro-life, in his book aborting america, 1979 - \"fewer women would have abortions if wombs had windows.\"[18] after the abortion, women are confronted with a much more profound sense of the reality of what they have done and what they have lost. this often triggers depression.\n",
      "abortion deprives couples that want to adopt of a potential child.\n",
      "0\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[PAD]', '[CLS]', '[SEP]', '[MASK]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2  8491  2308  2052  2031 11324  2015  2065  2027  2354  2054  2027\n",
      "  2020  2725  6795  7150  3385  1010  2280 11324  3460  2357  4013  1011\n",
      "  2166  1010  1999  2010  2338 11113 11589  2075  2637  1010  3245  1011\n",
      "  1000  8491  2308  2052  2031 11324  2015  2065 26578  2015  2018  3645\n",
      "  1012  1000  1031  2324  1033  2044  1996 11324  1010  2308  2024 12892\n",
      "  2007  1037  2172  2062 13769  3168  1997  1996  4507  1997  2054  2027\n",
      "  2031  2589  1998  2054  2027  2031  2439  1012  2023  2411 27099  6245\n",
      "  1012     3 11324  2139 18098 24653  6062  2008  2215  2000 11092  1997\n",
      "  1037  4022  2775  1012     3     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "101\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "epi token ids = \n",
      "[    2  8491  2308  2052  2031 11324  2015  2065  2027  2354  2054  2027\n",
      "  2020  2725  6795  7150  3385  1010  2280 11324  3460  2357  4013  1011\n",
      "  2166  1010  1999  2010  2338 11113 11589  2075  2637  1010  3245  1011\n",
      "  1000  8491  2308  2052  2031 11324  2015  2065 26578  2015  2018  3645\n",
      "  1012  1000  1031  2324  1033  2044  1996 11324  1010  2308  2024 12892\n",
      "  2007  1037  2172  2062 13769  3168  1997  1996  4507  1997  2054  2027\n",
      "  2031  2589  1998  2054  2027  2031  2439  1012  2023  2411 27099  6245\n",
      "  1012     3 11324  2139 18098 24653  6062  2008  2215  2000 11092  1997\n",
      "  1037  4022  2775  1012     3     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "epi valid length = \n",
      "101\n",
      "epi segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[0]\n",
      "Time for [3 - prepare training data]: 0:00:02.857111\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    print_infos(vocabulary, data_train_raw, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T10:16:43.434923Z",
     "start_time": "2019-07-20T10:16:43.213198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/within_traindev_proepi512_BCE_0.1’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "! mkdir data/within_traindev_proepi512_BCE_0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T15:11:36.894745Z",
     "start_time": "2019-07-20T10:17:16.595569Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57189899379f4428aafff1e767df243d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=57512), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [setup training]: 0:04:07.773317\n",
      "loaded checkpoint for epoch 0\n",
      "loaded checkpoint for epoch 1\n",
      "loaded checkpoint for epoch 2\n",
      "loaded checkpoint for epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92da0a9d62644aaeaf5da5eef4fc072e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=28760), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 12:21:28,336 : INFO : successfully opened events file: data/within_traindev_proepi512_BCE_0.1/events.out.tfevents.1563618088.cuda\n",
      "2019-07-20 12:21:28,338 : INFO : wrote 1 event to disk\n",
      "2019-07-20 12:21:28,339 : INFO : wrote 1 event to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 500/28760] loss=0.0653, lr=0.0000050, acc=0.983 - time 0:05:02.111346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 12:31:28,689 : INFO : wrote 1988 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 1000/28760] loss=0.0788, lr=0.0000050, acc=0.983 - time 0:05:02.276349\n",
      "[Epoch 4 Batch 1500/28760] loss=0.0474, lr=0.0000050, acc=0.984 - time 0:05:01.794535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 12:41:29,169 : INFO : wrote 1990 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 2000/28760] loss=0.0979, lr=0.0000050, acc=0.983 - time 0:05:01.756761\n",
      "[Epoch 4 Batch 2500/28760] loss=0.0718, lr=0.0000050, acc=0.982 - time 0:05:02.417710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 12:51:29,237 : INFO : wrote 1982 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 3000/28760] loss=0.1086, lr=0.0000050, acc=0.981 - time 0:05:03.141269\n",
      "[Epoch 4 Batch 3500/28760] loss=0.1108, lr=0.0000050, acc=0.981 - time 0:05:02.922105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 13:01:29,724 : INFO : wrote 1984 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 4000/28760] loss=0.0548, lr=0.0000050, acc=0.981 - time 0:05:02.306779\n",
      "[Epoch 4 Batch 4500/28760] loss=0.0669, lr=0.0000050, acc=0.982 - time 0:05:02.580927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 13:11:30,305 : INFO : wrote 1984 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 5000/28760] loss=0.1153, lr=0.0000050, acc=0.982 - time 0:05:02.941597\n",
      "[Epoch 4 Batch 5500/28760] loss=0.0417, lr=0.0000050, acc=0.983 - time 0:05:03.012856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 13:21:30,583 : INFO : wrote 1982 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 6000/28760] loss=0.0872, lr=0.0000050, acc=0.983 - time 0:05:02.604438\n",
      "[Epoch 4 Batch 6500/28760] loss=0.0655, lr=0.0000050, acc=0.983 - time 0:05:02.058273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 13:31:31,010 : INFO : wrote 1984 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 7000/28760] loss=0.0812, lr=0.0000050, acc=0.983 - time 0:05:03.185566\n",
      "[Epoch 4 Batch 7500/28760] loss=0.0567, lr=0.0000050, acc=0.983 - time 0:05:02.485660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 13:41:31,441 : INFO : wrote 1984 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 8000/28760] loss=0.0798, lr=0.0000050, acc=0.983 - time 0:05:02.807687\n",
      "[Epoch 4 Batch 8500/28760] loss=0.0870, lr=0.0000050, acc=0.983 - time 0:05:02.852985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 13:51:31,596 : INFO : wrote 1982 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 9000/28760] loss=0.0390, lr=0.0000050, acc=0.984 - time 0:05:02.643972\n",
      "[Epoch 4 Batch 9500/28760] loss=0.1006, lr=0.0000050, acc=0.983 - time 0:05:02.563690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 14:01:31,997 : INFO : wrote 1986 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 10000/28760] loss=0.0875, lr=0.0000050, acc=0.983 - time 0:05:02.225374\n",
      "[Epoch 4 Batch 10500/28760] loss=0.0635, lr=0.0000050, acc=0.983 - time 0:05:02.674031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 14:11:32,089 : INFO : wrote 1984 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 11000/28760] loss=0.0966, lr=0.0000050, acc=0.983 - time 0:05:02.167872\n",
      "[Epoch 4 Batch 11500/28760] loss=0.0335, lr=0.0000050, acc=0.983 - time 0:05:02.977575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 14:21:32,579 : INFO : wrote 1984 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 12000/28760] loss=0.0407, lr=0.0000050, acc=0.984 - time 0:05:02.494989\n",
      "[Epoch 4 Batch 12500/28760] loss=0.0986, lr=0.0000050, acc=0.984 - time 0:05:02.497300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 14:31:33,043 : INFO : wrote 1988 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 13000/28760] loss=0.0527, lr=0.0000050, acc=0.984 - time 0:05:01.530436\n",
      "[Epoch 4 Batch 13500/28760] loss=0.0810, lr=0.0000050, acc=0.984 - time 0:05:01.941817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 14:41:33,591 : INFO : wrote 1986 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 14000/28760] loss=0.0906, lr=0.0000050, acc=0.984 - time 0:05:02.983331\n",
      "[Epoch 4 Batch 14500/28760] loss=0.0527, lr=0.0000050, acc=0.984 - time 0:05:02.636554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 14:51:33,694 : INFO : wrote 1982 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 15000/28760] loss=0.0941, lr=0.0000050, acc=0.983 - time 0:05:02.885985\n",
      "[Epoch 4 Batch 15500/28760] loss=0.0554, lr=0.0000050, acc=0.984 - time 0:05:02.958115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 15:01:34,033 : INFO : wrote 1982 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 16000/28760] loss=0.0590, lr=0.0000050, acc=0.984 - time 0:05:02.927725\n",
      "[Epoch 4 Batch 16500/28760] loss=0.0967, lr=0.0000050, acc=0.984 - time 0:05:03.021551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 15:11:34,442 : INFO : wrote 1982 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 17000/28760] loss=0.1026, lr=0.0000050, acc=0.984 - time 0:05:02.463968\n",
      "[Epoch 4 Batch 17500/28760] loss=0.1177, lr=0.0000050, acc=0.983 - time 0:05:03.012037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 15:21:34,587 : INFO : wrote 1982 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 18000/28760] loss=0.0919, lr=0.0000050, acc=0.983 - time 0:05:02.987185\n",
      "[Epoch 4 Batch 18500/28760] loss=0.0790, lr=0.0000050, acc=0.983 - time 0:05:03.028482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 15:31:34,590 : INFO : wrote 1980 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 19000/28760] loss=0.1018, lr=0.0000050, acc=0.983 - time 0:05:02.580769\n",
      "[Epoch 4 Batch 19500/28760] loss=0.0904, lr=0.0000050, acc=0.983 - time 0:05:02.365107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 15:41:34,731 : INFO : wrote 1986 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 20000/28760] loss=0.1546, lr=0.0000050, acc=0.983 - time 0:05:02.369304\n",
      "[Epoch 4 Batch 20500/28760] loss=0.0848, lr=0.0000050, acc=0.983 - time 0:05:02.649543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 15:51:35,237 : INFO : wrote 1982 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 21000/28760] loss=0.0744, lr=0.0000050, acc=0.983 - time 0:05:02.948426\n",
      "[Epoch 4 Batch 21500/28760] loss=0.1056, lr=0.0000050, acc=0.983 - time 0:05:03.211777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 16:01:35,269 : INFO : wrote 1982 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 22000/28760] loss=0.1058, lr=0.0000050, acc=0.983 - time 0:05:02.112074\n",
      "[Epoch 4 Batch 22500/28760] loss=0.0892, lr=0.0000050, acc=0.983 - time 0:05:03.192492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 16:11:35,381 : INFO : wrote 1982 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 23000/28760] loss=0.1222, lr=0.0000050, acc=0.983 - time 0:05:02.599636\n",
      "[Epoch 4 Batch 23500/28760] loss=0.0570, lr=0.0000050, acc=0.983 - time 0:05:02.824275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 16:21:35,569 : INFO : wrote 1984 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 24000/28760] loss=0.0714, lr=0.0000050, acc=0.983 - time 0:05:02.301148\n",
      "[Epoch 4 Batch 24500/28760] loss=0.0383, lr=0.0000050, acc=0.983 - time 0:05:02.836298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 16:31:35,916 : INFO : wrote 1982 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 25000/28760] loss=0.1433, lr=0.0000050, acc=0.983 - time 0:05:03.143276\n",
      "[Epoch 4 Batch 25500/28760] loss=0.0643, lr=0.0000050, acc=0.983 - time 0:05:02.887986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 16:41:36,032 : INFO : wrote 1982 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 26000/28760] loss=0.0767, lr=0.0000050, acc=0.983 - time 0:05:02.585352\n",
      "[Epoch 4 Batch 26500/28760] loss=0.1172, lr=0.0000050, acc=0.983 - time 0:05:02.154772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 16:51:36,092 : INFO : wrote 1984 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 27000/28760] loss=0.1096, lr=0.0000050, acc=0.983 - time 0:05:02.657932\n",
      "[Epoch 4 Batch 27500/28760] loss=0.0470, lr=0.0000050, acc=0.983 - time 0:05:02.649858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 17:01:36,375 : INFO : wrote 1984 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4 Batch 28000/28760] loss=0.0731, lr=0.0000050, acc=0.983 - time 0:05:02.987318\n",
      "[Epoch 4 Batch 28500/28760] loss=0.0839, lr=0.0000050, acc=0.983 - time 0:05:02.373948\n",
      "\n",
      "Time for [epoch 4]: 4:50:06.663526\n",
      "Time for [training]: 4:50:10.072344\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 17:11:36,891 : INFO : wrote 1975 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [4 - train model]: 4:54:20.292070\n"
     ]
    }
   ],
   "source": [
    "run_name = \"within_traindev_proepi512_BCE_0.1\"\n",
    "with Timer(\"4 - train model\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=600) as sw:\n",
    "    stats = train(model, data_train, ctx, metric, loss_function, batch_size=2, lr=5e-6, num_epochs=5, sw=sw, checkpoint_dir=\"data/\" + run_name)\n",
    "    model.save_parameters(\"data/\" + run_name + \"/bert.model.params\")\n",
    "\n",
    "    plot_train_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T15:11:38.604408Z",
     "start_time": "2019-07-20T15:11:38.261069Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abortion opens the door to the sexual exploitation of women the existence of abortion gives men a little more of a safeguard against unintentionally impregnating a woman. as a result, men will be more aggressive in their sexual exploitation of women.\n",
      "the fact that a child is likely to have a short life does not justify further shortening it:\n",
      "0\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[PAD]', '[CLS]', '[SEP]', '[MASK]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2 11324  7480  1996  2341  2000  1996  4424 14427  1997  2308  1996\n",
      "  4598  1997 11324  3957  2273  1037  2210  2062  1997  1037 28805  2114\n",
      "  4895 18447  4765 19301  2135 17727  2890 16989  3436  1037  2450  1012\n",
      "  2004  1037  2765  1010  2273  2097  2022  2062  9376  1999  2037  4424\n",
      " 14427  1997  2308  1012     3  1996  2755  2008  1037  2775  2003  3497\n",
      "  2000  2031  1037  2460  2166  2515  2025 16114  2582  2460  7406  2009\n",
      "  1024     3     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "74\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "epi token ids = \n",
      "[    2 11324  7480  1996  2341  2000  1996  4424 14427  1997  2308  1996\n",
      "  4598  1997 11324  3957  2273  1037  2210  2062  1997  1037 28805  2114\n",
      "  4895 18447  4765 19301  2135 17727  2890 16989  3436  1037  2450  1012\n",
      "  2004  1037  2765  1010  2273  2097  2022  2062  9376  1999  2037  4424\n",
      " 14427  1997  2308  1012     3  1996  2755  2008  1037  2775  2003  3497\n",
      "  2000  2031  1037  2460  2166  2515  2025 16114  2582  2460  7406  2009\n",
      "  1024     3     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "epi valid length = \n",
      "74\n",
      "epi segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[0]\n",
      "Time for [5 - prepare eval data]: 0:00:00.340571\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T15:19:26.132398Z",
     "start_time": "2019-07-20T15:11:39.911309Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c0c533cd4343a0a6d167888f2f606f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3196), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 17:11:40,075 : INFO : successfully opened events file: data/within_traindev_proepi512_BCE_0.1/events.out.tfevents.1563635500.cuda\n",
      "2019-07-20 17:11:40,086 : INFO : wrote 1 event to disk\n",
      "2019-07-20 17:11:40,087 : INFO : wrote 1 event to disk\n",
      "2019-07-20 17:12:40,222 : INFO : wrote 836 events to disk\n",
      "2019-07-20 17:13:40,226 : INFO : wrote 816 events to disk\n",
      "2019-07-20 17:14:40,247 : INFO : wrote 810 events to disk\n",
      "2019-07-20 17:15:40,287 : INFO : wrote 824 events to disk\n",
      "2019-07-20 17:16:40,376 : INFO : wrote 818 events to disk\n",
      "2019-07-20 17:17:40,420 : INFO : wrote 814 events to disk\n",
      "2019-07-20 17:18:40,434 : INFO : wrote 858 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [prediction]: 0:07:45.549373\n",
      "Accuracy: 0.926302613049601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-20 17:19:26,129 : INFO : wrote 615 events to disk\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[2768  191]\n",
      " [ 280 3152]]\n",
      "\n",
      "Accuracy:  0.93 \n",
      "\n",
      "Report for [BERTClassifier - BCE prolog+epilog 0.1 split]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92      2959\n",
      "           1       0.94      0.92      0.93      3432\n",
      "\n",
      "    accuracy                           0.93      6391\n",
      "   macro avg       0.93      0.93      0.93      6391\n",
      "weighted avg       0.93      0.93      0.93      6391\n",
      "\n",
      "Time for [6 - evaluate]: 0:07:46.217231\n"
     ]
    }
   ],
   "source": [
    "run_name = \"within_traindev_proepi512_BCE_0.1\"\n",
    "with Timer(\"6 - evaluate\"), SummaryWriter(logdir=\"data/\" + run_name, flush_secs=60) as sw:\n",
    "    # model.load_parameters(\"data/\" + run_name + \"/bert.model.params\", ctx=ctx)\n",
    "    # bert.model.checkpoint4.params\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2, sw=sw)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier - BCE prolog+epilog 0.1 split\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)):\n",
    "        # stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=epoch_id + 1)\n",
    "        stats = train(model, data_train, ctx, metric, loss_function, batch_size=2, lr=5e-6, num_epochs=epoch_id + 1)  # seq_len: 512\n",
    "        # stats = train_multi(model, data_train, ctx, metric, loss_function, batch_size=4, lr=5e-6, num_epochs=epoch_id + 1)  # seq_len: 512\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)):\n",
    "        # all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function)\n",
    "        all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)  # seq_len: 512\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true, y_pred, name=\"BERTClassifier - last part\", heatmap=False)\n",
    "\n",
    "    model.save_parameters(\"data/bert.model.params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T15:19:40.183629Z",
     "start_time": "2019-07-20T15:19:40.163975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [1 - test/train split]: 0:00:00.016970\n"
     ]
    }
   ],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(cross_traindev_df, ratio=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. setup\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    print_infos(vocabulary, data_train_raw, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "with Timer(\"4 - train model\"):\n",
    "    # train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=3)\n",
    "    stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=2)\n",
    "    # model.save_parameters(\"data/same-side-classification/cross-topic/bert.model.params\")\n",
    "    model.save_parameters(\"data/bert.model.params\")\n",
    "\n",
    "    plot_train_stats(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T15:19:54.599834Z",
     "start_time": "2019-07-20T15:19:54.270467Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alcoholism and drug-use are common after abortions.\n",
      "uncertainty over whether fetuses are \"life\" should halt abortions.\n",
      "1\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[PAD]', '[CLS]', '[SEP]', '[MASK]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2 25519  1998  4319  1011  2224  2024  2691  2044 11324  2015  1012\n",
      "     3 12503  2058  3251 10768  5809  2229  2024  1000  2166  1000  2323\n",
      "  9190 11324  2015  1012     3     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "29\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "epi token ids = \n",
      "[    2 25519  1998  4319  1011  2224  2024  2691  2044 11324  2015  1012\n",
      "     3 12503  2058  3251 10768  5809  2229  2024  1000  2166  1000  2323\n",
      "  9190 11324  2015  1012     3     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "epi valid length = \n",
      "29\n",
      "epi segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[1]\n",
      "Time for [5 - prepare eval data]: 0:00:00.326569\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-20T15:27:36.774664Z",
     "start_time": "2019-07-20T15:20:08.639530Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809d0d76643040c1aaf6775e297f4fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3053), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time for [prediction]: 0:07:27.403377\n",
      "Accuracy: 0.9646191646191646\n",
      "Confusion Matrix:\n",
      "[[2931   93]\n",
      " [ 123 2958]]\n",
      "\n",
      "Accuracy:  0.96 \n",
      "\n",
      "Report for [BERTClassifier within->cross 0.1 split]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96      3024\n",
      "           1       0.97      0.96      0.96      3081\n",
      "\n",
      "    accuracy                           0.96      6105\n",
      "   macro avg       0.96      0.96      0.96      6105\n",
      "weighted avg       0.96      0.96      0.96      6105\n",
      "\n",
      "Time for [6 - evaluate]: 0:07:28.131418\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"6 - evaluate\"):\n",
    "    # model.load_parameters(\"data/same-side-classification/cross-topic/bert.model.params\", ctx=ctx)\n",
    "    #model.load_parameters(\"data/bert.model.params\", ctx=ctx)\n",
    "    # load model from \"within\" to evaluate with \"cross\" test-data\n",
    "    #model.load_parameters('data/within_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)\n",
    "    # model.load_parameters('data/cross_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier within->cross 0.1 split\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     2,
     14,
     22
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch_id in range(5):\n",
    "    with Timer(\"4 - train model - {}\".format(epoch_id)):\n",
    "        stats = train(model,\n",
    "                      data_train,\n",
    "                      ctx,\n",
    "                      metric,\n",
    "                      loss_function,\n",
    "                      batch_size=2,\n",
    "                      lr=5e-6,\n",
    "                      num_epochs=epoch_id + 1,\n",
    "                      checkpoint_dir='data/cross_traindev_proepi512_BCE')\n",
    "        plot_train_stats(stats)\n",
    "\n",
    "    with Timer(\"6 - evaluate - {}\".format(epoch_id)):\n",
    "        all_predictions, cum_loss = predict(model,\n",
    "                                            data_dev,\n",
    "                                            ctx,\n",
    "                                            metric,\n",
    "                                            loss_function,\n",
    "                                            batch_size=2)\n",
    "        print(\"Accuracy in epoch {}:\".format(epoch_id), metric.get()[1])\n",
    "        y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "        report_training_results(y_true,\n",
    "                                y_pred,\n",
    "                                name=\"BERTClassifier\",\n",
    "                                heatmap=False)\n",
    "\n",
    "    model.save_parameters(\n",
    "        \"data/cross_traindev_proepi512_BCE/bert.model.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"11 - test/train split\"):\n",
    "    # evaluate on \"within\" test-data\n",
    "    _, X_dev, _, y_dev = get_train_test_sets(within_traindev_df)\n",
    "\n",
    "with Timer(\"12 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)\n",
    "\n",
    "with Timer(\"13 - evaluate\"):\n",
    "    # model from \"cross\"\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier cross with within\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test Cross-Model with Within-Test\n",
    "\n",
    "5 epochs of cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_parameters('data/cross_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(within_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "report_training_results(y_true, y_pred, name=\"BERTClassifier cross with within\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:24:48.940295\n",
    "Accuracy: 0.8536330916488446\n",
    "Confusion Matrix:\n",
    "[[7659 1174]\n",
    " [1632 8706]]\n",
    "\n",
    "Accuracy:  0.85 \n",
    "\n",
    "Report for [BERTClassifier cross with within]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.82      0.87      0.85      8833\n",
    "           1       0.88      0.84      0.86     10338\n",
    "\n",
    "    accuracy                           0.85     19171\n",
    "   macro avg       0.85      0.85      0.85     19171\n",
    "weighted avg       0.85      0.85      0.85     19171\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Within-Model with Cross-Test\n",
    "\n",
    "5 epochs of within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_parameters('data/within_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(cross_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"evaluate within with cross\"):\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier within with cross\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:22:17.542674\n",
    "Accuracy: 0.9379197379197379\n",
    "Confusion Matrix:\n",
    "[[8397  539]\n",
    " [ 598 8781]]\n",
    "\n",
    "Accuracy:  0.94 \n",
    "\n",
    "Report for [BERTClassifier]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.93      0.94      0.94      8936\n",
    "           1       0.94      0.94      0.94      9379\n",
    "\n",
    "    accuracy                           0.94     18315\n",
    "   macro avg       0.94      0.94      0.94     18315\n",
    "weighted avg       0.94      0.94      0.94     18315\n",
    "\n",
    "Time for [6 - evaluate]: 0:22:19.841677\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Within-Model with Within-Test\n",
    "\n",
    "5 epochs of within"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_parameters('data/within_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(within_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Timer(\"evaluate within with within\"):\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier within with within\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:19:51.733113\n",
    "Accuracy: 0.9069427781545042\n",
    "Confusion Matrix:\n",
    "[[7972  861]\n",
    " [ 923 9415]]\n",
    "\n",
    "Accuracy:  0.91 \n",
    "\n",
    "Report for [BERTClassifier within with within]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.90      0.90      0.90      8833\n",
    "           1       0.92      0.91      0.91     10338\n",
    "\n",
    "    accuracy                           0.91     19171\n",
    "   macro avg       0.91      0.91      0.91     19171\n",
    "weighted avg       0.91      0.91      0.91     19171\n",
    "\n",
    "Time for [evaluate within with cross]: 0:19:52.352049\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Cross-Model with Cross-Test\n",
    "\n",
    "5 epochs of cross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_parameters('data/cross_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(cross_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "report_training_results(y_true, y_pred, name=\"BERTClassifier cross\", heatmap=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Time for [prediction]: 0:23:28.845010\n",
    "Accuracy: 0.9197925197925197\n",
    "Confusion Matrix:\n",
    "[[8329  607]\n",
    " [ 862 8517]]\n",
    "\n",
    "Accuracy:  0.92 \n",
    "\n",
    "Report for [BERTClassifier cross]:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.91      0.93      0.92      8936\n",
    "           1       0.93      0.91      0.92      9379\n",
    "\n",
    "    accuracy                           0.92     18315\n",
    "   macro avg       0.92      0.92      0.92     18315\n",
    "weighted avg       0.92      0.92      0.92     18315\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details to wrong classified arguments\n",
    "\n",
    "within_traindev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()\n",
    "\n",
    "model.load_parameters('data/within_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_dev, _, y_dev = get_train_test_sets(within_traindev_df)\n",
    "\n",
    "data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "# print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function, batch_size=2)\n",
    "print(\"Accuracy:\", metric.get()[1])\n",
    "\n",
    "y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "report_training_results(y_true, y_pred, name=\"BERTClassifier within-within\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to dataframe\n",
    "dev_pred_df = pd.DataFrame(data=y_pred, columns=[\"prediction\"], dtype=\"bool\")\n",
    "\n",
    "# merge all dataframes\n",
    "dev_df = X_dev.join(y_dev)\n",
    "dev_df = dev_df.reset_index()\n",
    "dev_df = pd.merge(dev_df, dev_pred_df, left_index=True, right_index=True, how='inner')\n",
    "dev_df.set_index('id', inplace=True)\n",
    "\n",
    "# re-apply tag value\n",
    "dev_df = dev_df.progress_apply(add_tag, axis=1)\n",
    "# info\n",
    "dev_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "dev_df_ser_file = \"data/within_traindev_proepi512_BCE/eval_dev_df.pickle\"\n",
    "\n",
    "\n",
    "with open(dev_df_ser_file, \"wb\") as f:\n",
    "    pickle.dump(dev_df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(dev_df_ser_file, \"rb\") as f:\n",
    "    dev_df = pickle.load(f)\n",
    "\n",
    "\n",
    "dev_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPFN_df = dev_df[(dev_df['is_same_side'] != dev_df['prediction'])]  #  and (dev_df['tag'] != 'abortion')\n",
    "FPFN_df.info()\n",
    "FPFN_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     6
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import re\n",
    "#import tabulate\n",
    "#display(HTML(tabulate.tabulate(table, tablefmt='html')))\n",
    "\n",
    "\n",
    "def print_args(df, idx, add_linebreaks=True):\n",
    "    row = df.iloc[idx]\n",
    "    print('IDX: {}, tag: {}, topics: {}'.format(idx, row['tag'], row['topic']))\n",
    "    print('Is-Same-Side: {}'.format(row['is_same_side']))\n",
    "\n",
    "    arg1 = row['argument1']\n",
    "    arg2 = row['argument2']\n",
    "    if add_linebreaks:\n",
    "        pat = re.compile(r'(?P<c>(\\.|\\?|\\!|\\:)+\\\"?)')\n",
    "        arg1 = pat.sub(r'\\1<br/>', arg1)\n",
    "        arg2 = pat.sub(r'\\1<br/>', arg2)\n",
    "\n",
    "    display(HTML('''<table>\n",
    "        <tr>\n",
    "            <td style=\"border-right:1px dashed black;\">{arg1}</td>\n",
    "            <td>{arg2}</td>\n",
    "        </tr>\n",
    "    </table>'''.format(arg1=arg1, arg2=arg2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = {print_args(FPFN_df, i) for i in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# tokenizer from BERT\n",
    "def tokenize_arguments(row):\n",
    "    # tokenize\n",
    "    row['argument1_tokens'] = tokenizer(row['argument1'])\n",
    "    row['argument2_tokens'] = tokenizer(row['argument2'])\n",
    "\n",
    "    # count tokens\n",
    "    row['argument1_len'] = len(row['argument1_tokens'])\n",
    "    row['argument2_len'] = len(row['argument2_tokens'])\n",
    "    # token number diff\n",
    "    row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "    row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff'])\n",
    "    return row\n",
    "\n",
    "\n",
    "FPFN_df = FPFN_df.progress_apply(tokenize_arguments, axis=1)\n",
    "FPFN_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPFN_df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make final results/predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, vocabulary, ctx, tokenizer, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_parameters('data/within_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)\n",
    "# model.load_parameters('data/cross_traindev_proepi512_BCE/bert.model.checkpoint4.params', ctx=ctx)\n",
    "model.load_parameters('data/within_traindev_proepi512_BCE_0.1/bert.model.checkpoint4.params', ctx=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = within_test_df[['argument1', 'argument2', 'topic']]\n",
    "#X_pred = cross_test_df[['argument1', 'argument2', 'topic']]\n",
    "y_pred = None\n",
    "\n",
    "data_pred_raw, data_pred = transform_dataset(X_pred, y_pred, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# data_pred_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# label_map=all_labels\n",
    "predictions = predict_unknown(model, data_pred, ctx, label_map=None, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(data_pred) == len(predictions) == len(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to dataframe\n",
    "# bool works because we mapped 0 to False, 1 to True, is default conversion\n",
    "test_pred_df = pd.DataFrame(data=predictions, columns=[\"prediction\"], dtype=\"bool\")\n",
    "\n",
    "# merge all dataframes\n",
    "# test_df = X_pred.join(y_pred)\n",
    "test_df = X_pred.reset_index()\n",
    "test_df = pd.merge(test_df, test_pred_df, left_index=True, right_index=True, how='inner')\n",
    "test_df.set_index('id', inplace=True)\n",
    "\n",
    "# re-apply tag value\n",
    "test_df = test_df.progress_apply(add_tag, axis=1)\n",
    "# info\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# ser_fn = \"data/within_traindev_proepi512_BCE/within_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/cross_traindev_proepi512_BCE/cross_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/cross_traindev_proepi512_BCE/within_with_cross_model_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/within_traindev_proepi512_BCE/cross_with_within_model_test_pred_df.pickle\"\n",
    "ser_fn = \"data/within_traindev_proepi512_BCE_0.1/within_test_pred_df.pickle\"\n",
    "# ser_fn = \"data/within_traindev_proepi512_BCE_0.1/cross_with_within_model_test_pred_df.pickle\"\n",
    "\n",
    "with open(ser_fn, \"wb\") as f:\n",
    "    pickle.dump(test_df, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(test_df.itertuples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_fn = \"data/within_traindev_proepi512_BCE/within_results.csv\"\n",
    "# res_fn = \"data/cross_traindev_proepi512_BCE/cross_results.csv\"\n",
    "# res_fn = \"data/cross_traindev_proepi512_BCE/within_with_cross_model_results.csv\"\n",
    "# res_fn = \"data/within_traindev_proepi512_BCE/cross_with_within_model_results.csv\"\n",
    "res_fn = \"data/within_traindev_proepi512_BCE_0.1/within_results.csv\"\n",
    "# res_fn = \"data/within_traindev_proepi512_BCE_0.1/cross_with_within_model_results.csv\"\n",
    "\n",
    "with open(res_fn, \"w\") as of:\n",
    "    of.write('\"id\",\"label\"\\n')\n",
    "    for row_id, row in test_df.iterrows():\n",
    "        of.write('{},\"{}\"\\n'.format(row_id, str(row['prediction'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: do this for within and cross !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test read\n",
    "temp_test_df = pd.read_csv(\"data/within_traindev_proepi512_BCE_0.1/cross_with_within_model_results.csv\", index_col='id')\n",
    "temp_test_df.info()\n",
    "temp_test_df.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
