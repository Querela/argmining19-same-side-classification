{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:13.830545Z",
     "start_time": "2019-12-08T11:04:12.781089Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import csv\n",
    "import gluonnlp as nlp\n",
    "import matplotlib.pyplot as plt\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from bert import *\n",
    "from mxboard import SummaryWriter\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon.data import Dataset, SimpleDataset\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:13.838528Z",
     "start_time": "2019-12-08T11:04:13.835865Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:13.936051Z",
     "start_time": "2019-12-08T11:04:13.930761Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:14.499197Z",
     "start_time": "2019-12-08T11:04:14.494265Z"
    }
   },
   "outputs": [],
   "source": [
    "# set repeatable random state\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:15.246737Z",
     "start_time": "2019-12-08T11:04:15.242741Z"
    }
   },
   "outputs": [],
   "source": [
    "# apply progress bars for pandas .apply() -> .progress_apply()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:16.094116Z",
     "start_time": "2019-12-08T11:04:16.063420Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4ac3e6d7f9413f84c3a16223c371bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# make tqdm jupyter friendly\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "# for .progress_apply() we have to hack it like this?\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:16.874374Z",
     "start_time": "2019-12-08T11:04:16.864598Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        time_end = time.time()\n",
    "        time_delta = datetime.timedelta(seconds=(time_end - self.time_start))\n",
    "        if self.name:\n",
    "            print((\"Time for [{}]: {}\".format(self.name, time_delta)))\n",
    "        else:\n",
    "            print((\"Time: {}\".format(time_delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:18.562325Z",
     "start_time": "2019-12-08T11:04:18.558772Z"
    }
   },
   "outputs": [],
   "source": [
    "fn_art_eval = \"data/artificial_evalset/artificial_evalset.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:19.317897Z",
     "start_time": "2019-12-08T11:04:19.309649Z"
    }
   },
   "outputs": [],
   "source": [
    "artificial_evalset_df = pd.DataFrame.from_csv(fn_art_eval, sep='\\t', index_col=None)\n",
    "\n",
    "new_cols = artificial_evalset_df.columns.to_list()\n",
    "new_cols[2] = \"type\"\n",
    "artificial_evalset_df.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:20.194492Z",
     "start_time": "2019-12-08T11:04:19.912949Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def fix_cols(row):\n",
    "    row[\"argument1_id\"] = row['arg_id']\n",
    "    row[\"argument2_id\"] = \"{}-{}\".format(row['arg_id'], row['type'])\n",
    "    row[\"topic\"] = \"gay marriage\"\n",
    "    return row\n",
    "\n",
    "artificial_evalset_df = artificial_evalset_df.apply(fix_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:20.676408Z",
     "start_time": "2019-12-08T11:04:20.558929Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def add_tag(row):\n",
    "    row[\"tag\"] = \"gay marriage\"\n",
    "    return row\n",
    "\n",
    "artificial_evalset_df = artificial_evalset_df.apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:22.104545Z",
     "start_time": "2019-12-08T11:04:21.308977Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "ctx = mx.cpu()\n",
    "_, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                    dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                    pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                    use_decoder=False, use_classifier=False)\n",
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "tokenizer = bert_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:22.994892Z",
     "start_time": "2019-12-08T11:04:22.223234Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9601fd95cd94832b4cf338c61e1b069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=175), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tokenizer from BERT\n",
    "def tokenize_arguments(row):\n",
    "    # tokenize\n",
    "    row['argument1_tokens'] = tokenizer(row['argument1'])\n",
    "    row['argument2_tokens'] = tokenizer(row['argument2'])\n",
    "\n",
    "    # count tokens\n",
    "    row['argument1_len'] = len(row['argument1_tokens'])\n",
    "    row['argument2_len'] = len(row['argument2_tokens'])\n",
    "    # token number diff\n",
    "    row['argument12_len_sum'] = row['argument1_len'] + row['argument2_len']\n",
    "    row['argument12_len_sum_half'] = row['argument12_len_sum'] / 2\n",
    "    row['argument12_len_diff'] = row['argument1_len'] - row['argument2_len']\n",
    "    row['argument12_len_diff_abs'] = np.abs(row['argument12_len_diff'])\n",
    "    return row\n",
    "\n",
    "\n",
    "artificial_evalset_df = artificial_evalset_df.progress_apply(tokenize_arguments, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:23.821468Z",
     "start_time": "2019-12-08T11:04:23.033433Z"
    },
    "code_folding": [
     5
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcec9378967e4c0ba6173a886dac501c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=175), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# nltk.download('punct')\n",
    "\n",
    "\n",
    "# tokenizer from BERT\n",
    "def sentenize_arguments(row):\n",
    "    # tokenize\n",
    "    row['argument1_sentences'] = sent_tokenize(row['argument1'])\n",
    "    row['argument2_sentences'] = sent_tokenize(row['argument2'])\n",
    "\n",
    "    # count tokens\n",
    "    row['argument1_sent_num'] = len(row['argument1_sentences'])\n",
    "    row['argument2_sent_num'] = len(row['argument2_sentences'])\n",
    "    # token number diff\n",
    "    row['argument12_sent_num_sum'] = row['argument1_sent_num'] + row['argument2_sent_num']\n",
    "    row['argument12_sent_num_sum_half'] = row['argument12_sent_num_sum'] / 2\n",
    "    row['argument12_sent_num_diff'] = row['argument1_sent_num'] - row['argument2_sent_num']\n",
    "    row['argument12_sent_num_diff_abs'] = np.abs(row['argument12_sent_num_diff'])\n",
    "    return row\n",
    "\n",
    "\n",
    "artificial_evalset_df = artificial_evalset_df.progress_apply(sentenize_arguments, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:27.208117Z",
     "start_time": "2019-12-08T11:04:27.183016Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_overview(df, task='same-side', class_name='is_same_side'):\n",
    "    # Total instance numbers\n",
    "    total = len(df)\n",
    "    print(\"Task: \", task)\n",
    "    print('=' * 40, '\\n')\n",
    "\n",
    "    print('Total instances: ', total)\n",
    "    print('\\n')\n",
    "\n",
    "    print('For each topic:')\n",
    "    for tag, tag_df in df.groupby(['tag']):\n",
    "        print(tag, ': ', len(tag_df), ' instances')\n",
    "        print('')\n",
    "        print('\\t\\tUnique argument1:', len(tag_df['argument1'].unique()))\n",
    "        print('\\t\\tUnique argument2:', len(tag_df['argument2'].unique()))\n",
    "        arguments = np.concatenate([tag_df['argument1'].values, tag_df['argument2'].values])\n",
    "        print('\\t\\tUnique total arguments:', len(set(list(arguments))), '\\n')\n",
    "        if class_name in df.columns:\n",
    "            for is_same_side, side_df in tag_df.groupby([class_name]):\n",
    "                print('\\t\\t', is_same_side, ': ', len(side_df), ' instances')\n",
    "    print('\\n')\n",
    "\n",
    "    if class_name in df.columns:\n",
    "        print('For each class value:')\n",
    "        for class_value, class_df in df.groupby([class_name]):\n",
    "            print(class_value, ': ', len(class_df), ' instances')\n",
    "            print('\\t\\tUnique argument1:', len(class_df['argument1'].unique()))\n",
    "            print('\\t\\tUnique argument2:', len(class_df['argument2'].unique()))\n",
    "            arguments = np.concatenate([class_df['argument1'].values, class_df['argument2'].values])\n",
    "            print('\\t\\tUnique total arguments:', len(set(list(arguments))), '\\n')\n",
    "        print('\\n')\n",
    "\n",
    "    print('Unique argument1:', len(df['argument1'].unique()))\n",
    "    print('Unique argument2:', len(df['argument2'].unique()))\n",
    "    arguments = df['argument1'].values\n",
    "    arguments = np.concatenate([arguments, df['argument2'].values])\n",
    "\n",
    "    print('Unique total arguments:', len(set(list(arguments))), '\\n')\n",
    "\n",
    "    print('-' * 40, '\\n')\n",
    "\n",
    "    arguments_length_lst = [x for x in df['argument1_len'].values]\n",
    "    arguments_length_lst.extend([x for x in df['argument2_len'].values])\n",
    "    print('Words:')\n",
    "    print('\\tshortest argument:', min(arguments_length_lst), ' words')\n",
    "    print('\\tlongest argument:', max(arguments_length_lst), ' words')\n",
    "    print('\\targument average length:', np.mean(arguments_length_lst),\n",
    "          ' words')\n",
    "\n",
    "    arguments_sent_length_lst = [x for x in df['argument1_sent_num'].values]\n",
    "    arguments_sent_length_lst.extend([x for x in df['argument2_sent_num'].values])\n",
    "    print('Sentences:')\n",
    "    print('\\tshortest argument:', min(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\tlongest argument:', max(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\targument average length:', np.mean(arguments_sent_length_lst),\n",
    "          ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:28.994257Z",
     "start_time": "2019-12-08T11:04:28.962879Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task:  same-side artificial evalset\n",
      "======================================== \n",
      "\n",
      "Total instances:  175\n",
      "\n",
      "\n",
      "For each topic:\n",
      "gay marriage :  175  instances\n",
      "\n",
      "\t\tUnique argument1: 25\n",
      "\t\tUnique argument2: 174\n",
      "\t\tUnique total arguments: 199 \n",
      "\n",
      "\t\t False :  100  instances\n",
      "\t\t True :  75  instances\n",
      "\n",
      "\n",
      "For each class value:\n",
      "False :  100  instances\n",
      "\t\tUnique argument1: 25\n",
      "\t\tUnique argument2: 100\n",
      "\t\tUnique total arguments: 125 \n",
      "\n",
      "True :  75  instances\n",
      "\t\tUnique argument1: 25\n",
      "\t\tUnique argument2: 74\n",
      "\t\tUnique total arguments: 99 \n",
      "\n",
      "\n",
      "\n",
      "Unique argument1: 25\n",
      "Unique argument2: 174\n",
      "Unique total arguments: 199 \n",
      "\n",
      "---------------------------------------- \n",
      "\n",
      "Words:\n",
      "\tshortest argument: 7  words\n",
      "\tlongest argument: 137  words\n",
      "\targument average length: 25.597142857142856  words\n",
      "Sentences:\n",
      "\tshortest argument: 1  sentences\n",
      "\tlongest argument: 5  sentences\n",
      "\targument average length: 1.3314285714285714  sentences\n"
     ]
    }
   ],
   "source": [
    "get_overview(artificial_evalset_df, task=\"same-side artificial evalset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:33.932081Z",
     "start_time": "2019-12-08T11:04:33.884321Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arg_id</th>\n",
       "      <th>argument1_id</th>\n",
       "      <th>argument1_len</th>\n",
       "      <th>argument2_len</th>\n",
       "      <th>argument12_len_sum</th>\n",
       "      <th>argument12_len_sum_half</th>\n",
       "      <th>argument12_len_diff</th>\n",
       "      <th>argument12_len_diff_abs</th>\n",
       "      <th>argument1_sent_num</th>\n",
       "      <th>argument2_sent_num</th>\n",
       "      <th>argument12_sent_num_sum</th>\n",
       "      <th>argument12_sent_num_sum_half</th>\n",
       "      <th>argument12_sent_num_diff</th>\n",
       "      <th>argument12_sent_num_diff_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>175.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>26.240000</td>\n",
       "      <td>24.954286</td>\n",
       "      <td>51.194286</td>\n",
       "      <td>25.597143</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>8.520000</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>1.382857</td>\n",
       "      <td>2.662857</td>\n",
       "      <td>1.331429</td>\n",
       "      <td>-0.102857</td>\n",
       "      <td>0.182857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.231794</td>\n",
       "      <td>7.231794</td>\n",
       "      <td>26.187423</td>\n",
       "      <td>16.712174</td>\n",
       "      <td>41.693007</td>\n",
       "      <td>20.846504</td>\n",
       "      <td>13.850954</td>\n",
       "      <td>10.977427</td>\n",
       "      <td>0.724291</td>\n",
       "      <td>0.814201</td>\n",
       "      <td>1.479959</td>\n",
       "      <td>0.739980</td>\n",
       "      <td>0.429839</td>\n",
       "      <td>0.402211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>-26.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>-6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>122.500000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           arg_id  argument1_id  argument1_len  argument2_len  \\\n",
       "count  175.000000    175.000000     175.000000     175.000000   \n",
       "mean    13.000000     13.000000      26.240000      24.954286   \n",
       "std      7.231794      7.231794      26.187423      16.712174   \n",
       "min      1.000000      1.000000       7.000000       8.000000   \n",
       "25%      7.000000      7.000000      15.000000      15.500000   \n",
       "50%     13.000000     13.000000      19.000000      20.000000   \n",
       "75%     19.000000     19.000000      32.000000      27.000000   \n",
       "max     25.000000     25.000000     137.000000     108.000000   \n",
       "\n",
       "       argument12_len_sum  argument12_len_sum_half  argument12_len_diff  \\\n",
       "count          175.000000               175.000000           175.000000   \n",
       "mean            51.194286                25.597143             1.285714   \n",
       "std             41.693007                20.846504            13.850954   \n",
       "min             15.000000                 7.500000           -26.000000   \n",
       "25%             30.000000                15.000000            -6.000000   \n",
       "50%             39.000000                19.500000            -1.000000   \n",
       "75%             54.000000                27.000000             5.000000   \n",
       "max            245.000000               122.500000            72.000000   \n",
       "\n",
       "       argument12_len_diff_abs  argument1_sent_num  argument2_sent_num  \\\n",
       "count               175.000000          175.000000          175.000000   \n",
       "mean                  8.520000            1.280000            1.382857   \n",
       "std                  10.977427            0.724291            0.814201   \n",
       "min                   0.000000            1.000000            1.000000   \n",
       "25%                   2.000000            1.000000            1.000000   \n",
       "50%                   6.000000            1.000000            1.000000   \n",
       "75%                  10.000000            1.000000            1.000000   \n",
       "max                  72.000000            4.000000            5.000000   \n",
       "\n",
       "       argument12_sent_num_sum  argument12_sent_num_sum_half  \\\n",
       "count               175.000000                    175.000000   \n",
       "mean                  2.662857                      1.331429   \n",
       "std                   1.479959                      0.739980   \n",
       "min                   2.000000                      1.000000   \n",
       "25%                   2.000000                      1.000000   \n",
       "50%                   2.000000                      1.000000   \n",
       "75%                   3.000000                      1.500000   \n",
       "max                   9.000000                      4.500000   \n",
       "\n",
       "       argument12_sent_num_diff  argument12_sent_num_diff_abs  \n",
       "count                175.000000                    175.000000  \n",
       "mean                  -0.102857                      0.182857  \n",
       "std                    0.429839                      0.402211  \n",
       "min                   -2.000000                      0.000000  \n",
       "25%                    0.000000                      0.000000  \n",
       "50%                    0.000000                      0.000000  \n",
       "75%                    0.000000                      0.000000  \n",
       "max                    1.000000                      2.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artificial_evalset_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:38.768941Z",
     "start_time": "2019-12-08T11:04:38.764525Z"
    }
   },
   "outputs": [],
   "source": [
    "names_columns_X = ['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']\n",
    "names_columns_X2 = ['argument1', 'argument2', 'tag']\n",
    "names_columns_y = ['is_same_side']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:41.459634Z",
     "start_time": "2019-12-08T11:04:41.449145Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compute_metrics(conf_mat, precision=3, dump=True):\n",
    "    conf_mat = np.array(conf_mat)\n",
    "    tn, fp, fn, tp = conf_mat.ravel()\n",
    "\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    prec = tp / (tp + fp)\n",
    "    rec  = tp / (tp + fn)\n",
    "    f1 = 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "    if dump:\n",
    "        print(\"{:>10}: {:.{prec}f}\".format(\"accuracy\", acc, prec=precision))\n",
    "        print(\"{:>10}: {:.{prec}f}\".format(\"precision\", prec, prec=precision))\n",
    "        print(\"{:>10}: {:.{prec}f}\".format(\"recall\", rec, prec=precision))\n",
    "        print(\"{:>10}: {:.{prec}f}\".format(\"f1-score\", f1, prec=precision))\n",
    "\n",
    "    return prec, rec, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:42.231735Z",
     "start_time": "2019-12-08T11:04:42.218505Z"
    },
    "code_folding": [
     0,
     12
    ]
   },
   "outputs": [],
   "source": [
    "def heatconmat(y_test, y_pred):\n",
    "    sns.set_context('talk')\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred),\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                cbar=False,\n",
    "                cmap='gist_earth_r',\n",
    "                yticklabels=sorted(np.unique(y_test)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def report_training_results(y_test, y_pred, name=None, heatmap=True):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print()\n",
    "    compute_metrics(confusion_matrix(y_test, y_pred))\n",
    "    if heatmap:\n",
    "        heatconmat(y_test, y_pred)\n",
    "    print()\n",
    "    print('Accuracy: ', round(accuracy_score(y_test, y_pred), 2), '\\n')  #\n",
    "\n",
    "    print('Report{}:'.format(\"\" if not name else \" for [{}]\".format(name)))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    f1_dic = {}\n",
    "    f1_dic['macro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='macro'), 2)\n",
    "    f1_dic['micro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='micro'), 2)\n",
    "    return f1_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:43.251880Z",
     "start_time": "2019-12-08T11:04:43.248318Z"
    }
   },
   "outputs": [],
   "source": [
    "fn = \"data/artificial_evalset/artificial_evalset.pred.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:04:47.114672Z",
     "start_time": "2019-12-08T11:04:47.096964Z"
    }
   },
   "outputs": [],
   "source": [
    "artificial_evalset_df = pd.DataFrame.from_csv(fn, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:15:16.675860Z",
     "start_time": "2019-12-08T11:15:16.650497Z"
    },
    "code_folding": [
     22
    ]
   },
   "outputs": [],
   "source": [
    "# invert NEG:\n",
    "# abs(v - 1) -- v is value (0|1)\n",
    "\n",
    "\n",
    "def dump_art_eval_results(artificial_evalset_df):\n",
    "    cols = [c for c in artificial_evalset_df.columns.tolist() if c.startswith(\"preds-\")]\n",
    "\n",
    "    for col in cols:\n",
    "        model_name = col[6:]\n",
    "        print(\"#\" * 60)\n",
    "        print(\"Model:\", model_name)\n",
    "        print(\"#\" * 60)\n",
    "        print()\n",
    "        \n",
    "        labels = artificial_evalset_df[\"is_same_side\"].values\n",
    "        preds = artificial_evalset_df[col].values\n",
    "        print(\"All (uninverted NEG labels):\")\n",
    "        compute_metrics(confusion_matrix(labels, preds))\n",
    "        print()\n",
    "        \n",
    "        labels, preds = zip(*[(l, p) if l == 1 else (1, abs(p - 1)) for l, p in zip(labels, preds)])\n",
    "        conf_mat = confusion_matrix(labels, preds)\n",
    "        print(\"All:\")            \n",
    "        compute_metrics(conf_mat)\n",
    "        print()\n",
    "\n",
    "        for crit, crit_df in artificial_evalset_df.groupby(\"type\"):\n",
    "            crit_df = crit_df[[\"is_same_side\", col]].astype({\"is_same_side\": \"int32\"})\n",
    "            labels = crit_df[\"is_same_side\"].values\n",
    "            preds = crit_df[col].values\n",
    "\n",
    "            if \"NEG\" in crit:\n",
    "                # invert values for conf_mat\n",
    "                labels = [abs(v - 1) for v in labels]\n",
    "                preds = [abs(v - 1) for v in preds]\n",
    "\n",
    "            conf_mat = confusion_matrix(labels, preds)\n",
    "            print(\"Criterion:\", crit)            \n",
    "            compute_metrics(conf_mat)\n",
    "            print()\n",
    "\n",
    "        # all negs\n",
    "        neg_cols = [c for c in artificial_evalset_df[\"type\"].unique().tolist() if \"NEG\" in c]\n",
    "        all_neg_df = artificial_evalset_df.loc[[\n",
    "            any(v) for v in zip(*[\n",
    "                artificial_evalset_df[\"type\"] == c for c in neg_cols\n",
    "            ])\n",
    "        ]]\n",
    "        labels = [abs(v - 1) for v in all_neg_df[\"is_same_side\"].values]\n",
    "        preds = [abs(v - 1) for v in all_neg_df[col].values]\n",
    "        conf_mat = confusion_matrix(labels, preds)\n",
    "        print(\"All negs:\", neg_cols)            \n",
    "        compute_metrics(conf_mat)\n",
    "        print()\n",
    "        \n",
    "        # all pos\n",
    "        pos_cols = [c for c in artificial_evalset_df[\"type\"].unique().tolist() if \"NEG\" not in c]\n",
    "        all_pos_df = artificial_evalset_df.loc[[\n",
    "            any(v) for v in zip(*[\n",
    "                artificial_evalset_df[\"type\"] == c for c in pos_cols\n",
    "            ])\n",
    "        ]]\n",
    "        labels = [v for v in all_pos_df[\"is_same_side\"].values]\n",
    "        preds = [v for v in all_pos_df[col].values]\n",
    "        conf_mat = confusion_matrix(labels, preds)\n",
    "        print(\"All pos:\", pos_cols)            \n",
    "        compute_metrics(conf_mat)\n",
    "        print()\n",
    "\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-08T11:15:17.307877Z",
     "start_time": "2019-12-08T11:15:17.106686Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "Model: cross_traindev_epi512_BCE_0.1\n",
      "############################################################\n",
      "\n",
      "All (uninverted NEG labels):\n",
      "  accuracy: 0.497\n",
      " precision: 0.447\n",
      "    recall: 0.733\n",
      "  f1-score: 0.556\n",
      "\n",
      "All:\n",
      "  accuracy: 0.497\n",
      " precision: 1.000\n",
      "    recall: 0.497\n",
      "  f1-score: 0.664\n",
      "\n",
      "Criterion: CIT\n",
      "  accuracy: 0.760\n",
      " precision: 1.000\n",
      "    recall: 0.760\n",
      "  f1-score: 0.864\n",
      "\n",
      "Criterion: CIT-NEG\n",
      "  accuracy: 0.320\n",
      " precision: 1.000\n",
      "    recall: 0.320\n",
      "  f1-score: 0.485\n",
      "\n",
      "Criterion: CON\n",
      "  accuracy: 0.920\n",
      " precision: 1.000\n",
      "    recall: 0.920\n",
      "  f1-score: 0.958\n",
      "\n",
      "Criterion: CON-NEG\n",
      "  accuracy: 0.320\n",
      " precision: 1.000\n",
      "    recall: 0.320\n",
      "  f1-score: 0.485\n",
      "\n",
      "Criterion: DIFF\n",
      "  accuracy: 0.520\n",
      " precision: 1.000\n",
      "    recall: 0.520\n",
      "  f1-score: 0.684\n",
      "\n",
      "Criterion: DIFF-NEG\n",
      "  accuracy: 0.440\n",
      " precision: 1.000\n",
      "    recall: 0.440\n",
      "  f1-score: 0.611\n",
      "\n",
      "Criterion: NEG\n",
      "  accuracy: 0.200\n",
      " precision: 1.000\n",
      "    recall: 0.200\n",
      "  f1-score: 0.333\n",
      "\n",
      "All negs: ['NEG', 'CON-NEG', 'DIFF-NEG', 'CIT-NEG']\n",
      "  accuracy: 0.320\n",
      " precision: 1.000\n",
      "    recall: 0.320\n",
      "  f1-score: 0.485\n",
      "\n",
      "All pos: ['CON', 'DIFF', 'CIT']\n",
      "  accuracy: 0.733\n",
      " precision: 1.000\n",
      "    recall: 0.733\n",
      "  f1-score: 0.846\n",
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "Model: within_traindev_epi128_BCE\n",
      "############################################################\n",
      "\n",
      "All (uninverted NEG labels):\n",
      "  accuracy: 0.469\n",
      " precision: 0.429\n",
      "    recall: 0.720\n",
      "  f1-score: 0.537\n",
      "\n",
      "All:\n",
      "  accuracy: 0.469\n",
      " precision: 1.000\n",
      "    recall: 0.469\n",
      "  f1-score: 0.638\n",
      "\n",
      "Criterion: CIT\n",
      "  accuracy: 0.680\n",
      " precision: 1.000\n",
      "    recall: 0.680\n",
      "  f1-score: 0.810\n",
      "\n",
      "Criterion: CIT-NEG\n",
      "  accuracy: 0.240\n",
      " precision: 1.000\n",
      "    recall: 0.240\n",
      "  f1-score: 0.387\n",
      "\n",
      "Criterion: CON\n",
      "  accuracy: 0.840\n",
      " precision: 1.000\n",
      "    recall: 0.840\n",
      "  f1-score: 0.913\n",
      "\n",
      "Criterion: CON-NEG\n",
      "  accuracy: 0.400\n",
      " precision: 1.000\n",
      "    recall: 0.400\n",
      "  f1-score: 0.571\n",
      "\n",
      "Criterion: DIFF\n",
      "  accuracy: 0.640\n",
      " precision: 1.000\n",
      "    recall: 0.640\n",
      "  f1-score: 0.780\n",
      "\n",
      "Criterion: DIFF-NEG\n",
      "  accuracy: 0.400\n",
      " precision: 1.000\n",
      "    recall: 0.400\n",
      "  f1-score: 0.571\n",
      "\n",
      "Criterion: NEG\n",
      "  accuracy: 0.080\n",
      " precision: 1.000\n",
      "    recall: 0.080\n",
      "  f1-score: 0.148\n",
      "\n",
      "All negs: ['NEG', 'CON-NEG', 'DIFF-NEG', 'CIT-NEG']\n",
      "  accuracy: 0.280\n",
      " precision: 1.000\n",
      "    recall: 0.280\n",
      "  f1-score: 0.438\n",
      "\n",
      "All pos: ['CON', 'DIFF', 'CIT']\n",
      "  accuracy: 0.720\n",
      " precision: 1.000\n",
      "    recall: 0.720\n",
      "  f1-score: 0.837\n",
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "Model: cross_traindev_epi128_BCE\n",
      "############################################################\n",
      "\n",
      "All (uninverted NEG labels):\n",
      "  accuracy: 0.480\n",
      " precision: 0.441\n",
      "    recall: 0.800\n",
      "  f1-score: 0.569\n",
      "\n",
      "All:\n",
      "  accuracy: 0.480\n",
      " precision: 1.000\n",
      "    recall: 0.480\n",
      "  f1-score: 0.649\n",
      "\n",
      "Criterion: CIT\n",
      "  accuracy: 0.720\n",
      " precision: 1.000\n",
      "    recall: 0.720\n",
      "  f1-score: 0.837\n",
      "\n",
      "Criterion: CIT-NEG\n",
      "  accuracy: 0.320\n",
      " precision: 1.000\n",
      "    recall: 0.320\n",
      "  f1-score: 0.485\n",
      "\n",
      "Criterion: CON\n",
      "  accuracy: 0.960\n",
      " precision: 1.000\n",
      "    recall: 0.960\n",
      "  f1-score: 0.980\n",
      "\n",
      "Criterion: CON-NEG\n",
      "  accuracy: 0.240\n",
      " precision: 1.000\n",
      "    recall: 0.240\n",
      "  f1-score: 0.387\n",
      "\n",
      "Criterion: DIFF\n",
      "  accuracy: 0.720\n",
      " precision: 1.000\n",
      "    recall: 0.720\n",
      "  f1-score: 0.837\n",
      "\n",
      "Criterion: DIFF-NEG\n",
      "  accuracy: 0.320\n",
      " precision: 1.000\n",
      "    recall: 0.320\n",
      "  f1-score: 0.485\n",
      "\n",
      "Criterion: NEG\n",
      "  accuracy: 0.080\n",
      " precision: 1.000\n",
      "    recall: 0.080\n",
      "  f1-score: 0.148\n",
      "\n",
      "All negs: ['NEG', 'CON-NEG', 'DIFF-NEG', 'CIT-NEG']\n",
      "  accuracy: 0.240\n",
      " precision: 1.000\n",
      "    recall: 0.240\n",
      "  f1-score: 0.387\n",
      "\n",
      "All pos: ['CON', 'DIFF', 'CIT']\n",
      "  accuracy: 0.800\n",
      " precision: 1.000\n",
      "    recall: 0.800\n",
      "  f1-score: 0.889\n",
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "Model: within_traindev_epi512_BCE\n",
      "############################################################\n",
      "\n",
      "All (uninverted NEG labels):\n",
      "  accuracy: 0.480\n",
      " precision: 0.443\n",
      "    recall: 0.827\n",
      "  f1-score: 0.577\n",
      "\n",
      "All:\n",
      "  accuracy: 0.480\n",
      " precision: 1.000\n",
      "    recall: 0.480\n",
      "  f1-score: 0.649\n",
      "\n",
      "Criterion: CIT\n",
      "  accuracy: 0.840\n",
      " precision: 1.000\n",
      "    recall: 0.840\n",
      "  f1-score: 0.913\n",
      "\n",
      "Criterion: CIT-NEG\n",
      "  accuracy: 0.200\n",
      " precision: 1.000\n",
      "    recall: 0.200\n",
      "  f1-score: 0.333\n",
      "\n",
      "Criterion: CON\n",
      "  accuracy: 0.920\n",
      " precision: 1.000\n",
      "    recall: 0.920\n",
      "  f1-score: 0.958\n",
      "\n",
      "Criterion: CON-NEG\n",
      "  accuracy: 0.280\n",
      " precision: 1.000\n",
      "    recall: 0.280\n",
      "  f1-score: 0.438\n",
      "\n",
      "Criterion: DIFF\n",
      "  accuracy: 0.720\n",
      " precision: 1.000\n",
      "    recall: 0.720\n",
      "  f1-score: 0.837\n",
      "\n",
      "Criterion: DIFF-NEG\n",
      "  accuracy: 0.280\n",
      " precision: 1.000\n",
      "    recall: 0.280\n",
      "  f1-score: 0.438\n",
      "\n",
      "Criterion: NEG\n",
      "  accuracy: 0.120\n",
      " precision: 1.000\n",
      "    recall: 0.120\n",
      "  f1-score: 0.214\n",
      "\n",
      "All negs: ['NEG', 'CON-NEG', 'DIFF-NEG', 'CIT-NEG']\n",
      "  accuracy: 0.220\n",
      " precision: 1.000\n",
      "    recall: 0.220\n",
      "  f1-score: 0.361\n",
      "\n",
      "All pos: ['CON', 'DIFF', 'CIT']\n",
      "  accuracy: 0.827\n",
      " precision: 1.000\n",
      "    recall: 0.827\n",
      "  f1-score: 0.905\n",
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "Model: within_traindev_pro128_BCE\n",
      "############################################################\n",
      "\n",
      "All (uninverted NEG labels):\n",
      "  accuracy: 0.497\n",
      " precision: 0.436\n",
      "    recall: 0.587\n",
      "  f1-score: 0.500\n",
      "\n",
      "All:\n",
      "  accuracy: 0.497\n",
      " precision: 1.000\n",
      "    recall: 0.497\n",
      "  f1-score: 0.664\n",
      "\n",
      "Criterion: CIT\n",
      "  accuracy: 0.560\n",
      " precision: 1.000\n",
      "    recall: 0.560\n",
      "  f1-score: 0.718\n",
      "\n",
      "Criterion: CIT-NEG\n",
      "  accuracy: 0.480\n",
      " precision: 1.000\n",
      "    recall: 0.480\n",
      "  f1-score: 0.649\n",
      "\n",
      "Criterion: CON\n",
      "  accuracy: 0.680\n",
      " precision: 1.000\n",
      "    recall: 0.680\n",
      "  f1-score: 0.810\n",
      "\n",
      "Criterion: CON-NEG\n",
      "  accuracy: 0.480\n",
      " precision: 1.000\n",
      "    recall: 0.480\n",
      "  f1-score: 0.649\n",
      "\n",
      "Criterion: DIFF\n",
      "  accuracy: 0.520\n",
      " precision: 1.000\n",
      "    recall: 0.520\n",
      "  f1-score: 0.684\n",
      "\n",
      "Criterion: DIFF-NEG\n",
      "  accuracy: 0.520\n",
      " precision: 1.000\n",
      "    recall: 0.520\n",
      "  f1-score: 0.684\n",
      "\n",
      "Criterion: NEG\n",
      "  accuracy: 0.240\n",
      " precision: 1.000\n",
      "    recall: 0.240\n",
      "  f1-score: 0.387\n",
      "\n",
      "All negs: ['NEG', 'CON-NEG', 'DIFF-NEG', 'CIT-NEG']\n",
      "  accuracy: 0.430\n",
      " precision: 1.000\n",
      "    recall: 0.430\n",
      "  f1-score: 0.601\n",
      "\n",
      "All pos: ['CON', 'DIFF', 'CIT']\n",
      "  accuracy: 0.587\n",
      " precision: 1.000\n",
      "    recall: 0.587\n",
      "  f1-score: 0.739\n",
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "Model: cross_traindev_pro128_BCE\n",
      "############################################################\n",
      "\n",
      "All (uninverted NEG labels):\n",
      "  accuracy: 0.457\n",
      " precision: 0.432\n",
      "    recall: 0.840\n",
      "  f1-score: 0.570\n",
      "\n",
      "All:\n",
      "  accuracy: 0.457\n",
      " precision: 1.000\n",
      "    recall: 0.457\n",
      "  f1-score: 0.627\n",
      "\n",
      "Criterion: CIT\n",
      "  accuracy: 0.800\n",
      " precision: 1.000\n",
      "    recall: 0.800\n",
      "  f1-score: 0.889\n",
      "\n",
      "Criterion: CIT-NEG\n",
      "  accuracy: 0.200\n",
      " precision: 1.000\n",
      "    recall: 0.200\n",
      "  f1-score: 0.333\n",
      "\n",
      "Criterion: CON\n",
      "  accuracy: 0.840\n",
      " precision: 1.000\n",
      "    recall: 0.840\n",
      "  f1-score: 0.913\n",
      "\n",
      "Criterion: CON-NEG\n",
      "  accuracy: 0.200\n",
      " precision: 1.000\n",
      "    recall: 0.200\n",
      "  f1-score: 0.333\n",
      "\n",
      "Criterion: DIFF\n",
      "  accuracy: 0.880\n",
      " precision: 1.000\n",
      "    recall: 0.880\n",
      "  f1-score: 0.936\n",
      "\n",
      "Criterion: DIFF-NEG\n",
      "  accuracy: 0.160\n",
      " precision: 1.000\n",
      "    recall: 0.160\n",
      "  f1-score: 0.276\n",
      "\n",
      "Criterion: NEG\n",
      "  accuracy: 0.120\n",
      " precision: 1.000\n",
      "    recall: 0.120\n",
      "  f1-score: 0.214\n",
      "\n",
      "All negs: ['NEG', 'CON-NEG', 'DIFF-NEG', 'CIT-NEG']\n",
      "  accuracy: 0.170\n",
      " precision: 1.000\n",
      "    recall: 0.170\n",
      "  f1-score: 0.291\n",
      "\n",
      "All pos: ['CON', 'DIFF', 'CIT']\n",
      "  accuracy: 0.840\n",
      " precision: 1.000\n",
      "    recall: 0.840\n",
      "  f1-score: 0.913\n",
      "\n",
      "\n",
      "\n",
      "############################################################\n",
      "Model: cross_traindev_epi512_BCE\n",
      "############################################################\n",
      "\n",
      "All (uninverted NEG labels):\n",
      "  accuracy: 0.446\n",
      " precision: 0.421\n",
      "    recall: 0.787\n",
      "  f1-score: 0.549\n",
      "\n",
      "All:\n",
      "  accuracy: 0.446\n",
      " precision: 1.000\n",
      "    recall: 0.446\n",
      "  f1-score: 0.617\n",
      "\n",
      "Criterion: CIT\n",
      "  accuracy: 0.760\n",
      " precision: 1.000\n",
      "    recall: 0.760\n",
      "  f1-score: 0.864\n",
      "\n",
      "Criterion: CIT-NEG\n",
      "  accuracy: 0.240\n",
      " precision: 1.000\n",
      "    recall: 0.240\n",
      "  f1-score: 0.387\n",
      "\n",
      "Criterion: CON\n",
      "  accuracy: 0.880\n",
      " precision: 1.000\n",
      "    recall: 0.880\n",
      "  f1-score: 0.936\n",
      "\n",
      "Criterion: CON-NEG\n",
      "  accuracy: 0.160\n",
      " precision: 1.000\n",
      "    recall: 0.160\n",
      "  f1-score: 0.276\n",
      "\n",
      "Criterion: DIFF\n",
      "  accuracy: 0.720\n",
      " precision: 1.000\n",
      "    recall: 0.720\n",
      "  f1-score: 0.837\n",
      "\n",
      "Criterion: DIFF-NEG\n",
      "  accuracy: 0.280\n",
      " precision: 1.000\n",
      "    recall: 0.280\n",
      "  f1-score: 0.438\n",
      "\n",
      "Criterion: NEG\n",
      "  accuracy: 0.080\n",
      " precision: 1.000\n",
      "    recall: 0.080\n",
      "  f1-score: 0.148\n",
      "\n",
      "All negs: ['NEG', 'CON-NEG', 'DIFF-NEG', 'CIT-NEG']\n",
      "  accuracy: 0.190\n",
      " precision: 1.000\n",
      "    recall: 0.190\n",
      "  f1-score: 0.319\n",
      "\n",
      "All pos: ['CON', 'DIFF', 'CIT']\n",
      "  accuracy: 0.787\n",
      " precision: 1.000\n",
      "    recall: 0.787\n",
      "  f1-score: 0.881\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dump_art_eval_results(artificial_evalset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
