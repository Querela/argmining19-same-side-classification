{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RATIO 2019 - Benchmarking Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gluon Data: https://gluon-nlp.mxnet.io/model_zoo/bert/index.html#bert-for-sentence-classification-on-glue-tasks\n",
    "\n",
    "```\n",
    "curl -L https://tinyurl.com/yaznh3os -o download_glue_data.py\n",
    "```\n",
    "\n",
    "https://gluon-nlp.mxnet.io/install.html\n",
    "\n",
    "```\n",
    "pip install --upgrade 'mxnet>=1.3.0'\n",
    "pip install gluonnlp\n",
    "wget https://gluon-nlp.mxnet.io/_downloads/sentence_embedding.zip\n",
    "unzip sentence_embedding.zip\n",
    "```\n",
    "\n",
    "https://pypi.org/project/pytorch-pretrained-bert/#installation\n",
    "\n",
    "```\n",
    "# nope\n",
    "# pip install pytorch-pretrained-bert\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:20:44.835699Z",
     "start_time": "2019-06-27T10:20:43.983106Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import gluonnlp as nlp\n",
    "from bert import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import utils\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:20:45.972748Z",
     "start_time": "2019-06-27T10:20:45.967692Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\n",
    "                    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:20:47.700680Z",
     "start_time": "2019-06-27T10:20:47.696829Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply progress bars for pandas .apply() -> .progress_apply()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:20:48.115176Z",
     "start_time": "2019-06-27T10:20:48.106394Z"
    },
    "code_folding": [
     0,
     4
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, name=None):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def __exit__(self, *exc):\n",
    "        time_end = time.time()\n",
    "        time_delta = datetime.timedelta(seconds=(time_end - self.time_start))\n",
    "        if self.name:\n",
    "            print((\"Time for [{}]: {}\".format(self.name, time_delta)))\n",
    "        else:\n",
    "            print((\"Time: {}\".format(time_delta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Same Side Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:20:50.382484Z",
     "start_time": "2019-06-27T10:20:50.378638Z"
    },
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cross_path = 'data/same-side-classification/cross-topic/{}.csv'\n",
    "data_within_path = 'data/same-side-classification/within-topic/{}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:20:51.434447Z",
     "start_time": "2019-06-27T10:20:51.405500Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n 5 data/same-side-classification/cross-topic/training.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load within-topics and cross-topics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:21:12.258921Z",
     "start_time": "2019-06-27T10:21:10.382011Z"
    },
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [read cross]: 0:00:00.907635\n",
      "Time for [read within]: 0:00:00.960050\n"
     ]
    }
   ],
   "source": [
    "# escapechar to detect quoting escapes, else it fails\n",
    "\n",
    "# na_filter=False, because pandas automatic \"nan\" detection fails with the topic column, too\n",
    "# cross_test_df['topic'].astype(str)[9270]\n",
    "\n",
    "with Timer(\"read cross\"):\n",
    "    cross_traindev_df = pd.read_csv(data_cross_path.format('training'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    cross_test_df = pd.read_csv(data_cross_path.format('test'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "\n",
    "with Timer(\"read within\"):\n",
    "    within_traindev_df = pd.read_csv(data_within_path.format('training'), index_col='id', escapechar='\\\\', na_filter=False)\n",
    "    within_test_df = pd.read_csv(data_within_path.format('test'), index_col='id', escapechar='\\\\', na_filter=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:22:59.075056Z",
     "start_time": "2019-06-27T10:21:21.001901Z"
    },
    "code_folding": [
     1
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [tag cross traindev]: 0:00:31.576688\n",
      "Time for [tag cross test]: 0:00:17.567800\n",
      "Time for [tag within traindev]: 0:00:32.856501\n",
      "Time for [tag within test]: 0:00:16.060382\n"
     ]
    }
   ],
   "source": [
    "# Adding a tag for the topics in focus: \"gay marriage\" and \"abortion\"\n",
    "def add_tag(row):\n",
    "    title = row['topic'].lower().strip()\n",
    "    if \"abortion\" in title:\n",
    "        row['tag'] = 'abortion'\n",
    "    elif \"gay marriage\"  in title:\n",
    "        row['tag'] = 'gay marriage'\n",
    "    else:\n",
    "        row['tag'] = 'NA'\n",
    "    return row\n",
    "\n",
    "\n",
    "with Timer(\"tag cross traindev\"):\n",
    "    cross_traindev_df = cross_traindev_df.apply(add_tag, axis=1)\n",
    "with Timer(\"tag cross test\"):\n",
    "    cross_test_df = cross_test_df.apply(add_tag, axis=1)\n",
    "\n",
    "with Timer(\"tag within traindev\"):\n",
    "    within_traindev_df = within_traindev_df.apply(add_tag, axis=1)\n",
    "with Timer(\"tag within test\"):\n",
    "    within_test_df = within_test_df.apply(add_tag, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an overview about each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:22:59.628057Z",
     "start_time": "2019-06-27T10:22:59.620276Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_overview(df, task='same-side', class_name='is_same_side'):\n",
    "    # Total instance numbers\n",
    "    total = len(df)\n",
    "    print(\"Task: \", task)\n",
    "    print('=' * 40, '\\n')\n",
    "\n",
    "    print('Total instances: ', total)\n",
    "    print('\\n')\n",
    "\n",
    "    print('For each topic:')\n",
    "    for tag, tag_df in df.groupby(['tag']):\n",
    "        print(tag, ': ', len(tag_df), ' instances')\n",
    "        if class_name in df.columns:\n",
    "            for is_same_side, side_df in tag_df.groupby([class_name]):\n",
    "                print('\\t\\t', is_same_side, ': ', len(side_df), ' instances')\n",
    "    print('\\n')\n",
    "\n",
    "    if class_name in df.columns:\n",
    "        print('For each class value:')\n",
    "        for class_value, class_df in df.groupby([class_name]):\n",
    "            print(class_value, ': ', len(class_df), ' instances')\n",
    "        print('\\n')\n",
    "\n",
    "    print('Unique argument1:', len(df['argument1'].unique()))\n",
    "    print('Unique argument2:', len(df['argument2'].unique()))\n",
    "    arguments = df['argument1'].values\n",
    "    arguments = np.concatenate([arguments, df['argument2'].values])\n",
    "\n",
    "    print('Unique total arguments:', len(set(list(arguments))), '\\n')\n",
    "\n",
    "    print('-' * 40, '\\n')\n",
    "\n",
    "    arguments_length_lst = [\n",
    "        len(word_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_length_lst.extend(\n",
    "        [len(word_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Words:')\n",
    "    print('\\tshortest argument:', min(arguments_length_lst), ' words')\n",
    "    print('\\tlongest argument:', max(arguments_length_lst), ' words')\n",
    "    print('\\targument average length:', np.mean(arguments_length_lst),\n",
    "          ' words')\n",
    "\n",
    "    arguments_sent_length_lst = [\n",
    "        len(sent_tokenize(x)) for x in df['argument1'].values\n",
    "    ]\n",
    "    arguments_sent_length_lst.extend(\n",
    "        [len(sent_tokenize(x)) for x in df['argument2'].values])\n",
    "    print('Sentences:')\n",
    "    print('\\tshortest argument:', min(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\tlongest argument:', max(arguments_sent_length_lst), ' sentences')\n",
    "    print('\\targument average length:', np.mean(arguments_sent_length_lst),\n",
    "          ' sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:23:00.170717Z",
     "start_time": "2019-06-27T10:23:00.169032Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with Timer(\"overview cross\"):\n",
    "#     get_overview(cross_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:23:00.718983Z",
     "start_time": "2019-06-27T10:23:00.717289Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with Timer(\"overview within\"):\n",
    "#     get_overview(within_traindev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train dev set - 70% 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:23:01.270556Z",
     "start_time": "2019-06-27T10:23:01.267478Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_train_test_sets(df, ratio=0.30, random_state=1):\n",
    "    X = df[['argument1', 'argument2', 'argument1_id', 'argument2_id', 'topic']]\n",
    "    y = df[['is_same_side']]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                        y,\n",
    "                                                        test_size=ratio,\n",
    "                                                        random_state=random_state,\n",
    "                                                        shuffle=True)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT\n",
    "\n",
    "- https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:23:01.825943Z",
     "start_time": "2019-06-27T10:23:01.821264Z"
    },
    "code_folding": [
     2
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon.data import SimpleDataset\n",
    "\n",
    "class MyBERTDataset(SimpleDataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        super(MyBERTDataset, self).__init__(self._convert())\n",
    "\n",
    "    # def _convert(self):\n",
    "    #     allsamples = list()\n",
    "    #     for _, row in self._X[['argument1', 'argument2', 'is_same_side']].iterrows():\n",
    "    #         allsamples.append([row['argument1'], row['argument2'], \"1\" if row['is_same_side'] else \"0\"])\n",
    "    #     return allsamples\n",
    "    \n",
    "    def _convert(self):\n",
    "        allsamples = list()\n",
    "\n",
    "        if self._y is not None:\n",
    "            df = self._X.merge(self._y, left_index=True, right_index=True)\n",
    "            for _, row in df.iterrows():\n",
    "                allsamples.append([row['argument1'], row['argument2'], \"1\" if row['is_same_side'] == \"True\" else \"0\"])\n",
    "        else:\n",
    "            for _, row in self._X.iterrows():\n",
    "                allsamples.append([row['argument1'], row['argument2'], None])\n",
    "        \n",
    "        return allsamples\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     row_X = self._X.iloc[idx]\n",
    "    #     row_y = self._y.iloc[idx]\n",
    "    #     return [row_X['argument1'], row_X['argument2'], \"1\" if row_y['is_same_side'] else \"0\"]\n",
    "\n",
    "    # def __len__(self):\n",
    "    #     return len(self._X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:23:02.381709Z",
     "start_time": "2019-06-27T10:23:02.379974Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = X_dev.merge(y_dev, left_index=True, right_index=True)\n",
    "# allsamples = list()\n",
    "# for _, row in df.iterrows():\n",
    "#     allsamples.append(\"1\" if row['is_same_side'] == \"True\" else \"0\")\n",
    "# np.unique(allsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:26:50.376314Z",
     "start_time": "2019-06-27T10:24:25.552Z"
    },
    "code_folding": [
     0,
     44,
     50,
     71,
     72,
     145,
     173,
     193
    ],
    "init_cell": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def setup_bert():\n",
    "    np.random.seed(100)\n",
    "    random.seed(100)\n",
    "    mx.random.seed(10000)\n",
    "    # change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "    ctx = mx.gpu(0)\n",
    "    # ctx =  mx.gpu() if mx.context.num_gpus() else mx.cpu()\n",
    "    # ctx = mx.cpu()\n",
    "    \n",
    "    bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                                 dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                                 pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                                 use_decoder=False, use_classifier=False)\n",
    "    print(bert_base)\n",
    "    \n",
    "    model = bert.BERTClassifier(bert_base, num_classes=2, dropout=0.1)\n",
    "    # only need to initialize the classifier layer.\n",
    "    model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "    model.hybridize(static_alloc=True)\n",
    "\n",
    "    # softmax cross entropy loss for classification\n",
    "    loss_function = gluon.loss.SoftmaxCELoss()\n",
    "    loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "    metric = mx.metric.Accuracy()\n",
    "    \n",
    "    # use the vocabulary from pre-trained model for tokenization\n",
    "    bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "    # maximum sequence length\n",
    "    max_len = 128\n",
    "    # the labels for the two classes\n",
    "    all_labels = [\"0\", \"1\"]\n",
    "    # whether to transform the data as sentence pairs.\n",
    "    # for single sentence classification, set pair=False\n",
    "    pair = True\n",
    "    transform = dataset.BERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                             labels=all_labels,\n",
    "                                             label_dtype='int32',\n",
    "                                             pad=True,\n",
    "                                             pair=pair)\n",
    "\n",
    "    return model, vocabulary, ctx, transform, loss_function, metric, all_labels\n",
    "\n",
    "    \n",
    "def transform_dataset(X, y, transform):\n",
    "    data_train_raw = MyBERTDataset(X, y)\n",
    "    data_train = data_train_raw.transform(transform)\n",
    "    return data_train_raw, data_train\n",
    "\n",
    "\n",
    "def print_infos(vocabulary, data_train_raw, data_train):\n",
    "    sample_id = 0\n",
    "\n",
    "    # sentence a\n",
    "    print(data_train_raw[sample_id][0])\n",
    "    # sentence b\n",
    "    print(data_train_raw[sample_id][1])\n",
    "    # 1 means equivalent, 0 means not equivalent\n",
    "    print(data_train_raw[sample_id][2])\n",
    "\n",
    "    print('vocabulary used for tokenization = \\n%s'%vocabulary)\n",
    "    print('[PAD] token id = %s'%(vocabulary['[PAD]']))\n",
    "    print('[CLS] token id = %s'%(vocabulary['[CLS]']))\n",
    "    print('[SEP] token id = %s'%(vocabulary['[SEP]']))\n",
    "\n",
    "    print('token ids = \\n%s'%data_train[sample_id][0])\n",
    "    print('valid length = \\n%s'%data_train[sample_id][1])\n",
    "    print('segment ids = \\n%s'%data_train[sample_id][2])\n",
    "    print('label = \\n%s'%data_train[sample_id][3])\n",
    "    \n",
    "\n",
    "def train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=3):\n",
    "    with Timer(\"setup training\"):\n",
    "        train_sampler = nlp.data.FixedBucketSampler(\n",
    "            lengths=[int(item[1]) for item in tqdm(data_train)],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True)\n",
    "        bert_dataloader = mx.gluon.data.DataLoader(data_train,\n",
    "                                                   batch_sampler=train_sampler)\n",
    "\n",
    "        trainer = gluon.Trainer(model.collect_params(), 'adam', {\n",
    "            'learning_rate': lr,\n",
    "            'epsilon': 1e-9\n",
    "        })\n",
    "\n",
    "        # collect all differentiable parameters\n",
    "        # grad_req == 'null' indicates no gradients are calculated (e.g. constant parameters)\n",
    "        # the gradients for these params are clipped later\n",
    "        params = [p for p in model.collect_params().values() if p.grad_req != 'null']\n",
    "\n",
    "    log_interval = 10\n",
    "    with Timer(\"training\"):\n",
    "        stats = list()\n",
    "        for epoch_id in range(num_epochs):\n",
    "            epoch_checkpoint_savefile = \"data/bert.model.checkpoint{}.params\".format(epoch_id)\n",
    "            if os.path.exists(epoch_checkpoint_savefile):\n",
    "                model.load_parameters(epoch_checkpoint_savefile, ctx=ctx)\n",
    "                print(\"loaded checkpoint for epoch {}\".format(epoch_id))\n",
    "                continue\n",
    "\n",
    "            with Timer(\"epoch {}\".format(epoch_id)):\n",
    "                metric.reset()\n",
    "                step_loss = 0\n",
    "                t_p = time.time()  # time keeping\n",
    "                for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                               label) in enumerate(bert_dataloader):\n",
    "                    with mx.autograd.record():\n",
    "\n",
    "                        # load data to GPU\n",
    "                        token_ids = token_ids.as_in_context(ctx)\n",
    "                        valid_length = valid_length.as_in_context(ctx)\n",
    "                        segment_ids = segment_ids.as_in_context(ctx)\n",
    "                        label = label.as_in_context(ctx)\n",
    "\n",
    "                        # forward computation\n",
    "                        out = model(token_ids, segment_ids,\n",
    "                                    valid_length.astype('float32'))\n",
    "                        ls = loss_function(out, label).mean()\n",
    "\n",
    "                    # backward computation\n",
    "                    ls.backward()\n",
    "\n",
    "                    # gradient clipping\n",
    "                    trainer.allreduce_grads()\n",
    "                    nlp.utils.clip_grad_global_norm(params, 1)\n",
    "                    trainer.update(1)\n",
    "\n",
    "                    step_loss += ls.asscalar()\n",
    "                    metric.update([label], [out])\n",
    "                    stats.append((metric.get()[1], ls.asscalar()))\n",
    "                    if (batch_id + 1) % (log_interval) == 0:\n",
    "                        print(\n",
    "                            '[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f} - time {}'\n",
    "                            .format(epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                    step_loss / log_interval, trainer.learning_rate,\n",
    "                                    metric.get()[1],\n",
    "                                    datetime.timedelta(seconds=(time.time() - t_p))))\n",
    "                        t_p = time.time()\n",
    "                        step_loss = 0\n",
    "\n",
    "            model.save_parameters(epoch_checkpoint_savefile)\n",
    "            \n",
    "    return stats\n",
    "\n",
    "\n",
    "def predict(model, data_predict, ctx, metric, loss_function, batch_size=32):\n",
    "    bert_dataloader = mx.gluon.data.DataLoader(data_predict, batch_size=batch_size)\n",
    "    \n",
    "    all_predictions = list()\n",
    "\n",
    "    with Timer(\"prediction\"):\n",
    "        metric.reset()\n",
    "        cum_loss = 0\n",
    "        for batch_id, (token_ids, valid_length, segment_ids,\n",
    "                       label) in enumerate(tqdm(bert_dataloader)):\n",
    "            # load data to GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # forward computation\n",
    "            out = model(token_ids, segment_ids,\n",
    "                        valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "            metric.update([label], [out])\n",
    "            cum_loss += ls.asscalar()  # .sum() ?\n",
    "            all_predictions.append((batch_id, label, out))\n",
    "            \n",
    "    return all_predictions, cum_loss\n",
    "\n",
    "\n",
    "def predict_out_to_ys(all_predictions, all_labels):\n",
    "    y_true, y_pred = list(), list()\n",
    "    \n",
    "    for _, y_true_many, y_pred_many in all_predictions:\n",
    "        y_true_many = y_true_many.T[0].asnumpy()\n",
    "        # https://mxnet.incubator.apache.org/api/python/gluon/loss.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss\n",
    "        # pred: the prediction tensor, where the batch_axis dimension ranges over batch size and axis dimension ranges over the number of classes.\n",
    "        y_pred_many = np.argmax(y_pred_many, axis=1).asnumpy()\n",
    "\n",
    "        y_true.extend(list(y_true_many))\n",
    "        y_pred.extend(list(y_pred_many))\n",
    "        # TODO: convert label_id to label?\n",
    "        # y_pred.extend(all_labels[c] for c in list(y_pred_many))\n",
    "        \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    return y_true, y_pred\n",
    "\n",
    "\n",
    "def plot_train_stats(stats):\n",
    "    if not stats:\n",
    "        print(\"no stats to plot\")\n",
    "        return\n",
    "\n",
    "    x = np.arange(len(stats))  # arange/linspace\n",
    "\n",
    "    acc_dots, loss_dots = zip(*stats)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(x, acc_dots, 'o-')\n",
    "    plt.title('Training BERTClassifier')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, loss_dots, '.-')\n",
    "    plt.xlabel('Batches')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:10:05.835266Z",
     "start_time": "2019-06-27T10:10:05.782163Z"
    }
   },
   "outputs": [],
   "source": [
    "# stats epoch 0\n",
    "stats = [(0.463, 0.7123), (0.472, 0.7044), (0.492, 0.6946), (0.491, 0.7027), (0.485, 0.7174), (0.486, 0.7049), (0.489, 0.6898), (0.498, 0.6801), (0.507, 0.6777), (0.510, 0.6774), (0.512, 0.6804), (0.515, 0.6767), (0.521, 0.6557), (0.525, 0.6541), (0.529, 0.6627), (0.533, 0.6571), (0.538, 0.6374), (0.540, 0.6530), (0.542, 0.6601), (0.545, 0.6472), (0.548, 0.6380), (0.551, 0.6240), (0.554, 0.5918), (0.556, 0.6199), (0.557, 0.6604), (0.559, 0.6456), (0.561, 0.6307), (0.564, 0.5906), (0.566, 0.5811), (0.570, 0.5783), (0.570, 0.6558), (0.572, 0.6158), (0.575, 0.6104), (0.576, 0.6459), (0.578, 0.6042), (0.580, 0.6000), (0.580, 0.6210), (0.581, 0.5967), (0.584, 0.5439), (0.586, 0.5840), (0.588, 0.5868), (0.589, 0.5722), (0.592, 0.5480), (0.593, 0.5927), (0.595, 0.5637), (0.597, 0.5776), (0.598, 0.5984), (0.600, 0.5582), (0.601, 0.6020), (0.602, 0.5802), (0.603, 0.6227), (0.604, 0.5647), (0.606, 0.5723), (0.607, 0.5867), (0.608, 0.6026), (0.610, 0.5912), (0.611, 0.5460), (0.612, 0.5881), (0.614, 0.5492), (0.615, 0.5646), (0.616, 0.5599), (0.618, 0.5465), (0.619, 0.5451), (0.620, 0.5630), (0.622, 0.5262), (0.623, 0.5364), (0.624, 0.5776), (0.625, 0.5395), (0.625, 0.5752), (0.627, 0.5260), (0.628, 0.5089), (0.630, 0.4781), (0.632, 0.5075), (0.632, 0.5947), (0.634, 0.4587), (0.635, 0.4911), (0.637, 0.4553), (0.638, 0.4978), (0.639, 0.5615), (0.640, 0.5320), (0.641, 0.5166), (0.641, 0.5333), (0.642, 0.5349), (0.644, 0.4600), (0.646, 0.4905), (0.647, 0.4696), (0.649, 0.4941), (0.649, 0.5356), (0.650, 0.5460), (0.651, 0.4847), (0.651, 0.5764), (0.652, 0.4925), (0.653, 0.4920), (0.654, 0.5040), (0.655, 0.4575), (0.656, 0.4878), (0.657, 0.4863), (0.658, 0.4382), (0.659, 0.5263), (0.660, 0.4658), (0.660, 0.5230), (0.661, 0.4819), (0.662, 0.4789), (0.663, 0.4649), (0.663, 0.5330), (0.664, 0.5286), (0.665, 0.3950), (0.666, 0.4627), (0.666, 0.5199), (0.667, 0.4338), (0.668, 0.4790), (0.669, 0.3865), (0.670, 0.4556), (0.670, 0.4920), (0.671, 0.4475), (0.672, 0.4176), (0.673, 0.4944), (0.673, 0.4776), (0.673, 0.5621), (0.673, 0.4697), (0.675, 0.3726), (0.675, 0.4617), (0.676, 0.5107), (0.676, 0.4507), (0.677, 0.4423), (0.677, 0.4501), (0.678, 0.4360), (0.679, 0.4173), (0.680, 0.4354), (0.680, 0.4053), (0.681, 0.4539), (0.682, 0.4677), (0.682, 0.4357), (0.682, 0.5352), (0.683, 0.4342), (0.684, 0.3658), (0.684, 0.5012), (0.684, 0.4274), (0.685, 0.3888), (0.686, 0.4580)]\n",
    "stats1 = [(0.762, 0.4022), (0.784, 0.3754), (0.757, 0.4910), (0.752, 0.4802), (0.745, 0.5064), (0.749, 0.4351), (0.761, 0.3358), (0.764, 0.4418), (0.770, 0.3495), (0.772, 0.3822), (0.777, 0.3288), (0.779, 0.3499), (0.777, 0.4608), (0.776, 0.4025), (0.773, 0.4587), (0.773, 0.3971), (0.771, 0.4782), (0.767, 0.5041), (0.768, 0.4014), (0.770, 0.3562), (0.771, 0.4098), (0.770, 0.4362), (0.771, 0.3615), (0.772, 0.3492), (0.773, 0.3819), (0.774, 0.3715), (0.772, 0.4625), (0.772, 0.3953), (0.772, 0.3972), (0.774, 0.3222), (0.772, 0.4583), (0.772, 0.4265), (0.772, 0.4151), (0.775, 0.2895), (0.773, 0.4737), (0.771, 0.4858), (0.771, 0.4258), (0.771, 0.3748), (0.771, 0.3944), (0.770, 0.4685), (0.770, 0.4081), (0.769, 0.4215), (0.769, 0.3755), (0.770, 0.3139), (0.772, 0.3441), (0.772, 0.4142), (0.772, 0.4515), (0.771, 0.3986), (0.771, 0.3995), (0.772, 0.3128), (0.773, 0.3351), (0.773, 0.3750), (0.773, 0.4671), (0.772, 0.4311), (0.771, 0.4054), (0.771, 0.3879), (0.772, 0.3446), (0.772, 0.3895), (0.773, 0.2833), (0.774, 0.3271), (0.774, 0.3914), (0.775, 0.3145), (0.776, 0.3383), (0.776, 0.3849), (0.776, 0.3866), (0.777, 0.2966), (0.776, 0.3446), (0.777, 0.3054), (0.778, 0.3164), (0.778, 0.3753), (0.778, 0.3885), (0.778, 0.3327), (0.778, 0.3364), (0.779, 0.2948), (0.779, 0.3672), (0.778, 0.3656), (0.778, 0.3924), (0.778, 0.3592), (0.778, 0.3584), (0.777, 0.4452), (0.778, 0.2480), (0.779, 0.2843), (0.778, 0.3921), (0.778, 0.3477), (0.778, 0.3009), (0.779, 0.3591), (0.779, 0.3435), (0.779, 0.3548), (0.779, 0.3534), (0.779, 0.3555), (0.778, 0.4349), (0.778, 0.3850), (0.778, 0.3005), (0.779, 0.3127), (0.779, 0.2829), (0.781, 0.1936), (0.781, 0.3559), (0.781, 0.3700), (0.782, 0.2512), (0.782, 0.3481), (0.782, 0.4027), (0.783, 0.2924), (0.783, 0.3477), (0.783, 0.3057), (0.785, 0.2244), (0.786, 0.2277), (0.786, 0.3636), (0.786, 0.2908), (0.787, 0.3115), (0.787, 0.2987), (0.788, 0.2362), (0.787, 0.3892), (0.788, 0.3240), (0.789, 0.2397), (0.789, 0.3245), (0.789, 0.2749), (0.789, 0.3293), (0.790, 0.3606), (0.790, 0.3428), (0.790, 0.2628), (0.790, 0.2960), (0.791, 0.3174), (0.791, 0.3123), (0.791, 0.2579), (0.791, 0.3368), (0.792, 0.2781), (0.791, 0.3672), (0.791, 0.3612), (0.791, 0.3621), (0.791, 0.3119), (0.791, 0.3438), (0.791, 0.2874), (0.792, 0.3129), (0.792, 0.2876), (0.792, 0.3525), (0.792, 0.3677), (0.792, 0.3154), (0.792, 0.2791), (0.793, 0.3408), (0.793, 0.2209)]\n",
    "stats2 = [(0.784, 0.4021), (0.808, 0.2928), (0.819, 0.2680), (0.805, 0.3782), (0.813, 0.2370), (0.808, 0.3956), (0.817, 0.2744), (0.821, 0.2447), (0.816, 0.4031), (0.813, 0.3375), (0.813, 0.3493), (0.814, 0.2917), (0.816, 0.2842), (0.817, 0.3099), (0.814, 0.3550), (0.815, 0.3104), (0.818, 0.2659), (0.819, 0.3006), (0.822, 0.2382), (0.821, 0.3389), (0.823, 0.2935), (0.826, 0.2225), (0.824, 0.3450), (0.823, 0.3455), (0.823, 0.3110), (0.821, 0.3629), (0.823, 0.2309), (0.824, 0.2557), (0.826, 0.2459), (0.826, 0.2998), (0.826, 0.3181), (0.825, 0.3317), (0.825, 0.2639), (0.826, 0.2462), (0.825, 0.3095), (0.825, 0.3129), (0.827, 0.1889), (0.827, 0.2447), (0.828, 0.2963), (0.828, 0.2754), (0.828, 0.2996), (0.827, 0.3191), (0.827, 0.2630), (0.828, 0.2772), (0.828, 0.2549), (0.828, 0.2956), (0.829, 0.1692), (0.829, 0.3063), (0.829, 0.2829), (0.829, 0.2868), (0.829, 0.3313), (0.828, 0.3181), (0.829, 0.1545), (0.830, 0.2538), (0.831, 0.2703), (0.830, 0.3438), (0.830, 0.2023), (0.831, 0.2892), (0.831, 0.2871), (0.830, 0.3245), (0.830, 0.2961), (0.830, 0.2729), (0.830, 0.3000), (0.830, 0.2875), (0.830, 0.2239), (0.830, 0.2513), (0.830, 0.2744), (0.831, 0.1982), (0.832, 0.3053), (0.832, 0.2393), (0.832, 0.2492), (0.832, 0.3203), (0.833, 0.2268), (0.833, 0.2522), (0.833, 0.2234), (0.834, 0.2575), (0.833, 0.3414), (0.834, 0.2789), (0.834, 0.2647), (0.833, 0.3643), (0.834, 0.2385), (0.833, 0.3488), (0.834, 0.2366), (0.832, 0.4040), (0.832, 0.2774), (0.833, 0.2627), (0.832, 0.4101), (0.832, 0.2817), (0.831, 0.3215), (0.831, 0.2931), (0.831, 0.2940), (0.831, 0.2718), (0.832, 0.2800), (0.832, 0.1830), (0.831, 0.3987), (0.832, 0.2587), (0.832, 0.2232), (0.832, 0.2434), (0.832, 0.3609), (0.832, 0.2691), (0.832, 0.3043), (0.832, 0.2163), (0.833, 0.2471), (0.833, 0.1837), (0.833, 0.2906), (0.833, 0.3173), (0.833, 0.2041), (0.833, 0.2797), (0.833, 0.3028), (0.833, 0.3056), (0.833, 0.2621), (0.834, 0.1788), (0.834, 0.2522), (0.834, 0.2986), (0.834, 0.2757), (0.834, 0.3359), (0.834, 0.2305), (0.834, 0.2881), (0.833, 0.3825), (0.833, 0.2566), (0.834, 0.2298), (0.834, 0.2410), (0.834, 0.2020), (0.834, 0.3683), (0.833, 0.3932), (0.833, 0.2345), (0.833, 0.2657), (0.834, 0.1845), (0.834, 0.2546), (0.834, 0.2679), (0.834, 0.2808), (0.834, 0.3604), (0.834, 0.2851), (0.833, 0.3526), (0.834, 0.3193), (0.833, 0.3584), (0.833, 0.2667), (0.833, 0.2769), (0.833, 0.3096), (0.833, 0.2349)]\n",
    "\n",
    "plot_train_stats(stats + stats1 + stats2)\n",
    "# plot_train_stats(stats1)\n",
    "# plot_train_stats(stats2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:23:03.521225Z",
     "start_time": "2019-06-27T10:23:03.516250Z"
    },
    "code_folding": [
     0,
     22,
     33,
     44,
     56
    ],
    "init_cell": true,
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def heatconmat(y_test, y_pred):\n",
    "    sns.set_context('talk')\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred),\n",
    "                annot=True,\n",
    "                fmt='d',\n",
    "                cbar=False,\n",
    "                cmap='gist_earth_r',\n",
    "                yticklabels=sorted(y_test.unique()))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def report_training_results(y_test, y_pred, name=None, heatmap=True):\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    if heatmap:\n",
    "        heatconmat(y_test['is_same_side'], y_pred)\n",
    "    print()\n",
    "    print('Accuracy: ', round(accuracy_score(y_test, y_pred), 2), '\\n')  #\n",
    "\n",
    "    print('Report{}:'.format(\"\" if not name else \" for [{}]\".format(name)))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    f1_dic = {}\n",
    "    f1_dic['macro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='macro'), 2)\n",
    "    f1_dic['micro'] = round(\n",
    "        f1_score(y_pred=y_pred, y_true=y_test, average='micro'), 2)\n",
    "    return f1_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross topic - Training and evaluating model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:23:04.089611Z",
     "start_time": "2019-06-27T10:23:04.073744Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [1 - test/train split]: 0:00:00.013185\n"
     ]
    }
   ],
   "source": [
    "# 1. Getting train and dev data\n",
    "with Timer(\"1 - test/train split\"):\n",
    "    X_train, X_dev, y_train, y_dev = get_train_test_sets(within_traindev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:23:19.876730Z",
     "start_time": "2019-06-27T10:23:17.595337Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTModel(\n",
      "  (encoder): BERTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (1): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (2): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (3): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (4): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (5): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (6): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (7): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (8): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (9): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (10): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (11): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): HybridSequential(\n",
      "    (0): Embedding(30522 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (token_type_embed): HybridSequential(\n",
      "    (0): Embedding(2 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n",
      "Time for [2 - setup BERT model]: 0:00:02.275504\n"
     ]
    }
   ],
   "source": [
    "# 2. setup\n",
    "with Timer(\"2 - setup BERT model\"):\n",
    "    model, vocabulary, ctx, transform, loss_function, metric, all_labels = setup_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:23:34.642674Z",
     "start_time": "2019-06-27T10:23:32.246943Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wanted fetuses are beloved \"babies\"; unwanted ones are \"tissue\" (inconsistent)\n",
      "abortions are emotionally and psychologically unsafe.\n",
      "1\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[PAD]', '[CLS]', '[SEP]', '[MASK]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2  2359 10768  5809  2229  2024 11419  1000 10834  1000  1025 18162\n",
      "  3924  2024  1000  8153  1000  1006 20316  1007     3 11324  2015  2024\n",
      " 14868  1998  8317  2135 25135  1012     3     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "31\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[1]\n",
      "Time for [3 - prepare training data]: 0:00:02.390510\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"3 - prepare training data\"):\n",
    "    data_train_raw, data_train = transform_dataset(X_train, y_train, transform)\n",
    "    print_infos(vocabulary, data_train_raw, data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:26:50.345912Z",
     "start_time": "2019-06-27T10:23:39.563913Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44732/44732 [03:08<00:00, 237.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [setup training]: 0:03:08.036337\n",
      "loaded checkpoint for epoch 0\n",
      "loaded checkpoint for epoch 1\n",
      "loaded checkpoint for epoch 2\n",
      "Time for [training]: 0:00:00.455203\n",
      "Time for [4 - train model]: 0:03:10.686810\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4651477848ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/bert.model.params\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplot_train_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-294bc5026abb>\u001b[0m in \u001b[0;36mplot_train_stats\u001b[0;34m(stats)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# arange/linspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0macc_dots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_dots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "with Timer(\"4 - train model\"):\n",
    "    # train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=3)\n",
    "    stats = train(model, data_train, ctx, metric, loss_function, batch_size=32, lr=5e-6, num_epochs=3)\n",
    "    model.save_parameters(\"data/bert.model.params\")\n",
    "    \n",
    "    plot_train_stats(stats)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:27:15.632079Z",
     "start_time": "2019-06-27T10:27:14.615195Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abortion opens the door to the sexual exploitation of women the existence of abortion gives men a little more of a safeguard against unintentionally impregnating a woman. as a result, men will be more aggressive in their sexual exploitation of women.\n",
      "the fact that a child is likely to have a short life does not justify further shortening it:\n",
      "0\n",
      "vocabulary used for tokenization = \n",
      "Vocab(size=30522, unk=\"[UNK]\", reserved=\"['[PAD]', '[CLS]', '[SEP]', '[MASK]']\")\n",
      "[PAD] token id = 1\n",
      "[CLS] token id = 2\n",
      "[SEP] token id = 3\n",
      "token ids = \n",
      "[    2 11324  7480  1996  2341  2000  1996  4424 14427  1997  2308  1996\n",
      "  4598  1997 11324  3957  2273  1037  2210  2062  1997  1037 28805  2114\n",
      "  4895 18447  4765 19301  2135 17727  2890 16989  3436  1037  2450  1012\n",
      "  2004  1037  2765  1010  2273  2097  2022  2062  9376  1999  2037  4424\n",
      " 14427  1997  2308  1012     3  1996  2755  2008  1037  2775  2003  3497\n",
      "  2000  2031  1037  2460  2166  2515  2025 16114  2582  2460  7406  2009\n",
      "  1024     3     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "     1     1     1     1     1     1     1     1]\n",
      "valid length = \n",
      "74\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[0]\n",
      "Time for [5 - prepare eval data]: 0:00:01.011090\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"5 - prepare eval data\"):\n",
    "    data_dev_raw, data_dev = transform_dataset(X_dev, y_dev, transform)\n",
    "    print_infos(vocabulary, data_dev_raw, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-27T10:30:17.994194Z",
     "start_time": "2019-06-27T10:27:22.409843Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 600/600 [02:55<00:00,  3.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for [prediction]: 0:02:55.341361\n",
      "Accuracy: 0.8265609514370664\n",
      "Confusion Matrix:\n",
      "[[8598  235]\n",
      " [3090 7248]]\n",
      "\n",
      "Accuracy:  0.83 \n",
      "\n",
      "Report for [BERTClassifier]:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.97      0.84      8833\n",
      "           1       0.97      0.70      0.81     10338\n",
      "\n",
      "    accuracy                           0.83     19171\n",
      "   macro avg       0.85      0.84      0.83     19171\n",
      "weighted avg       0.86      0.83      0.82     19171\n",
      "\n",
      "Time for [6 - evaluate]: 0:02:55.578261\n"
     ]
    }
   ],
   "source": [
    "with Timer(\"6 - evaluate\"):\n",
    "    model.load_parameters(\"data/bert.model.params\", ctx=ctx)\n",
    "    all_predictions, cum_loss = predict(model, data_dev, ctx, metric, loss_function)\n",
    "    print(\"Accuracy:\", metric.get()[1])\n",
    "    \n",
    "    y_true, y_pred = predict_out_to_ys(all_predictions, all_labels)\n",
    "    report_training_results(y_true, y_pred, name=\"BERTClassifier\", heatmap=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
